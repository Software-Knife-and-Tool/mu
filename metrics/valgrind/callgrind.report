==1241288== Callgrind, a call-graph generating cache profiler
==1241288== Copyright (C) 2002-2017, and GNU GPL'd, by Josef Weidendorfer et al.
==1241288== Using Valgrind-3.21.0 and LibVEX; rerun with -h for copyright info
==1241288== Command: ../../dist/mu-sys -p -q ((:lambda\ (g)\ (mu:apply\ g\ (mu:cons\ g\ (mu:cons\ 101010101\ (mu:cons\ 11011\ ())))))\ (:lambda\ (g\ a\ b)\ (:if\ (mu:eq\ 0\ b)\ a\ (mu:apply\ g\ (mu:cons\ g\ (mu:cons\ b\ (mu:cons\ (mu:fx-sub\ a\ (mu:fx-mul\ b\ (mu:fx-div\ a\ b)))\ ())))))))\ 
==1241288== 
==1241288== For interactive control, run 'callgrind_control -h'.
==1241288== 
==1241288== Events    : Ir
==1241288== Collected : 2823312
==1241288== 
==1241288== I   refs:      2,823,312
--------------------------------------------------------------------------------
Profile data file 'callgrind.out.1241288' (creator: callgrind-3.21.0)
--------------------------------------------------------------------------------
I1 cache: 
D1 cache: 
LL cache: 
Timerange: Basic block 0 - 525068
Trigger: Program termination
Profiled target:  ../../dist/mu-sys -p -q ((:lambda (g) (mu:apply g (mu:cons g (mu:cons 101010101 (mu:cons 11011 ()))))) (:lambda (g a b) (:if (mu:eq 0 b) a (mu:apply g (mu:cons g (mu:cons b (mu:cons (mu:fx-sub a (mu:fx-mul b (mu:fx-div a b))) ())))))))  (PID 1241288, part 1)
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Thresholds:       99
Include dirs:     
User annotated:   
Auto-annotation:  on

--------------------------------------------------------------------------------
Ir                 
--------------------------------------------------------------------------------
2,823,312 (100.0%)  PROGRAM TOTALS

--------------------------------------------------------------------------------
Ir                file:function
--------------------------------------------------------------------------------
219,294 ( 7.77%)  ???:<futures_locks::rwlock::RwLockReadFut<T> as core::future::future::Future>::poll [/vol/projects/mu/dist/mu-sys]
215,104 ( 7.62%)  ???:futures_executor::local_pool::block_on [/vol/projects/mu/dist/mu-sys]
201,609 ( 7.14%)  ???:futures_locks::rwlock::RwLock<T>::unlock_reader [/vol/projects/mu/dist/mu-sys]
198,480 ( 7.03%)  ???:libmu::core::types::Tag::type_of [/vol/projects/mu/dist/mu-sys]
154,393 ( 5.47%)  ???:core::hash::BuildHasher::hash_one [/vol/projects/mu/dist/mu-sys]
 92,187 ( 3.27%)  ???:<core::hash::sip::Hasher<S> as core::hash::Hasher>::write [/vol/projects/mu/dist/mu-sys]
 76,824 ( 2.72%)  ???:<futures_locks::rwlock::RwLockWriteFut<T> as core::future::future::Future>::poll [/vol/projects/mu/dist/mu-sys]
 76,824 ( 2.72%)  ???:futures_locks::rwlock::RwLock<T>::unlock_writer [/vol/projects/mu/dist/mu-sys]
 66,342 ( 2.35%)  ./elf/./elf/dl-lookup.c:do_lookup_x [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
 63,680 ( 2.26%)  ???:libmu::types::cons::Cons::to_image [/vol/projects/mu/dist/mu-sys]
 61,114 ( 2.16%)  ???:futures_executor::enter::enter [/vol/projects/mu/dist/mu-sys]
 55,046 ( 1.95%)  ./stdio-common/./stdio-common/vfscanf-internal.c:__vfscanf_internal [/usr/lib/x86_64-linux-gnu/libc.so.6]
 51,711 ( 1.83%)  ???:<futures_executor::enter::Enter as core::ops::drop::Drop>::drop [/vol/projects/mu/dist/mu-sys]
 48,490 ( 1.72%)  ???:libmu::allocators::bump_allocator::BumpAllocator::alloc [/vol/projects/mu/dist/mu-sys]
 43,550 ( 1.54%)  ???:libmu::types::function::Function::to_image [/vol/projects/mu/dist/mu-sys]
 39,600 ( 1.40%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockReadFut<libmu::allocators::bump_allocator::BumpAllocator>> [/vol/projects/mu/dist/mu-sys]
 34,862 ( 1.23%)  ???:<libmu::types::cons::ConsIter as core::iter::traits::iterator::Iterator>::next [/vol/projects/mu/dist/mu-sys]
 33,428 ( 1.18%)  ./elf/./elf/dl-tunables.c:__GI___tunables_init [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
 32,592 ( 1.15%)  ???:core::ptr::drop_in_place<<alloc::collections::vec_deque::drain::Drain<T,A> as core::ops::drop::Drop>::drop::DropGuard<futures_channel::oneshot::Sender<()>,alloc::alloc::Global>> [/vol/projects/mu/dist/mu-sys]
 31,122 ( 1.10%)  ???:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter'2 [/vol/projects/mu/dist/mu-sys]
 30,203 ( 1.07%)  ???:libmu::core::frame::Frame::apply'2 [/vol/projects/mu/dist/mu-sys]
 28,606 ( 1.01%)  ./elf/../sysdeps/x86_64/dl-machine.h:_dl_relocate_object
 27,370 ( 0.97%)  ???:<libmu::types::stream::Stream as libmu::types::stream::Core>::read_char [/vol/projects/mu/dist/mu-sys]
 26,453 ( 0.94%)  ???:libmu::core::readtable::map_char_syntax [/vol/projects/mu/dist/mu-sys]
 26,064 ( 0.92%)  ./stdlib/../stdlib/strtol_l.c:____strtoul_l_internal [/usr/lib/x86_64-linux-gnu/libc.so.6]
 24,223 ( 0.86%)  ???:<libmu::core::mu::Mu as libmu::core::mu::Core>::eval'2 [/vol/projects/mu/dist/mu-sys]
 24,181 ( 0.86%)  ???:libmu::core::namespace::Namespace::intern_symbol [/vol/projects/mu/dist/mu-sys]
 21,534 ( 0.76%)  ./elf/./elf/dl-reloc.c:_dl_relocate_object [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
 21,359 ( 0.76%)  ./elf/./elf/do-rel.h:_dl_relocate_object
 20,475 ( 0.73%)  ./elf/./elf/dl-lookup.c:_dl_lookup_symbol_x [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
 19,596 ( 0.69%)  ./string/../sysdeps/x86_64/multiarch/../multiarch/strcmp-sse2.S:strcmp [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
 19,254 ( 0.68%)  ./elf/../sysdeps/generic/dl-new-hash.h:_dl_lookup_symbol_x
 18,624 ( 0.66%)  ???:<alloc::collections::vec_deque::drain::Drain<T,A> as core::ops::drop::Drop>::drop [/vol/projects/mu/dist/mu-sys]
 17,766 ( 0.63%)  ???:<libmu::types::stream::Stream as libmu::types::stream::Core>::is_open [/vol/projects/mu/dist/mu-sys]
 17,271 ( 0.61%)  ???:libmu::core::namespace::Namespace::map_symbol [/vol/projects/mu/dist/mu-sys]
 16,920 ( 0.60%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockReadFut<alloc::vec::Vec<core::cell::RefCell<libmu::types::stream::Stream>>>> [/vol/projects/mu/dist/mu-sys]
 16,093 ( 0.57%)  ???:libmu::types::symbol::Symbol::to_image [/vol/projects/mu/dist/mu-sys]
 16,002 ( 0.57%)  ???:libmu::types::cons::Cons::car [/vol/projects/mu/dist/mu-sys]
 14,760 ( 0.52%)  ???:tikv_jemallocator::layout_to_flags [/vol/projects/mu/dist/mu-sys]
 13,659 ( 0.48%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_cache_bin_fill_small [/vol/projects/mu/dist/mu-sys]
 13,550 ( 0.48%)  ./elf/./elf/dl-lookup.c:check_match [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
 12,972 ( 0.46%)  ???:<libmu::core::frame::Frame as libmu::core::frame::MuFunction>::mu_fr_ref [/vol/projects/mu/dist/mu-sys]
 12,248 ( 0.43%)  ???:<libmu::types::cons::Cons as libmu::types::cons::Core>::evict [/vol/projects/mu/dist/mu-sys]
 12,130 ( 0.43%)  ???:hashbrown::raw::RawTable<T,A>::reserve_rehash [/vol/projects/mu/dist/mu-sys]
 11,874 ( 0.42%)  ???:<libmu::core::mu::Mu as libmu::core::funcall::Core>::fp_argv_check [/vol/projects/mu/dist/mu-sys]
 11,723 ( 0.42%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_alloc_impl [/vol/projects/mu/dist/mu-sys]
 10,300 ( 0.36%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sc.c:_rjem_je_sc_data_init [/vol/projects/mu/dist/mu-sys]
  9,601 ( 0.34%)  ???:libmu::core::namespace::Namespace::is_ns [/vol/projects/mu/dist/mu-sys]
  9,563 ( 0.34%)  ???:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold [/vol/projects/mu/dist/mu-sys]
  8,862 ( 0.31%)  ???:<libmu::core::stream::SystemStream as libmu::core::stream::Core>::read_byte [/vol/projects/mu/dist/mu-sys]
  8,660 ( 0.31%)  ???:hashbrown::map::HashMap<K,V,S,A>::insert [/vol/projects/mu/dist/mu-sys]
  8,480 ( 0.30%)  ???:<libmu::types::stream::Stream as libmu::types::stream::Core>::unread_char [/vol/projects/mu/dist/mu-sys]
  8,326 ( 0.29%)  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:__memcpy_avx_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc.so.6]
  8,065 ( 0.29%)  ./elf/./elf/dl-version.c:_dl_check_map_versions [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  7,780 ( 0.28%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_sdallocx [/vol/projects/mu/dist/mu-sys]
  7,691 ( 0.27%)  ???:<alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter [/vol/projects/mu/dist/mu-sys]
  7,424 ( 0.26%)  ./nptl/./nptl/pthread_mutex_trylock.c:pthread_mutex_trylock@@GLIBC_2.34 [/usr/lib/x86_64-linux-gnu/libc.so.6]
  7,133 ( 0.25%)  ???:<libmu::core::mu::Mu as libmu::core::system::Core>::read_stream'2 [/vol/projects/mu/dist/mu-sys]
  6,796 ( 0.24%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/ph.h:_rjem_je_edata_heap_remove_first
  6,655 ( 0.24%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_remove_first [/vol/projects/mu/dist/mu-sys]
  6,640 ( 0.24%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockReadFut<std::collections::hash::map::HashMap<u64,(libmu::core::types::Tag,futures_locks::rwlock::RwLock<std::collections::hash::map::HashMap<alloc::string::String,libmu::core::types::Tag>>)>>> [/vol/projects/mu/dist/mu-sys]
  6,540 ( 0.23%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<libmu::allocators::bump_allocator::BumpAllocator>> [/vol/projects/mu/dist/mu-sys]
  6,446 ( 0.23%)  ./elf/./elf/dl-tunables.h:__GI___tunables_init
  6,395 ( 0.23%)  ???:libmu::core::compiler::Compiler::compile'2 [/vol/projects/mu/dist/mu-sys]
  6,240 ( 0.22%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<alloc::vec::Vec<futures_locks::rwlock::RwLock<libmu::core::allocator::AllocTypeInfo>>>> [/vol/projects/mu/dist/mu-sys]
  6,220 ( 0.22%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<libmu::core::allocator::AllocTypeInfo>> [/vol/projects/mu/dist/mu-sys]
  6,160 ( 0.22%)  ???:__rust_dealloc [/vol/projects/mu/dist/mu-sys]
  6,144 ( 0.22%)  ???:<hashbrown::map::HashMap<K,V,S,A> as core::iter::traits::collect::Extend<(K,V)>>::extend [/vol/projects/mu/dist/mu-sys]
  6,023 ( 0.21%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/cache_bin.h:_rjem_malloc
  5,980 ( 0.21%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/cache_bin.h:_rjem_sdallocx
  5,980 ( 0.21%)  ???:__rust_alloc [/vol/projects/mu/dist/mu-sys]
  5,952 ( 0.21%)  ???:<libmu::core::reader::Reader as libmu::core::reader::Core>::read_atom [/vol/projects/mu/dist/mu-sys]
  5,951 ( 0.21%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/str/validations.rs:core::str::converts::from_utf8
  5,872 ( 0.21%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_new [/vol/projects/mu/dist/mu-sys]
  5,778 ( 0.20%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_boot [/vol/projects/mu/dist/mu-sys]
  5,445 ( 0.19%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_recycle [/vol/projects/mu/dist/mu-sys]
  5,400 ( 0.19%)  ???:<libmu::types::cons::Cons as libmu::types::cons::Core>::read'2 [/vol/projects/mu/dist/mu-sys]
  5,309 ( 0.19%)  ???:<libmu::types::symbol::Symbol as libmu::types::symbol::Core>::parse [/vol/projects/mu/dist/mu-sys]
  5,307 ( 0.19%)  ???:<libmu::types::vector::Vector as libmu::types::vector::Core>::as_string [/vol/projects/mu/dist/mu-sys]
  5,064 ( 0.18%)  ???:libmu::types::symbol::Symbol::new [/vol/projects/mu/dist/mu-sys]
  4,971 ( 0.18%)  ./nptl/./nptl/pthread_mutex_unlock.c:pthread_mutex_unlock@@GLIBC_2.2.5 [/usr/lib/x86_64-linux-gnu/libc.so.6]
  4,939 ( 0.17%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/bit_util.h:_rjem_je_arena_cache_bin_fill_small
  4,811 ( 0.17%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/jemalloc_internal_inlines_c.h:_rjem_malloc
  4,300 ( 0.15%)  ???:alloc::raw_vec::finish_grow [/vol/projects/mu/dist/mu-sys]
  4,151 ( 0.15%)  ???:libmu::core::frame::Frame::frame_stack_push [/vol/projects/mu/dist/mu-sys]
  4,134 ( 0.15%)  ???:libmu::core::types::Tag::data [/vol/projects/mu/dist/mu-sys]
  3,963 ( 0.14%)  ???:<libmu::types::symbol::Symbol as libmu::types::symbol::Core>::evict [/vol/projects/mu/dist/mu-sys]
  3,800 ( 0.13%)  ./libio/./libio/genops.c:_IO_sputbackc [/usr/lib/x86_64-linux-gnu/libc.so.6]
  3,708 ( 0.13%)  ./libio/./libio/iogetdelim.c:getdelim [/usr/lib/x86_64-linux-gnu/libc.so.6]
  3,485 ( 0.12%)  ???:libmu::types::function::Function::evict [/vol/projects/mu/dist/mu-sys]
  3,424 ( 0.12%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_fit [/vol/projects/mu/dist/mu-sys]
  3,379 ( 0.12%)  ???:<core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::try_fold'2 [/vol/projects/mu/dist/mu-sys]
  3,138 ( 0.11%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_init [/vol/projects/mu/dist/mu-sys]
  3,132 ( 0.11%)  ???:<libmu::types::vector::Vector as libmu::types::vector::Core>::from_string [/vol/projects/mu/dist/mu-sys]
  3,124 ( 0.11%)  ./elf/../sysdeps/x86/dl-cacheinfo.h:intel_check_word.constprop.0 [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  3,092 ( 0.11%)  ???:alloc::raw_vec::RawVec<T,A>::reserve::do_reserve_and_handle [/vol/projects/mu/dist/mu-sys]
  2,970 ( 0.11%)  ???:<libmu::types::symbol::Symbol as libmu::types::symbol::Core>::keyword [/vol/projects/mu/dist/mu-sys]
  2,840 ( 0.10%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockReadFut<std::collections::hash::map::HashMap<u64,futures_locks::rwlock::RwLock<alloc::vec::Vec<libmu::core::frame::Frame>>>>> [/vol/projects/mu/dist/mu-sys]
  2,699 ( 0.10%)  ???:libmu::core::compiler::Compiler::list'2 [/vol/projects/mu/dist/mu-sys]
  2,678 ( 0.09%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/fb.h:_rjem_je_eset_fit
  2,668 ( 0.09%)  ./nptl/./nptl/pthread_mutex_init.c:pthread_mutex_init@@GLIBC_2.2.5 [/usr/lib/x86_64-linux-gnu/libc.so.6]
  2,640 ( 0.09%)  ???:<alloc::string::String as core::fmt::Write>::write_str [/vol/projects/mu/dist/mu-sys]
  2,480 ( 0.09%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockReadFut<std::collections::hash::map::HashMap<alloc::string::String,libmu::core::types::Tag>>> [/vol/projects/mu/dist/mu-sys]
  2,413 ( 0.09%)  ./elf/../bits/stdlib-bsearch.h:intel_check_word.constprop.0
  2,404 ( 0.09%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_malloc
  2,400 ( 0.09%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/atomic.h:_rjem_je_eset_init
  2,392 ( 0.08%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_sdallocx
  2,374 ( 0.08%)  ./string/../sysdeps/x86_64/multiarch/memchr-avx2.S:__memchr_avx2 [/usr/lib/x86_64-linux-gnu/libc.so.6]
  2,298 ( 0.08%)  ./elf/./elf/dl-load.c:_dl_map_object_from_fd [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  2,291 ( 0.08%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_ralloc [/vol/projects/mu/dist/mu-sys]
  2,260 ( 0.08%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockReadFut<alloc::vec::Vec<libmu::core::frame::Frame>>> [/vol/projects/mu/dist/mu-sys]
  2,240 ( 0.08%)  ???:libmu::core::frame::Frame::frame_stack_pop [/vol/projects/mu/dist/mu-sys]
  2,238 ( 0.08%)  ???:<hashbrown::raw::RawTable<T,A> as core::ops::drop::Drop>::drop'2 [/vol/projects/mu/dist/mu-sys]
  2,204 ( 0.08%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init [/vol/projects/mu/dist/mu-sys]
  2,178 ( 0.08%)  ???:<core::str::iter::Split<P> as core::iter::traits::iterator::Iterator>::next [/vol/projects/mu/dist/mu-sys]
  2,080 ( 0.07%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/fmt/mod.rs:core::fmt::Formatter::pad [/vol/projects/mu/dist/mu-sys]
  2,041 ( 0.07%)  ???:alloc::raw_vec::RawVec<T,A>::reserve_for_push [/vol/projects/mu/dist/mu-sys]
  1,952 ( 0.07%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_split_impl.isra.0 [/vol/projects/mu/dist/mu-sys]
  1,950 ( 0.07%)  ./libio/./libio/strops.c:_IO_str_init_static_internal [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,908 ( 0.07%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_ecache_alloc_grow [/vol/projects/mu/dist/mu-sys]
  1,888 ( 0.07%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_split_interior.constprop.0 [/vol/projects/mu/dist/mu-sys]
  1,856 ( 0.07%)  ???:<alloc::collections::vec_deque::VecDeque<T,A> as core::ops::drop::Drop>::drop [/vol/projects/mu/dist/mu-sys]
  1,832 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_boot [/vol/projects/mu/dist/mu-sys]
  1,827 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:do_rallocx [/vol/projects/mu/dist/mu-sys]
  1,778 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_malloc [/vol/projects/mu/dist/mu-sys]
  1,764 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_update_edata_state
  1,750 ( 0.06%)  ./stdio-common/./stdio-common/isoc23_sscanf.c:__isoc23_sscanf [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,740 ( 0.06%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<std::collections::hash::map::HashMap<alloc::string::String,libmu::core::types::Tag>>> [/vol/projects/mu/dist/mu-sys]
  1,729 ( 0.06%)  ???:std::sys_common::once::futex::Once::call [/vol/projects/mu/dist/mu-sys]
  1,701 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_update_edata_state [/vol/projects/mu/dist/mu-sys]
  1,648 ( 0.06%)  ./string/../sysdeps/x86_64/multiarch/strlen-avx2.S:__strlen_avx2 [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,632 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_split_prepare
  1,581 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:arena_slab_alloc [/vol/projects/mu/dist/mu-sys]
  1,566 ( 0.06%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_ralloc_no_move [/vol/projects/mu/dist/mu-sys]
  1,538 ( 0.05%)  ./elf/./elf/dl-cache.c:_dl_cache_libcmp [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  1,536 ( 0.05%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pac.c:pac_alloc_real [/vol/projects/mu/dist/mu-sys]
  1,530 ( 0.05%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pa.c:_rjem_je_pa_alloc [/vol/projects/mu/dist/mu-sys]
  1,473 ( 0.05%)  ./malloc/./malloc/malloc.c:_int_malloc [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,461 ( 0.05%)  ./libio/./libio/genops.c:_IO_setb [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,408 ( 0.05%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_split_prepare [/vol/projects/mu/dist/mu-sys]
  1,408 ( 0.05%)  ???:<libmu::types::vector::Vector as libmu::types::vector::Core>::evict [/vol/projects/mu/dist/mu-sys]
  1,387 ( 0.05%)  ./elf/./elf/dl-deps.c:_dl_map_object_deps [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  1,376 ( 0.05%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_insert [/vol/projects/mu/dist/mu-sys]
  1,375 ( 0.05%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_remap
  1,354 ( 0.05%)  ./string/../sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S:__memcmp_avx2_movbe [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,343 ( 0.05%)  ./nptl/./nptl/pthread_getattr_np.c:pthread_getattr_np@@GLIBC_2.32 [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,330 ( 0.05%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/str/converts.rs:core::str::converts::from_utf8 [/vol/projects/mu/dist/mu-sys]
  1,280 ( 0.05%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pac.c:pac_alloc_impl [/vol/projects/mu/dist/mu-sys]
  1,231 ( 0.04%)  ./elf/./elf/rtld.c:dl_main [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  1,189 ( 0.04%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/thread_event.h:_rjem_malloc
  1,188 ( 0.04%)  ???:<libmu::types::cons::Cons as libmu::types::cons::MuFunction>::mu_cons [/vol/projects/mu/dist/mu-sys]
  1,180 ( 0.04%)  ./libio/./libio/genops.c:_IO_no_init [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,178 ( 0.04%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_extent_bump_alloc_post [/vol/projects/mu/dist/mu-sys]
  1,175 ( 0.04%)  ./libio/./libio/genops.c:_IO_old_init [/usr/lib/x86_64-linux-gnu/libc.so.6]
  1,164 ( 0.04%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_remap [/vol/projects/mu/dist/mu-sys]
  1,087 ( 0.04%)  ???:libmu::allocators::bump_allocator::BumpAllocator::new [/vol/projects/mu/dist/mu-sys]
  1,086 ( 0.04%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/dec2flt/mod.rs:core::num::dec2flt::<impl core::str::traits::FromStr for f32>::from_str [/vol/projects/mu/dist/mu-sys]
  1,055 ( 0.04%)  ./elf/../sysdeps/generic/ldsodefs.h:do_lookup_x
  1,054 ( 0.04%)  ./elf/./elf/dl-misc.c:_dl_name_match_p [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
  1,054 ( 0.04%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_remove [/vol/projects/mu/dist/mu-sys]
  1,053 ( 0.04%)  /cargo/registry/src/index.crates.io-6f17d22bba15001f/compiler_builtins-0.1.103/./lib/builtins/popcountdi2.c:__popcountdi2 [/vol/projects/mu/dist/mu-sys]
  1,023 ( 0.04%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/ph.h:_rjem_je_edata_heap_insert
  1,011 ( 0.04%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/mod.rs:core::num::from_str_radix [/vol/projects/mu/dist/mu-sys]
  1,000 ( 0.04%)  ./stdio-common/../include/scratch_buffer.h:__vfscanf_internal
  1,000 ( 0.04%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<alloc::vec::Vec<(u64,usize)>>> [/vol/projects/mu/dist/mu-sys]
    991 ( 0.04%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_eset_fit
    978 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_insert [/vol/projects/mu/dist/mu-sys]
    973 ( 0.03%)  ./elf/./elf/dl-load.c:_dl_map_object [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    957 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_realloc [/vol/projects/mu/dist/mu-sys]
    948 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:malloc_init_hard_a0_locked [/vol/projects/mu/dist/mu-sys]
    948 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:tcache_init.isra.0 [/vol/projects/mu/dist/mu-sys]
    945 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_emap_update_edata_state
    930 ( 0.03%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/dec2flt/parse.rs:core::num::dec2flt::parse::parse_number [/vol/projects/mu/dist/mu-sys]
    920 ( 0.03%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<alloc::vec::Vec<libmu::core::frame::Frame>>> [/vol/projects/mu/dist/mu-sys]
    876 ( 0.03%)  ./elf/./elf/get-dynamic-info.h:_dl_map_object_from_fd
    872 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_psz_quantize_ceil [/vol/projects/mu/dist/mu-sys]
    869 ( 0.03%)  ???:<libmu::core::mu::Mu as libmu::core::mu::MuFunction>::if_'2 [/vol/projects/mu/dist/mu-sys]
    866 ( 0.03%)  ???:libmu::core::dynamic::<impl libmu::core::mu::Mu>::dynamic_push [/vol/projects/mu/dist/mu-sys]
    864 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:extent_split_impl.isra.0
    861 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin_info.c:_rjem_je_bin_info_boot [/vol/projects/mu/dist/mu-sys]
    844 ( 0.03%)  ./elf/../sysdeps/generic/dl-protected.h:do_lookup_x
    843 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_register_interior
    841 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_arena_ralloc_no_move
    836 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:base_extent_bump_alloc_post
    836 ( 0.03%)  ???:<libmu::core::mu::Mu as libmu::core::mu::MuFunction>::mu_apply'2 [/vol/projects/mu/dist/mu-sys]
    814 ( 0.03%)  ???:<libmu::types::fixnum::Fixnum as libmu::types::fixnum::MuFunction>::mu_fxdiv [/vol/projects/mu/dist/mu-sys]
    803 ( 0.03%)  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms [/usr/lib/x86_64-linux-gnu/libc.so.6]
    794 ( 0.03%)  ???:<getopt::parser::Parser as core::iter::traits::iterator::Iterator>::next [/vol/projects/mu/dist/mu-sys]
    772 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/mutex.h:extent_recycle
    772 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_sz_psz_quantize_ceil
    771 ( 0.03%)  ./malloc/./malloc/malloc.c:ptmalloc_init.part.0
    769 ( 0.03%)  ./elf/./elf/dl-minimal-malloc.c:__minimal_malloc [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    756 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_sz_psz_quantize_floor
    754 ( 0.03%)  ./nptl/./nptl/pthread_mutexattr_settype.c:pthread_mutexattr_settype@@GLIBC_2.34 [/usr/lib/x86_64-linux-gnu/libc.so.6]
    754 ( 0.03%)  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:mempcpy [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    750 ( 0.03%)  ./stdio-common/../libio/strfile.h:__isoc23_sscanf
    743 ( 0.03%)  ./elf/./elf/dl-object.c:_dl_new_object [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    741 ( 0.03%)  ./elf/./elf/dl-tunables.c:__tunable_get_val [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    726 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_stashed [/vol/projects/mu/dist/mu-sys]
    720 ( 0.03%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin.c:_rjem_je_bin_init [/vol/projects/mu/dist/mu-sys]
    704 ( 0.02%)  ???:core::ptr::drop_in_place<libmu::types::vector::Vector> [/vol/projects/mu/dist/mu-sys]
    702 ( 0.02%)  ./nptl/./nptl/libc-cleanup.c:__libc_cleanup_push_defer [/usr/lib/x86_64-linux-gnu/libc.so.6]
    700 ( 0.02%)  ???:mu_sys::options [/vol/projects/mu/dist/mu-sys]
    696 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/tsd.h:do_rallocx
    693 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata_cache.c:_rjem_je_edata_cache_get [/vol/projects/mu/dist/mu-sys]
    685 ( 0.02%)  ./elf/../sysdeps/generic/ldsodefs.h:_dl_relocate_object
    680 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_arena_choose_hard [/vol/projects/mu/dist/mu-sys]
    672 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_split_commit [/vol/projects/mu/dist/mu-sys]
    672 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_ecache_alloc [/vol/projects/mu/dist/mu-sys]
    667 ( 0.02%)  ???:libmu::types::symbol::Symbol::value [/vol/projects/mu/dist/mu-sys]
    662 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/cache_bin.h:_rjem_je_arena_ralloc
    662 ( 0.02%)  ???:alloc::sync::Arc<T,A>::drop_slow'2 [/vol/projects/mu/dist/mu-sys]
    660 ( 0.02%)  ???:<libmu::core::mu::Mu as libmu::core::mu::Core>::apply'2 [/vol/projects/mu/dist/mu-sys]
    660 ( 0.02%)  ???:<libmu::types::fixnum::Fixnum as libmu::types::fixnum::MuFunction>::mu_fxmul [/vol/projects/mu/dist/mu-sys]
    651 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_new [/vol/projects/mu/dist/mu-sys]
    649 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/mutex.h:_rjem_je_arena_cache_bin_fill_small
    649 ( 0.02%)  ???:<libmu::types::fixnum::Fixnum as libmu::types::fixnum::MuFunction>::mu_fxsub [/vol/projects/mu/dist/mu-sys]
    646 ( 0.02%)  ./elf/./elf/dl-cache.c:_dl_load_cache_lookup [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    646 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/util.h:_rjem_je_arena_cache_bin_fill_small
    644 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_register_interior [/vol/projects/mu/dist/mu-sys]
    636 ( 0.02%)  ???:libmu::core::compiler::Compiler::lambda [/vol/projects/mu/dist/mu-sys]
    630 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_psz_quantize_floor [/vol/projects/mu/dist/mu-sys]
    628 ( 0.02%)  ???:futures_locks::rwlock::RwLock<T>::new [/vol/projects/mu/dist/mu-sys]
    625 ( 0.02%)  ???:libmu::core::dynamic::<impl libmu::core::mu::Mu>::dynamic_pop [/vol/projects/mu/dist/mu-sys]
    615 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c:_rjem_je_cache_bin_init [/vol/projects/mu/dist/mu-sys]
    609 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/hook.c:_rjem_je_hook_invoke_alloc [/vol/projects/mu/dist/mu-sys]
    609 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/hook.c:_rjem_je_hook_invoke_dalloc [/vol/projects/mu/dist/mu-sys]
    608 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_eset_insert
    600 ( 0.02%)  ./stdio-common/./stdio-common/printf-parse.h:__vfscanf_internal
    598 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/thread_event.h:_rjem_sdallocx
    598 ( 0.02%)  ???:libmu::core::direct::DirectTag::cdr [/vol/projects/mu/dist/mu-sys]
    594 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_alloc_small_hard [/vol/projects/mu/dist/mu-sys]
    590 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_boot [/vol/projects/mu/dist/mu-sys]
    589 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_eset_remove
    580 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:_rjem_je_arena_ralloc
    580 ( 0.02%)  ???:core::ptr::drop_in_place<std::sync::mutex::Mutex<futures_locks::rwlock::RwLockData>> [/vol/projects/mu/dist/mu-sys]
    576 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:_rjem_je_emap_split_commit
    561 ( 0.02%)  ./elf/../sysdeps/x86/dl-cacheinfo.h:handle_intel.constprop.0 [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    528 ( 0.02%)  ./string/../sysdeps/x86_64/multiarch/../multiarch/strchr-sse2.S:index [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    528 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_alloc_edata [/vol/projects/mu/dist/mu-sys]
    521 ( 0.02%)  /usr/include/x86_64-linux-gnu/bits/string_fortified.h:_rjem_je_pages_boot
    508 ( 0.02%)  ./misc/../sysdeps/unix/sysv/linux/mmap64.c:mmap [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    505 ( 0.02%)  ???:<libmu::core::mu::Mu as libmu::core::mu::Core>::new [/vol/projects/mu/dist/mu-sys]
    502 ( 0.02%)  ./string/../sysdeps/x86_64/multiarch/../multiarch/strlen-sse2.S:strlen [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    500 ( 0.02%)  ./stdlib/../stdlib/strtol.c:__strtoul_internal [/usr/lib/x86_64-linux-gnu/libc.so.6]
    500 ( 0.02%)  ???:core::ptr::drop_in_place<futures_locks::rwlock::RwLockWriteFut<std::collections::hash::map::HashMap<u64,futures_locks::rwlock::RwLock<alloc::vec::Vec<libmu::core::frame::Frame>>>>> [/vol/projects/mu/dist/mu-sys]
    496 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_first [/vol/projects/mu/dist/mu-sys]
    496 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_remove [/vol/projects/mu/dist/mu-sys]
    496 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/nstime.c:_rjem_je_nstime_copy [/vol/projects/mu/dist/mu-sys]
    489 ( 0.02%)  ./elf/./elf/dl-hwcaps_split.c:_dl_hwcaps_split_masked [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    486 ( 0.02%)  ./nptl/./nptl/libc-cleanup.c:__libc_cleanup_pop_restore [/usr/lib/x86_64-linux-gnu/libc.so.6]
    480 ( 0.02%)  ./elf/./elf/dl-runtime.c:_dl_fixup [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    474 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bitmap.c:_rjem_je_bitmap_init [/vol/projects/mu/dist/mu-sys]
    464 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:_rjem_je_arena_ralloc_no_move
    464 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/thread_event.h:do_rallocx
    464 ( 0.02%)  ???:__rust_realloc [/vol/projects/mu/dist/mu-sys]
    462 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/cache_bin.h:_rjem_je_arena_cache_bin_fill_small
    459 ( 0.02%)  ./malloc/./malloc/malloc.c:_int_free [/usr/lib/x86_64-linux-gnu/libc.so.6]
    459 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/mutex.h:base_alloc_impl
    448 ( 0.02%)  ./elf/./elf/dl-environ.c:_dl_next_ld_env_entry [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    448 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_eset_insert
    446 ( 0.02%)  ./stdlib/./stdlib/getenv.c:getenv [/usr/lib/x86_64-linux-gnu/libc.so.6]
    435 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/jemalloc_internal_inlines_c.h:do_rallocx
    435 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h:do_rallocx
    434 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_extent_commit_zero [/vol/projects/mu/dist/mu-sys]
    431 ( 0.02%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/mutex.h:_rjem_je_edata_cache_get
    430 ( 0.02%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/dec2flt/common.rs:core::num::dec2flt::parse::parse_number
    424 ( 0.02%)  ./string/../sysdeps/x86_64/multiarch/../multiarch/memset-vec-unaligned-erms.S:memset [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    418 ( 0.01%)  ./elf/./elf/dl-load.c:open_path [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    418 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/mutex.h:_rjem_je_ecache_alloc_grow
    416 ( 0.01%)  /usr/include/x86_64-linux-gnu/bits/string_fortified.h:_rjem_je_decay_init
    416 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_emap_split_prepare
    415 ( 0.01%)  ./elf/./dl-map-segments.h:_dl_map_object_from_fd
    400 ( 0.01%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/fmt/mod.rs:<str as core::fmt::Display>::fmt [/vol/projects/mu/dist/mu-sys]
    396 ( 0.01%)  ./elf/./elf/dl-load.c:open_verify.constprop.0 [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    387 ( 0.01%)  ???:getopt::parser::Parser::new [/vol/projects/mu/dist/mu-sys]
    386 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_new [/vol/projects/mu/dist/mu-sys]
    382 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/bit_util.h:_rjem_je_eset_fit
    372 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_eset_remove
    366 ( 0.01%)  ./elf/./get-dynamic-info.h:dl_main
    360 ( 0.01%)  ???:libmu::core::compiler::Compiler::lambda'2 [/vol/projects/mu/dist/mu-sys]
    355 ( 0.01%)  ???:<libmu::core::types::Tag as libmu::core::types::MuFunction>::mu_eq [/vol/projects/mu/dist/mu-sys]
    354 ( 0.01%)  ???:__cpu_indicator_init [/usr/lib/x86_64-linux-gnu/libgcc_s.so.1]
    350 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_malloc_default [/vol/projects/mu/dist/mu-sys]
    348 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/nstime.h:_rjem_je_malloc_mutex_init
    346 ( 0.01%)  ???:libmu::types::cons::Cons::length [/vol/projects/mu/dist/mu-sys]
    341 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/ph.h:_rjem_je_edata_heap_remove
    335 ( 0.01%)  ./elf/../elf/dl-tls.c:_dl_allocate_tls_storage [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    332 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_emap_remap
    322 ( 0.01%)  ???:<libmu::types::cons::Cons as libmu::types::cons::Core>::nth [/vol/projects/mu/dist/mu-sys]
    321 ( 0.01%)  ./elf/../sysdeps/x86/dl-prop.h:_dl_map_object_from_fd
    320 ( 0.01%)  ./elf/./dl-find_object.h:_dl_find_object_from_map [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    320 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_pa_alloc
    314 ( 0.01%)  ./elf/./elf/dl-load.c:_dl_init_paths [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    308 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/tcache_inlines.h:_rjem_je_malloc_default
    306 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_edata_heap_insert
    291 ( 0.01%)  ./elf/../sysdeps/unix/sysv/linux/dl-sysdep.c:_dl_sysdep_parse_arguments [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    291 ( 0.01%)  ???:libmu::core::compiler::Compiler::if_ [/vol/projects/mu/dist/mu-sys]
    290 ( 0.01%)  /usr/include/x86_64-linux-gnu/bits/string_fortified.h:_rjem_je_malloc_mutex_init
    290 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:do_rallocx
    289 ( 0.01%)  ???:<libmu::core::stream::SystemStream as libmu::core::stream::Core>::open_std_stream [/vol/projects/mu/dist/mu-sys]
    288 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/div.c:_rjem_je_div_init [/vol/projects/mu/dist/mu-sys]
    287 ( 0.01%)  ???:libmu::core::namespace::Namespace::add_ns [/vol/projects/mu/dist/mu-sys]
    279 ( 0.01%)  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:memcpy [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    279 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/atomic.h:_rjem_je_eset_remove
    277 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:malloc_conf_init_helper [/vol/projects/mu/dist/mu-sys]
    265 ( 0.01%)  ./elf/./elf/dl-lookup-direct.c:_dl_lookup_direct [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    263 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/tcache_inlines.h:_rjem_je_arena_ralloc
    256 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/atomic.h:_rjem_je_eset_insert
    256 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/bit_util.h:_rjem_je_sz_psz_quantize_ceil
    256 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/pai.h:_rjem_je_pa_alloc
    254 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h:base_alloc_impl
    252 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/bit_util.h:_rjem_je_sz_psz_quantize_floor
    252 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_empty [/vol/projects/mu/dist/mu-sys]
    251 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:base_alloc_impl
    248 ( 0.01%)  ./elf/./elf/dl-minimal.c:strsep [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    248 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:arena_slab_alloc
    248 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/ph.h:_rjem_je_edata_heap_first
    242 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/arena_inlines_b.h:_rjem_je_arena_cache_bin_fill_small
    240 ( 0.01%)  ./elf/./elf/dl-catch.c:_dl_catch_exception [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    239 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/tsd.h:_rjem_je_malloc_default
    238 ( 0.01%)  ./string/../string/strcspn.c:strcspn [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    237 ( 0.01%)  ./libio/./libio/fileops.c:_IO_file_underflow@@GLIBC_2.2.5 [/usr/lib/x86_64-linux-gnu/libc.so.6]
    232 ( 0.01%)  ./nptl/../string/bits/string_fortified.h:pthread_mutex_init@@GLIBC_2.2.5
    232 ( 0.01%)  ./nptl/./nptl/pthread_mutexattr_init.c:pthread_mutexattr_init@@GLIBC_2.34 [/usr/lib/x86_64-linux-gnu/libc.so.6]
    231 ( 0.01%)  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/mod.rs:core::num::<impl core::str::traits::FromStr for i64>::from_str [/vol/projects/mu/dist/mu-sys]
    230 ( 0.01%)  ./elf/./get-dynamic-info.h:_dl_start
    226 ( 0.01%)  ./elf/./elf/dl-init.c:call_init.part.0 [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    225 ( 0.01%)  ./elf/../sysdeps/x86/dl-cacheinfo.h:get_common_cache_info.constprop.0 [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    225 ( 0.01%)  ./elf/./elf/dl-tunables.c:__GI___tunable_set_val [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    221 ( 0.01%)  ./elf/../sysdeps/x86/dl-cacheinfo.h:init_cpu_features.constprop.0
    220 ( 0.01%)  ./elf/./elf/dl-hwcaps.c:_dl_important_hwcaps [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    220 ( 0.01%)  ./posix/../sysdeps/unix/sysv/linux/x86/sysconf.c:sysconf [/usr/lib/x86_64-linux-gnu/libc.so.6]
    218 ( 0.01%)  ./malloc/./malloc/malloc.c:malloc [/usr/lib/x86_64-linux-gnu/libc.so.6]
    216 ( 0.01%)  /usr/include/x86_64-linux-gnu/bits/string_fortified.h:_rjem_je_bin_init
    215 ( 0.01%)  ???:mu_sys::main [/vol/projects/mu/dist/mu-sys]
    213 ( 0.01%)  /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h:_rjem_je_arena_cache_bin_fill_small
    207 ( 0.01%)  ./malloc/./malloc/malloc.c:calloc [/usr/lib/x86_64-linux-gnu/libc.so.6]
    206 ( 0.01%)  ./elf/./elf/dl-find_object.c:_dlfo_process_initial [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]
    205 ( 0.01%)  ./elf/../sysdeps/x86_64/dl-trampoline.h:_dl_runtime_resolve_xsave [/usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2]

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c
--------------------------------------------------------------------------------
Ir             

-- line 2 ----------------------------------------
    .           #include "jemalloc/internal/jemalloc_internal_includes.h"
    .           #include "jemalloc/internal/sz.h"
    .           
    .           JEMALLOC_ALIGNED(CACHELINE)
    .           size_t sz_pind2sz_tab[SC_NPSIZES+1];
    .           size_t sz_large_pad;
    .           
    .           size_t
   63 ( 0.00%)  sz_psz_quantize_floor(size_t size) {
    .           	size_t ret;
    .           	pszind_t pind;
    .           
    .           	assert(size > 0);
    .           	assert((size & PAGE_MASK) == 0);
    .           
  508 ( 0.02%)  	pind = sz_psz2ind(size - sz_large_pad + 1);
   92 ( 0.00%)  	if (pind == 0) {
    .           		/*
    .           		 * Avoid underflow.  This short-circuit would also do the right
    .           		 * thing for all sizes in the range for which there are
    .           		 * PAGE-spaced size classes, but it's simplest to just handle
    .           		 * the one case that would cause erroneous results.
    .           		 */
    .           		return size;
    .           	}
  378 ( 0.01%)  	ret = sz_pind2sz(pind - 1) + sz_large_pad;
    .           	assert(ret <= size);
    .           	return ret;
   63 ( 0.00%)  }
    .           
    .           size_t
  128 ( 0.00%)  sz_psz_quantize_ceil(size_t size) {
    .           	size_t ret;
    .           
    .           	assert(size > 0);
    .           	assert(size - sz_large_pad <= SC_LARGE_MAXCLASS);
    .           	assert((size & PAGE_MASK) == 0);
    .           
   46 ( 0.00%)  	ret = sz_psz_quantize_floor(size);
   96 ( 0.00%)  	if (ret < size) {
    .           		/*
    .           		 * Skip a quantization that may have an adequately large extent,
    .           		 * because under-sized extents may be mixed in.  This only
    .           		 * happens when an unusual size is requested, i.e. for aligned
    .           		 * allocation, and is just one of several places where linear
    .           		 * search would potentially find sufficiently aligned available
    .           		 * memory somewhere lower.
    .           		 */
    .           		ret = sz_pind2sz(sz_psz2ind(ret - sz_large_pad + 1)) +
    .           		    sz_large_pad;
    .           	}
    .           	return ret;
  128 ( 0.00%)  }
    .           
    .           static void
    .           sz_boot_pind2sz_tab(const sc_data_t *sc_data) {
    1 ( 0.00%)  	int pind = 0;
  117 ( 0.00%)  	for (unsigned i = 0; i < SC_NSIZES; i++) {
    .           		const sc_t *sc = &sc_data->sc[i];
  464 ( 0.02%)  		if (sc->psz) {
  798 ( 0.03%)  			sz_pind2sz_tab[pind] = (ZU(1) << sc->lg_base)
  995 ( 0.04%)  			    + (ZU(sc->ndelta) << sc->lg_delta);
  199 ( 0.01%)  			pind++;
    .           		}
    .           	}
    3 ( 0.00%)  	for (int i = pind; i <= (int)SC_NPSIZES; i++) {
    4 ( 0.00%)  		sz_pind2sz_tab[pind] = sc_data->large_maxclass + PAGE;
    .           	}
    .           }
    .           
    .           JEMALLOC_ALIGNED(CACHELINE)
    .           size_t sz_index2size_tab[SC_NSIZES];
    .           
    .           static void
    .           sz_boot_index2size_tab(const sc_data_t *sc_data) {
  117 ( 0.00%)  	for (unsigned i = 0; i < SC_NSIZES; i++) {
    .           		const sc_t *sc = &sc_data->sc[i];
  699 ( 0.02%)  		sz_index2size_tab[i] = (ZU(1) << sc->lg_base)
1,160 ( 0.04%)  		    + (ZU(sc->ndelta) << (sc->lg_delta));
    .           	}
    .           }
    .           
    .           /*
    .            * To keep this table small, we divide sizes by the tiny min size, which gives
    .            * the smallest interval for which the result can change.
    .            */
    .           JEMALLOC_ALIGNED(CACHELINE)
    .           uint8_t sz_size2index_tab[(SC_LOOKUP_MAXCLASS >> SC_LG_TINY_MIN) + 1];
    .           
    .           static void
    .           sz_boot_size2index_tab(const sc_data_t *sc_data) {
    .           	size_t dst_max = (SC_LOOKUP_MAXCLASS >> SC_LG_TINY_MIN) + 1;
    1 ( 0.00%)  	size_t dst_ind = 0;
  117 ( 0.00%)  	for (unsigned sc_ind = 0; sc_ind < SC_NSIZES && dst_ind < dst_max;
   29 ( 0.00%)  	    sc_ind++) {
    .           		const sc_t *sc = &sc_data->sc[sc_ind];
  117 ( 0.00%)  		size_t sz = (ZU(1) << sc->lg_base)
  203 ( 0.01%)  		    + (ZU(sc->ndelta) << sc->lg_delta);
  116 ( 0.00%)  		size_t max_ind = ((sz + (ZU(1) << SC_LG_TINY_MIN) - 1)
    .           				   >> SC_LG_TINY_MIN);
   58 ( 0.00%)  		for (; dst_ind <= max_ind && dst_ind < dst_max; dst_ind++) {
  567 ( 0.02%)  			sz_size2index_tab[dst_ind] = sc_ind;
    .           		}
    .           	}
    .           }
    .           
    .           void
    5 ( 0.00%)  sz_boot(const sc_data_t *sc_data, bool cache_oblivious) {
    5 ( 0.00%)  	sz_large_pad = cache_oblivious ? PAGE : 0;
    .           	sz_boot_pind2sz_tab(sc_data);
    .           	sz_boot_index2size_tab(sc_data);
    .           	sz_boot_size2index_tab(sc_data);
    3 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c
--------------------------------------------------------------------------------
Ir             

-- line 290 ----------------------------------------
    .           	if (unlikely(malloc_init_state == malloc_init_uninitialized)) {
    .           		return malloc_init_hard_a0();
    .           	}
    .           	return false;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           malloc_init(void) {
    4 ( 0.00%)  	if (unlikely(!malloc_initialized()) && malloc_init_hard()) {
57,861 ( 2.05%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:malloc_init_hard (1x)
    .           		return true;
    .           	}
    .           	return false;
    .           }
    .           
    .           /*
    .            * The a0*() functions are used instead of i{d,}alloc() in situations that
    .            * cannot tolerate TLS variable access.
-- line 306 ----------------------------------------
-- line 364 ----------------------------------------
    .           	if (unlikely(ptr == NULL)) {
    .           		return;
    .           	}
    .           
    .           	a0idalloc(ptr, false);
    .           }
    .           
    .           void
    1 ( 0.00%)  arena_set(unsigned ind, arena_t *arena) {
    .           	atomic_store_p(&arenas[ind], arena, ATOMIC_RELEASE);
    1 ( 0.00%)  }
    .           
    .           static void
    .           narenas_total_set(unsigned narenas) {
    .           	atomic_store_u(&narenas_total, narenas, ATOMIC_RELEASE);
    .           }
    .           
    .           static void
    .           narenas_total_inc(void) {
    .           	atomic_fetch_add_u(&narenas_total, 1, ATOMIC_RELEASE);
    2 ( 0.00%)  }
    .           
    .           unsigned
    1 ( 0.00%)  narenas_total_get(void) {
    .           	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
    1 ( 0.00%)  }
    .           
    .           /* Create a new arena and insert it into the arenas array at index ind. */
    .           static arena_t *
    .           arena_init_locked(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
    .           	arena_t *arena;
    .           
    .           	assert(ind <= narenas_total_get());
    2 ( 0.00%)  	if (ind >= MALLOCX_ARENA_LIMIT) {
    .           		return NULL;
    .           	}
    2 ( 0.00%)  	if (ind == narenas_total_get()) {
    .           		narenas_total_inc();
    .           	}
    .           
    .           	/*
    .           	 * Another thread may have already initialized arenas[ind] if it's an
    .           	 * auto arena.
    .           	 */
    .           	arena = arena_get(tsdn, ind, false);
    .           	if (arena != NULL) {
    .           		assert(arena_is_auto(arena));
    .           		return arena;
    .           	}
    .           
    .           	/* Actually initialize the arena. */
    5 ( 0.00%)  	arena = arena_new(tsdn, ind, config);
19,173 ( 0.68%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_new (1x)
    .           
    1 ( 0.00%)  	return arena;
    .           }
    .           
    .           static void
    .           arena_new_create_background_thread(tsdn_t *tsdn, unsigned ind) {
    2 ( 0.00%)  	if (ind == 0) {
    .           		return;
    .           	}
    .           	/*
    .           	 * Avoid creating a new background thread just for the huge arena, which
    .           	 * purges eagerly by default.
    .           	 */
    .           	if (have_background_thread && !arena_is_huge(ind)) {
    .           		if (background_thread_create(tsdn_tsd(tsdn), ind)) {
-- line 430 ----------------------------------------
-- line 431 ----------------------------------------
    .           			malloc_printf("<jemalloc>: error in background thread "
    .           				      "creation for arena %u. Abort.\n", ind);
    .           			abort();
    .           		}
    .           	}
    .           }
    .           
    .           arena_t *
    9 ( 0.00%)  arena_init(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
    .           	arena_t *arena;
    .           
    .           	malloc_mutex_lock(tsdn, &arenas_lock);
    .           	arena = arena_init_locked(tsdn, ind, config);
    .           	malloc_mutex_unlock(tsdn, &arenas_lock);
    .           
    .           	arena_new_create_background_thread(tsdn, ind);
    .           
    .           	return arena;
    7 ( 0.00%)  }
    .           
    .           static void
    .           arena_bind(tsd_t *tsd, unsigned ind, bool internal) {
    .           	arena_t *arena = arena_get(tsd_tsdn(tsd), ind, false);
    6 ( 0.00%)  	arena_nthreads_inc(arena, internal);
    8 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_nthreads_inc (2x)
    .           
    4 ( 0.00%)  	if (internal) {
    .           		tsd_iarena_set(tsd, arena);
    .           	} else {
    .           		tsd_arena_set(tsd, arena);
    .           		unsigned shard = atomic_fetch_add_u(&arena->binshard_next, 1,
    .           		    ATOMIC_RELAXED);
    .           		tsd_binshards_t *bins = tsd_binshardsp_get(tsd);
   20 ( 0.00%)  		for (unsigned i = 0; i < SC_NBINS; i++) {
    .           			assert(bin_infos[i].n_shards > 0 &&
    .           			    bin_infos[i].n_shards <= BIN_SHARDS_MAX);
  144 ( 0.01%)  			bins->binshard[i] = shard % bin_infos[i].n_shards;
    .           		}
    .           	}
    .           }
    .           
    .           void
    .           arena_migrate(tsd_t *tsd, arena_t *oldarena, arena_t *newarena) {
    .           	assert(oldarena != NULL);
    .           	assert(newarena != NULL);
-- line 474 ----------------------------------------
-- line 495 ----------------------------------------
    .           		tsd_iarena_set(tsd, NULL);
    .           	} else {
    .           		tsd_arena_set(tsd, NULL);
    .           	}
    .           }
    .           
    .           /* Slow path, called only by arena_choose(). */
    .           arena_t *
   13 ( 0.00%)  arena_choose_hard(tsd_t *tsd, bool internal) {
    1 ( 0.00%)  	arena_t *ret JEMALLOC_CC_SILENCE_INIT(NULL);
    .           
    3 ( 0.00%)  	if (have_percpu_arena && PERCPU_ARENA_ENABLED(opt_percpu_arena)) {
    .           		unsigned choose = percpu_arena_choose();
    .           		ret = arena_get(tsd_tsdn(tsd), choose, true);
    .           		assert(ret != NULL);
    .           		arena_bind(tsd, arena_ind_get(ret), false);
    .           		arena_bind(tsd, arena_ind_get(ret), true);
    .           
    .           		return ret;
    .           	}
    .           
    3 ( 0.00%)  	if (narenas_auto > 1) {
    .           		unsigned i, j, choose[2], first_null;
    .           		bool is_new_arena[2];
    .           
    .           		/*
    .           		 * Determine binding for both non-internal and internal
    .           		 * allocation.
    .           		 *
    .           		 *   choose[0]: For application allocation.
    .           		 *   choose[1]: For internal metadata allocation.
    .           		 */
    .           
    .           		for (j = 0; j < 2; j++) {
    1 ( 0.00%)  			choose[j] = 0;
    2 ( 0.00%)  			is_new_arena[j] = false;
    .           		}
    .           
    .           		first_null = narenas_auto;
    .           		malloc_mutex_lock(tsd_tsdn(tsd), &arenas_lock);
    .           		assert(arena_get(tsd_tsdn(tsd), 0, false) != NULL);
  259 ( 0.01%)  		for (i = 1; i < narenas_auto; i++) {
    .           			if (arena_get(tsd_tsdn(tsd), i, false) != NULL) {
    .           				/*
    .           				 * Choose the first arena that has the lowest
    .           				 * number of threads assigned to it.
    .           				 */
    .           				for (j = 0; j < 2; j++) {
    .           					if (arena_nthreads_get(arena_get(
    .           					    tsd_tsdn(tsd), i, false), !!j) <
    .           					    arena_nthreads_get(arena_get(
    .           					    tsd_tsdn(tsd), choose[j], false),
    .           					    !!j)) {
    .           						choose[j] = i;
    .           					}
    .           				}
   63 ( 0.00%)  			} else if (first_null == narenas_auto) {
    .           				/*
    .           				 * Record the index of the first uninitialized
    .           				 * arena, in case all extant arenas are in use.
    .           				 *
    .           				 * NB: It is possible for there to be
    .           				 * discontinuities in terms of initialized
    .           				 * versus uninitialized arenas, due to the
    .           				 * "thread.arena" mallctl.
    .           				 */
  126 ( 0.00%)  				first_null = i;
    .           			}
    .           		}
    .           
    .           		for (j = 0; j < 2; j++) {
   15 ( 0.00%)  			if (arena_nthreads_get(arena_get(tsd_tsdn(tsd),
    8 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_nthreads_get (2x)
    .           			    choose[j], false), !!j) == 0 || first_null ==
    .           			    narenas_auto) {
    .           				/*
    .           				 * Use an unloaded arena, or the least loaded
    .           				 * arena if all arenas are already initialized.
    .           				 */
    4 ( 0.00%)  				if (!!j == internal) {
    .           					ret = arena_get(tsd_tsdn(tsd),
    .           					    choose[j], false);
    .           				}
    .           			} else {
    .           				arena_t *arena;
    .           
    .           				/* Initialize a new arena. */
    .           				choose[j] = first_null;
-- line 581 ----------------------------------------
-- line 591 ----------------------------------------
    .           					ret = arena;
    .           				}
    .           			}
    .           			arena_bind(tsd, choose[j], !!j);
    .           		}
    .           		malloc_mutex_unlock(tsd_tsdn(tsd), &arenas_lock);
    .           
    .           		for (j = 0; j < 2; j++) {
    4 ( 0.00%)  			if (is_new_arena[j]) {
    .           				assert(choose[j] > 0);
    .           				arena_new_create_background_thread(
    .           				    tsd_tsdn(tsd), choose[j]);
    .           			}
    .           		}
    .           
    .           	} else {
    .           		ret = arena_get(tsd_tsdn(tsd), 0, false);
    .           		arena_bind(tsd, 0, false);
    .           		arena_bind(tsd, 0, true);
    .           	}
    .           
    .           	return ret;
   12 ( 0.00%)  }
    .           
    .           void
    .           iarena_cleanup(tsd_t *tsd) {
    .           	arena_t *iarena;
    .           
    .           	iarena = tsd_iarena_get(tsd);
    .           	if (iarena != NULL) {
    .           		arena_unbind(tsd, arena_ind_get(iarena), true);
-- line 621 ----------------------------------------
-- line 697 ----------------------------------------
    .           /******************************************************************************/
    .           /*
    .            * Begin initialization functions.
    .            */
    .           
    .           static char *
    .           jemalloc_secure_getenv(const char *name) {
    .           #ifdef JEMALLOC_HAVE_SECURE_GETENV
    4 ( 0.00%)  	return secure_getenv(name);
  504 ( 0.02%)  => ./stdlib/./stdlib/secure-getenv.c:secure_getenv (1x)
    .           #else
    .           #  ifdef JEMALLOC_HAVE_ISSETUGID
    .           	if (issetugid() != 0) {
    .           		return NULL;
    .           	}
    .           #  endif
    .           	return getenv(name);
    .           #endif
-- line 713 ----------------------------------------
-- line 734 ----------------------------------------
    .           	 */
    .           	{
    .           #  if defined(__FreeBSD__) || defined(__DragonFly__)
    .           		cpuset_t set;
    .           #  else
    .           		cpu_set_t set;
    .           #  endif
    .           #  if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
    6 ( 0.00%)  		sched_getaffinity(0, sizeof(set), &set);
   43 ( 0.00%)  => ./posix/../sysdeps/unix/sysv/linux/sched_getaffinity.c:sched_getaffinity@@GLIBC_2.3.4 (1x)
    .           #  else
    .           		pthread_getaffinity_np(pthread_self(), sizeof(set), &set);
    .           #  endif
    4 ( 0.00%)  		result = CPU_COUNT(&set);
  170 ( 0.01%)  => ./posix/./posix/sched_cpucount.c:__sched_cpucount (1x)
    .           	}
    .           #else
    .           	result = sysconf(_SC_NPROCESSORS_ONLN);
    .           #endif
    3 ( 0.00%)  	return ((result == -1) ? 1 : (unsigned)result);
    .           }
    .           
    .           /*
    .            * Ensure that number of CPUs is determistinc, i.e. it is the same based on:
    .            * - sched_getaffinity()
    .            * - _SC_NPROCESSORS_ONLN
    .            * - _SC_NPROCESSORS_CONF
    .            * Since otherwise tricky things is possible with percpu arenas in use.
-- line 759 ----------------------------------------
-- line 959 ----------------------------------------
    .           }
    .           
    .           static void
    .           malloc_slow_flag_init(void) {
    .           	/*
    .           	 * Combine the runtime options into malloc_slow for fast path.  Called
    .           	 * after processing all the options.
    .           	 */
    1 ( 0.00%)  	malloc_slow_flags |= (opt_junk_alloc ? flag_opt_junk_alloc : 0)
    2 ( 0.00%)  	    | (opt_junk_free ? flag_opt_junk_free : 0)
    4 ( 0.00%)  	    | (opt_zero ? flag_opt_zero : 0)
    3 ( 0.00%)  	    | (opt_utrace ? flag_opt_utrace : 0)
    3 ( 0.00%)  	    | (opt_xmalloc ? flag_opt_xmalloc : 0);
    .           
    1 ( 0.00%)  	malloc_slow = (malloc_slow_flags != 0);
    .           }
    .           
    .           /* Number of sources for initializing malloc_conf */
    .           #define MALLOC_CONF_NSOURCES 5
    .           
    .           static const char *
    .           obtain_malloc_conf(unsigned which_source, char buf[PATH_MAX + 1]) {
    .           	if (config_debug) {
-- line 981 ----------------------------------------
-- line 984 ----------------------------------------
    .           		 * Each source should only be read once, to minimize # of
    .           		 * syscalls on init.
    .           		 */
    .           		assert(read_source++ == which_source);
    .           	}
    .           	assert(which_source < MALLOC_CONF_NSOURCES);
    .           
    .           	const char *ret;
   28 ( 0.00%)  	switch (which_source) {
    .           	case 0:
    .           		ret = config_malloc_conf;
    .           		break;
    .           	case 1:
    2 ( 0.00%)  		if (je_malloc_conf != NULL) {
    .           			/* Use options that were compiled into the program. */
    .           			ret = je_malloc_conf;
    .           		} else {
    .           			/* No configuration specified. */
    .           			ret = NULL;
    .           		}
    .           		break;
    .           	case 2: {
    .           		ssize_t linklen = 0;
    .           #ifndef _WIN32
    3 ( 0.00%)  		int saved_errno = errno;
    5 ( 0.00%)  => ???:0x000000000011b2c0 (1x)
    .           		const char *linkname =
    .           #  ifdef JEMALLOC_PREFIX
    .           		    "/etc/"JEMALLOC_PREFIX"malloc.conf"
    .           #  else
    .           		    "/etc/malloc.conf"
    .           #  endif
    .           		    ;
    .           
-- line 1016 ----------------------------------------
-- line 1018 ----------------------------------------
    .           		 * Try to use the contents of the "/etc/malloc.conf" symbolic
    .           		 * link's name.
    .           		 */
    .           #ifndef JEMALLOC_READLINKAT
    .           		linklen = readlink(linkname, buf, PATH_MAX);
    .           #else
    .           		linklen = readlinkat(AT_FDCWD, linkname, buf, PATH_MAX);
    .           #endif
    2 ( 0.00%)  		if (linklen == -1) {
    .           			/* No configuration specified. */
    .           			linklen = 0;
    .           			/* Restore errno. */
    .           			set_errno(saved_errno);
    .           		}
    .           #endif
    2 ( 0.00%)  		buf[linklen] = '\0';
    .           		ret = buf;
    .           		break;
    .           	} case 3: {
    .           		const char *envname =
    .           #ifdef JEMALLOC_PREFIX
    .           		    JEMALLOC_CPREFIX"MALLOC_CONF"
    .           #else
    .           		    "MALLOC_CONF"
-- line 1041 ----------------------------------------
-- line 1048 ----------------------------------------
    .           			 * of the MALLOC_CONF environment variable.
    .           			 */
    .           		} else {
    .           			/* No configuration specified. */
    .           			ret = NULL;
    .           		}
    .           		break;
    .           	} case 4: {
    1 ( 0.00%)  		ret = je_malloc_conf_2_conf_harder;
    .           		break;
    .           	} default:
    .           		not_reached();
    .           		ret = NULL;
    .           	}
    .           	return ret;
    .           }
    .           
    .           static void
    .           malloc_conf_init_helper(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],
    .               bool initial_call, const char *opts_cache[MALLOC_CONF_NSOURCES],
   32 ( 0.00%)      char buf[PATH_MAX + 1]) {
    .           	static const char *opts_explain[MALLOC_CONF_NSOURCES] = {
    .           		"string specified via --with-malloc-conf",
    .           		"string pointed to by the global variable malloc_conf",
    .           		"\"name\" of the file referenced by the symbolic link named "
    .           		    "/etc/malloc.conf",
    .           		"value of the environment variable MALLOC_CONF",
    .           		"string pointed to by the global variable "
    .           		    "malloc_conf_2_conf_harder",
    .           	};
    .           	unsigned i;
    .           	const char *opts, *k, *v;
    .           	size_t klen, vlen;
    .           
   42 ( 0.00%)  	for (i = 0; i < MALLOC_CONF_NSOURCES; i++) {
    .           		/* Get runtime configuration. */
   68 ( 0.00%)  		if (initial_call) {
   11 ( 0.00%)  			opts_cache[i] = obtain_malloc_conf(i, buf);
    .           		}
   12 ( 0.00%)  		opts = opts_cache[i];
   10 ( 0.00%)  		if (!initial_call && opt_confirm_conf) {
    .           			malloc_printf(
    .           			    "<jemalloc>: malloc_conf #%u (%s): \"%s\"\n",
    .           			    i + 1, opts_explain[i], opts != NULL ? opts : "");
    .           		}
   21 ( 0.00%)  		if (opts == NULL) {
    .           			continue;
    .           		}
    .           
    6 ( 0.00%)  		while (*opts != '\0' && !malloc_conf_next(&opts, &k, &klen, &v,
    .           		    &vlen)) {
    .           
    .           #define CONF_ERROR(msg, k, klen, v, vlen)				\
    .           			if (!initial_call) {				\
    .           				malloc_conf_error(			\
    .           				    msg, k, klen, v, vlen);		\
    .           				cur_opt_valid = false;			\
    .           			}
-- line 1105 ----------------------------------------
-- line 1216 ----------------------------------------
    .           			CONF_HANDLE_BOOL(opt_confirm_conf, "confirm_conf")
    .           			if (initial_call) {
    .           				continue;
    .           			}
    .           
    .           			CONF_HANDLE_BOOL(opt_abort, "abort")
    .           			CONF_HANDLE_BOOL(opt_abort_conf, "abort_conf")
    .           			CONF_HANDLE_BOOL(opt_trust_madvise, "trust_madvise")
    2 ( 0.00%)  			if (strncmp("metadata_thp", k, klen) == 0) {
    .           				int m;
    .           				bool match = false;
    .           				for (m = 0; m < metadata_thp_mode_limit; m++) {
    .           					if (strncmp(metadata_thp_mode_names[m],
    .           					    v, vlen) == 0) {
    .           						opt_metadata_thp = m;
    .           						match = true;
    .           						break;
-- line 1232 ----------------------------------------
-- line 1717 ----------------------------------------
    .           #undef CONF_HANDLE_T_SIGNED
    .           #undef CONF_HANDLE_UNSIGNED
    .           #undef CONF_HANDLE_SIZE_T
    .           #undef CONF_HANDLE_SSIZE_T
    .           #undef CONF_HANDLE_CHAR_P
    .               /* Re-enable diagnostic "-Wtype-limits" */
    .               JEMALLOC_DIAGNOSTIC_POP
    .           		}
    9 ( 0.00%)  		if (opt_abort_conf && had_conf_error) {
    .           			malloc_abort_invalid_conf();
    .           		}
    .           	}
    .           	atomic_store_b(&log_init_done, true, ATOMIC_RELEASE);
   22 ( 0.00%)  }
    .           
    .           static bool
    .           malloc_conf_init_check_deps(void) {
    3 ( 0.00%)  	if (opt_prof_leak_error && !opt_prof_final) {
    .           		malloc_printf("<jemalloc>: prof_leak_error is set w/o "
    .           		    "prof_final.\n");
    .           		return true;
    .           	}
    .           
    .           	return false;
    .           }
    .           
    .           static void
    .           malloc_conf_init(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS]) {
    4 ( 0.00%)  	const char *opts_cache[MALLOC_CONF_NSOURCES] = {NULL, NULL, NULL, NULL,
    .           		NULL};
    .           	char buf[PATH_MAX + 1];
    .           
    .           	/* The first call only set the confirm_conf option and opts_cache */
    7 ( 0.00%)  	malloc_conf_init_helper(NULL, NULL, true, opts_cache, buf);
  656 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:malloc_conf_init_helper (1x)
    6 ( 0.00%)  	malloc_conf_init_helper(sc_data, bin_shard_sizes, false, opts_cache,
  151 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:malloc_conf_init_helper (1x)
    .           	    NULL);
    .           	if (malloc_conf_init_check_deps()) {
    .           		/* check_deps does warning msg only; abort below if needed. */
    .           		if (opt_abort_conf) {
    .           			malloc_abort_invalid_conf();
    .           		}
    .           	}
    .           }
    .           
    .           #undef MALLOC_CONF_NSOURCES
    .           
    .           static bool
    .           malloc_init_hard_needed(void) {
    7 ( 0.00%)  	if (malloc_initialized() || (IS_INITIALIZER && malloc_init_state ==
    4 ( 0.00%)  => ???:0x000000000011b2a8 (1x)
    .           	    malloc_init_recursible)) {
    .           		/*
    .           		 * Another thread initialized the allocator before this one
    .           		 * acquired init_lock, or this thread is the initializing
    .           		 * thread, and it is recursively allocating.
    .           		 */
    .           		return false;
    .           	}
    .           #ifdef JEMALLOC_THREADED_INIT
    2 ( 0.00%)  	if (malloc_initializer != NO_INITIALIZER && !IS_INITIALIZER) {
    .           		/* Busy-wait until the initializing thread completes. */
    .           		spin_t spinner = SPIN_INITIALIZER;
    .           		do {
    .           			malloc_mutex_unlock(TSDN_NULL, &init_lock);
    .           			spin_adaptive(&spinner);
    .           			malloc_mutex_lock(TSDN_NULL, &init_lock);
    .           		} while (!malloc_initialized());
    .           		return false;
    .           	}
    .           #endif
    .           	return true;
    .           }
    .           
    .           static bool
   11 ( 0.00%)  malloc_init_hard_a0_locked() {
    2 ( 0.00%)  	malloc_initializer = INITIALIZER;
    4 ( 0.00%)  => ???:0x000000000011b2a8 (1x)
    .           
    .           	JEMALLOC_DIAGNOSTIC_PUSH
    .           	JEMALLOC_DIAGNOSTIC_IGNORE_MISSING_STRUCT_FIELD_INITIALIZERS
  827 ( 0.03%)  	sc_data_t sc_data = {0};
    .           	JEMALLOC_DIAGNOSTIC_POP
    .           
    .           	/*
    .           	 * Ordering here is somewhat tricky; we need sc_boot() first, since that
    .           	 * determines what the size classes will be, and then
    .           	 * malloc_conf_init(), since any slab size tweaking will need to be done
    .           	 * before sz_boot and bin_info_boot, which assume that the values they
    .           	 * read out of sc_data_global are final.
    .           	 */
    2 ( 0.00%)  	sc_boot(&sc_data);
10,307 ( 0.37%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sc.c:_rjem_je_sc_boot (1x)
    .           	unsigned bin_shard_sizes[SC_NBINS];
    3 ( 0.00%)  	bin_shard_sizes_boot(bin_shard_sizes);
   12 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin.c:_rjem_je_bin_shard_sizes_boot (1x)
    .           	/*
    .           	 * prof_boot0 only initializes opt_prof_prefix.  We need to do it before
    .           	 * we parse malloc_conf options, in case malloc_conf parsing overwrites
    .           	 * it.
    .           	 */
    .           	if (config_prof) {
    .           		prof_boot0();
    .           	}
    .           	malloc_conf_init(&sc_data, bin_shard_sizes);
    3 ( 0.00%)  	san_init(opt_lg_san_uaf_align);
    6 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/san.c:_rjem_je_san_init (1x)
    3 ( 0.00%)  	sz_boot(&sc_data, opt_cache_oblivious);
5,778 ( 0.20%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_boot (1x)
    3 ( 0.00%)  	bin_info_boot(&sc_data, bin_shard_sizes);
  861 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin_info.c:_rjem_je_bin_info_boot (1x)
    .           
    3 ( 0.00%)  	if (opt_stats_print) {
    .           		/* Print statistics at exit. */
    .           		if (atexit(stats_print_atexit) != 0) {
    .           			malloc_write("<jemalloc>: Error in atexit()\n");
    .           			if (opt_abort) {
    .           				abort();
    .           			}
    .           		}
    .           	}
    .           
    3 ( 0.00%)  	if (stats_boot()) {
   15 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/stats.c:_rjem_je_stats_boot (1x)
    .           		return true;
    .           	}
    3 ( 0.00%)  	if (pages_boot()) {
1,380 ( 0.05%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pages.c:_rjem_je_pages_boot (1x)
    .           		return true;
    .           	}
    4 ( 0.00%)  	if (base_boot(TSDN_NULL)) {
2,130 ( 0.08%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_boot (1x)
    .           		return true;
    .           	}
    .           	/* emap_global is static, hence zeroed. */
    7 ( 0.00%)  	if (emap_init(&arena_emap_global, b0get(), /* zeroed */ true)) {
  137 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_init (1x)
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_b0get (1x)
    .           		return true;
    .           	}
    3 ( 0.00%)  	if (extent_boot()) {
   53 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_extent_boot (1x)
    .           		return true;
    .           	}
    3 ( 0.00%)  	if (ctl_boot()) {
  139 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ctl.c:_rjem_je_ctl_boot (1x)
    .           		return true;
    .           	}
    .           	if (config_prof) {
    .           		prof_boot1();
    .           	}
    2 ( 0.00%)  	if (opt_hpa && !hpa_supported()) {
    1 ( 0.00%)  		malloc_printf("<jemalloc>: HPA not supported in the current "
    .           		    "configuration; %s.",
    .           		    opt_abort_conf ? "aborting" : "disabling");
    .           		if (opt_abort_conf) {
    .           			malloc_abort_invalid_conf();
    .           		} else {
    .           			opt_hpa = false;
    .           		}
    .           	}
    7 ( 0.00%)  	if (arena_boot(&sc_data, b0get(), opt_hpa)) {
  903 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_boot (1x)
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_b0get (1x)
    .           		return true;
    .           	}
    6 ( 0.00%)  	if (tcache_boot(TSDN_NULL, b0get())) {
3,086 ( 0.11%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_boot (1x)
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_b0get (1x)
    .           		return true;
    .           	}
    8 ( 0.00%)  	if (malloc_mutex_init(&arenas_lock, "arenas", WITNESS_RANK_ARENAS,
  127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
    .           	    malloc_mutex_rank_exclusive)) {
    .           		return true;
    .           	}
    2 ( 0.00%)  	hook_boot();
  133 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/hook.c:_rjem_je_hook_boot (1x)
    .           	/*
    .           	 * Create enough scaffolding to allow recursive allocation in
    .           	 * malloc_ncpus().
    .           	 */
    1 ( 0.00%)  	narenas_auto = 1;
    1 ( 0.00%)  	manual_arena_base = narenas_auto + 1;
    .           	memset(arenas, 0, sizeof(arena_t *) * narenas_auto);
    .           	/*
    .           	 * Initialize one arena here.  The rest are lazily created in
    .           	 * arena_choose_hard().
    .           	 */
    6 ( 0.00%)  	if (arena_init(TSDN_NULL, 0, &arena_config_default) == NULL) {
19,275 ( 0.68%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_arena_init (1x)
    .           		return true;
    .           	}
    1 ( 0.00%)  	a0 = arena_get(TSDN_NULL, 0, false);
    .           
    2 ( 0.00%)  	if (opt_hpa && !hpa_supported()) {
    .           		malloc_printf("<jemalloc>: HPA not supported in the current "
    .           		    "configuration; %s.",
    .           		    opt_abort_conf ? "aborting" : "disabling");
    .           		if (opt_abort_conf) {
    .           			malloc_abort_invalid_conf();
    .           		} else {
    .           			opt_hpa = false;
    .           		}
-- line 1898 ----------------------------------------
-- line 1900 ----------------------------------------
    .           		hpa_shard_opts_t hpa_shard_opts = opt_hpa_opts;
    .           		hpa_shard_opts.deferral_allowed = background_thread_enabled();
    .           		if (pa_shard_enable_hpa(TSDN_NULL, &a0->pa_shard,
    .           		    &hpa_shard_opts, &opt_hpa_sec_opts)) {
    .           			return true;
    .           		}
    .           	}
    .           
    1 ( 0.00%)  	malloc_init_state = malloc_init_a0_initialized;
    .           
    1 ( 0.00%)  	return false;
    9 ( 0.00%)  }
    .           
    .           static bool
    .           malloc_init_hard_a0(void) {
    .           	bool ret;
    .           
    .           	malloc_mutex_lock(TSDN_NULL, &init_lock);
    .           	ret = malloc_init_hard_a0_locked();
    .           	malloc_mutex_unlock(TSDN_NULL, &init_lock);
    .           	return ret;
    .           }
    .           
    .           /* Initialize data structures which may trigger recursive allocation. */
    .           static bool
    .           malloc_init_hard_recursible(void) {
    1 ( 0.00%)  	malloc_init_state = malloc_init_recursible;
    .           
    1 ( 0.00%)  	ncpus = malloc_ncpus();
    3 ( 0.00%)  	if (opt_percpu_arena != percpu_arena_disabled) {
    .           		bool cpu_count_is_deterministic =
    .           		    malloc_cpu_count_is_deterministic();
    .           		if (!cpu_count_is_deterministic) {
    .           			/*
    .           			 * If # of CPU is not deterministic, and narenas not
    .           			 * specified, disables per cpu arena since it may not
    .           			 * detect CPU IDs properly.
    .           			 */
-- line 1937 ----------------------------------------
-- line 1949 ----------------------------------------
    .           			}
    .           		}
    .           	}
    .           
    .           #if (defined(JEMALLOC_HAVE_PTHREAD_ATFORK) && !defined(JEMALLOC_MUTEX_INIT_CB) \
    .               && !defined(JEMALLOC_ZONE) && !defined(_WIN32) && \
    .               !defined(__native_client__))
    .           	/* LinuxThreads' pthread_atfork() allocates. */
    6 ( 0.00%)  	if (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,
   52 ( 0.00%)  => ???:pthread_atfork (1x)
    .           	    jemalloc_postfork_child) != 0) {
    .           		malloc_write("<jemalloc>: Error in pthread_atfork()\n");
    .           		if (opt_abort) {
    .           			abort();
    .           		}
    .           		return true;
    .           	}
    .           #endif
    .           
    3 ( 0.00%)  	if (background_thread_boot0()) {
    5 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/background_thread.c:_rjem_je_background_thread_boot0 (1x)
    .           		return true;
    .           	}
    .           
    .           	return false;
    .           }
    .           
    .           static unsigned
    .           malloc_narenas_default(void) {
    .           	assert(ncpus > 0);
    .           	/*
    .           	 * For SMP systems, create more than one arena per CPU by
    .           	 * default.
    .           	 */
    3 ( 0.00%)  	if (ncpus > 1) {
    2 ( 0.00%)  		fxp_t fxp_ncpus = FXP_INIT_INT(ncpus);
    .           		fxp_t goal = fxp_mul(fxp_ncpus, opt_narenas_ratio);
    .           		uint32_t int_goal = fxp_round_nearest(goal);
    .           		if (int_goal == 0) {
    .           			return 1;
    .           		}
    .           		return int_goal;
    .           	} else {
    .           		return 1;
-- line 1990 ----------------------------------------
-- line 1992 ----------------------------------------
    .           }
    .           
    .           static percpu_arena_mode_t
    .           percpu_arena_as_initialized(percpu_arena_mode_t mode) {
    .           	assert(!malloc_initialized());
    .           	assert(mode <= percpu_arena_disabled);
    .           
    .           	if (mode != percpu_arena_disabled) {
    3 ( 0.00%)  		mode += percpu_arena_mode_enabled_base;
    .           	}
    .           
    .           	return mode;
    .           }
    .           
    .           static bool
    .           malloc_init_narenas(void) {
    .           	assert(ncpus > 0);
    .           
    2 ( 0.00%)  	if (opt_percpu_arena != percpu_arena_disabled) {
    .           		if (!have_percpu_arena || malloc_getcpu() < 0) {
    .           			opt_percpu_arena = percpu_arena_disabled;
    .           			malloc_printf("<jemalloc>: perCPU arena getcpu() not "
    .           			    "available. Setting narenas to %u.\n", opt_narenas ?
    .           			    opt_narenas : malloc_narenas_default());
    .           			if (opt_abort) {
    .           				abort();
    .           			}
-- line 2018 ----------------------------------------
-- line 2051 ----------------------------------------
    .           				 * of affinity setting from numactl), reserving
    .           				 * narenas this way provides a workaround for
    .           				 * percpu_arena.
    .           				 */
    .           				opt_narenas = n;
    .           			}
    .           		}
    .           	}
    3 ( 0.00%)  	if (opt_narenas == 0) {
    1 ( 0.00%)  		opt_narenas = malloc_narenas_default();
    .           	}
    .           	assert(opt_narenas > 0);
    .           
    1 ( 0.00%)  	narenas_auto = opt_narenas;
    .           	/*
    .           	 * Limit the number of arenas to the indexing range of MALLOCX_ARENA().
    .           	 */
    2 ( 0.00%)  	if (narenas_auto >= MALLOCX_ARENA_LIMIT) {
    .           		narenas_auto = MALLOCX_ARENA_LIMIT - 1;
    .           		malloc_printf("<jemalloc>: Reducing narenas to limit (%d)\n",
    .           		    narenas_auto);
    .           	}
    .           	narenas_total_set(narenas_auto);
    3 ( 0.00%)  	if (arena_init_huge()) {
   17 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_init_huge (1x)
    .           		narenas_total_inc();
    .           	}
    1 ( 0.00%)  	manual_arena_base = narenas_total_get();
    .           
    .           	return false;
    .           }
    .           
    .           static void
    .           malloc_init_percpu(void) {
    2 ( 0.00%)  	opt_percpu_arena = percpu_arena_as_initialized(opt_percpu_arena);
    .           }
    .           
    .           static bool
    .           malloc_init_hard_finish(void) {
    3 ( 0.00%)  	if (malloc_mutex_boot()) {
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_boot (1x)
    .           		return true;
    .           	}
    .           
    1 ( 0.00%)  	malloc_init_state = malloc_init_initialized;
    .           	malloc_slow_flag_init();
    .           
    .           	return false;
    .           }
    .           
    .           static void
    .           malloc_init_hard_cleanup(tsdn_t *tsdn, bool reentrancy_set) {
    .           	malloc_mutex_assert_owner(tsdn, &init_lock);
-- line 2101 ----------------------------------------
-- line 2104 ----------------------------------------
    .           		assert(!tsdn_null(tsdn));
    .           		tsd_t *tsd = tsdn_tsd(tsdn);
    .           		assert(tsd_reentrancy_level_get(tsd) > 0);
    .           		post_reentrancy(tsd);
    .           	}
    .           }
    .           
    .           static bool
    9 ( 0.00%)  malloc_init_hard(void) {
    .           	tsd_t *tsd;
    .           
    .           #if defined(_WIN32) && _WIN32_WINNT < 0x0600
    .           	_init_init_lock();
    .           #endif
    .           	malloc_mutex_lock(TSDN_NULL, &init_lock);
    .           
    .           #define UNLOCK_RETURN(tsdn, ret, reentrancy)		\
    .           	malloc_init_hard_cleanup(tsdn, reentrancy);	\
    .           	return ret;
    .           
    .           	if (!malloc_init_hard_needed()) {
    2 ( 0.00%)  		UNLOCK_RETURN(TSDN_NULL, false, false)
    .           	}
    .           
    4 ( 0.00%)  	if (malloc_init_state != malloc_init_a0_initialized &&
    2 ( 0.00%)  	    malloc_init_hard_a0_locked()) {
46,112 ( 1.63%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:malloc_init_hard_a0_locked (1x)
    .           		UNLOCK_RETURN(TSDN_NULL, true, false)
    .           	}
    .           
    .           	malloc_mutex_unlock(TSDN_NULL, &init_lock);
    .           	/* Recursive allocation relies on functional tsd. */
    2 ( 0.00%)  	tsd = malloc_tsd_boot0();
9,185 ( 0.33%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tsd.c:_rjem_je_malloc_tsd_boot0 (1x)
    2 ( 0.00%)  	if (tsd == NULL) {
    .           		return true;
    .           	}
    .           	if (malloc_init_hard_recursible()) {
    .           		return true;
    .           	}
    .           
    .           	malloc_mutex_lock(tsd_tsdn(tsd), &init_lock);
    .           	/* Set reentrancy level to 1 during init. */
    .           	pre_reentrancy(tsd, NULL);
    .           	/* Initialize narenas before prof_boot2 (for allocation). */
    .           	if (malloc_init_narenas()
    6 ( 0.00%)  	    || background_thread_boot1(tsd_tsdn(tsd), b0get())) {
1,909 ( 0.07%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/background_thread.c:_rjem_je_background_thread_boot1 (1x)
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_b0get (1x)
    .           		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
    .           	}
    .           	if (config_prof && prof_boot2(tsd, b0get())) {
    .           		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
    .           	}
    .           
    .           	malloc_init_percpu();
    .           
-- line 2156 ----------------------------------------
-- line 2157 ----------------------------------------
    .           	if (malloc_init_hard_finish()) {
    .           		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
    .           	}
    .           	post_reentrancy(tsd);
    .           	malloc_mutex_unlock(tsd_tsdn(tsd), &init_lock);
    .           
    .           	witness_assert_lockless(witness_tsd_tsdn(
    .           	    tsd_witness_tsdp_get_unsafe(tsd)));
    1 ( 0.00%)  	malloc_tsd_boot1();
   43 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tsd.c:_rjem_je_malloc_tsd_boot1 (1x)
    .           	/* Update TSD after tsd_boot1. */
    .           	tsd = tsd_fetch();
    3 ( 0.00%)  	if (opt_background_thread) {
    .           		assert(have_background_thread);
    .           		/*
    .           		 * Need to finish init & unlock first before creating background
    .           		 * threads (pthread_create depends on malloc).  ctl_init (which
    .           		 * sets isthreaded) needs to be called without holding any lock.
    .           		 */
    .           		background_thread_ctl_init(tsd_tsdn(tsd));
    .           		if (background_thread_create(tsd, 0)) {
    .           			return true;
    .           		}
    .           	}
    .           #undef UNLOCK_RETURN
    .           	return false;
   10 ( 0.00%)  }
    .           
    .           /*
    .            * End initialization functions.
    .            */
    .           /******************************************************************************/
    .           /*
    .            * Begin allocation-path internal functions and data structures.
    .            */
-- line 2190 ----------------------------------------
-- line 2290 ----------------------------------------
    .           /*
    .            * ind parameter is optional and is only checked and filled if alignment == 0;
    .            * return true if result is out of range.
    .            */
    .           JEMALLOC_ALWAYS_INLINE bool
    .           aligned_usize_get(size_t size, size_t alignment, size_t *usize, szind_t *ind,
    .               bool bump_empty_aligned_alloc) {
    .           	assert(usize != NULL);
   60 ( 0.00%)  	if (alignment == 0) {
    .           		if (ind != NULL) {
    .           			*ind = sz_size2index(size);
   28 ( 0.00%)  			if (unlikely(*ind >= SC_NSIZES)) {
    .           				return true;
    .           			}
    .           			*usize = sz_index2size(*ind);
    .           			assert(*usize > 0 && *usize <= SC_LARGE_MAXCLASS);
    .           			return false;
    .           		}
    .           		*usize = sz_s2u(size);
    .           	} else {
    .           		if (bump_empty_aligned_alloc && unlikely(size == 0)) {
    .           			size = 1;
    .           		}
    .           		*usize = sz_sa2u(size, alignment);
    .           	}
  116 ( 0.00%)  	if (unlikely(*usize == 0 || *usize > SC_LARGE_MAXCLASS)) {
    .           		return true;
    .           	}
    .           	return false;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           zero_get(bool guarantee, bool slow) {
   29 ( 0.00%)  	if (config_fill && slow && unlikely(opt_zero)) {
  116 ( 0.00%)  		return true;
    .           	} else {
    .           		return guarantee;
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE tcache_t *
    .           tcache_get_from_ind(tsd_t *tsd, unsigned tcache_ind, bool slow, bool is_alloc) {
    .           	tcache_t *tcache;
    .           	if (tcache_ind == TCACHE_IND_AUTOMATIC) {
    2 ( 0.00%)  		if (likely(!slow)) {
    .           			/* Getting tcache ptr unconditionally. */
    .           			tcache = tsd_tcachep_get(tsd);
    .           			assert(tcache == tcache_get(tsd));
    .           		} else if (is_alloc ||
    .           		    likely(tsd_reentrancy_level_get(tsd) == 0)) {
    .           			tcache = tcache_get(tsd);
    .           		} else {
    .           			tcache = NULL;
-- line 2342 ----------------------------------------
-- line 2361 ----------------------------------------
    .           JEMALLOC_ALWAYS_INLINE bool
    .           arena_get_from_ind(tsd_t *tsd, unsigned arena_ind, arena_t **arena_p) {
    .           	if (arena_ind == ARENA_IND_AUTOMATIC) {
    .           		/*
    .           		 * In case of automatic arena management, we defer arena
    .           		 * computation until as late as we can, hoping to fill the
    .           		 * allocation out of the tcache.
    .           		 */
   29 ( 0.00%)  		*arena_p = NULL;
    .           	} else {
    .           		*arena_p = arena_get(tsd_tsdn(tsd), arena_ind, true);
    .           		if (unlikely(*arena_p == NULL) && arena_ind >= narenas_auto) {
    .           			return true;
    .           		}
    .           	}
    .           	return false;
    .           }
-- line 2377 ----------------------------------------
-- line 2567 ----------------------------------------
    .           			prof_alloc_rollback(tsd, tctx);
    .           			goto label_oom;
    .           		}
    .           		prof_malloc(tsd, allocation, size, usize, &alloc_ctx, tctx);
    .           	} else {
    .           		assert(!opt_prof);
    .           		allocation = imalloc_no_sample(sopts, dopts, tsd, size, usize,
    .           		    ind);
   28 ( 0.00%)  		if (unlikely(allocation == NULL)) {
    .           			goto label_oom;
    .           		}
    .           	}
    .           
    .           	/*
    .           	 * Allocation has been done at this point.  We still have some
    .           	 * post-allocation work to do though.
    .           	 */
-- line 2583 ----------------------------------------
-- line 2692 ----------------------------------------
    .           
    .           		sopts->slow = true;
    .           		return imalloc_body(sopts, dopts, tsd);
    .           	}
    .           }
    .           
    .           JEMALLOC_NOINLINE
    .           void *
  126 ( 0.00%)  malloc_default(size_t size) {
    .           	void *ret;
    .           	static_opts_t sopts;
    .           	dynamic_opts_t dopts;
    .           
    .           	/*
    .           	 * This variant has logging hook on exit but not on entry.  It's callled
    .           	 * only by je_malloc, below, which emits the entry one for us (and, if
    .           	 * it calls us, does so only via tail call).
-- line 2708 ----------------------------------------
-- line 2727 ----------------------------------------
    .           	if (sopts.slow) {
    .           		uintptr_t args[3] = {size};
    .           		hook_invoke_alloc(hook_alloc_malloc, ret, (uintptr_t)ret, args);
    .           	}
    .           
    .           	LOG("core.malloc.exit", "result: %p", ret);
    .           
    .           	return ret;
  168 ( 0.01%)  }
    .           
    .           /******************************************************************************/
    .           /*
    .            * Begin malloc(3)-compatible functions.
    .            */
    .           
    .           JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
    .           void JEMALLOC_NOTHROW *
    .           JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)
  602 ( 0.02%)  je_malloc(size_t size) {
    .           	return imalloc_fastpath(size, &malloc_default);
1,176 ( 0.04%)  }
    .           
    .           JEMALLOC_EXPORT int JEMALLOC_NOTHROW
    .           JEMALLOC_ATTR(nonnull(1))
    .           je_posix_memalign(void **memptr, size_t alignment, size_t size) {
    .           	int ret;
    .           	static_opts_t sopts;
    .           	dynamic_opts_t dopts;
    .           
-- line 2755 ----------------------------------------
-- line 3089 ----------------------------------------
    .           		}
    .           		assert(alloc_ctx.szind != SC_NSIZES);
    .           	} else {
    .           		/*
    .           		 * Check for both sizes that are too large, and for sampled /
    .           		 * special aligned objects.  The alignment check will also check
    .           		 * for null ptr.
    .           		 */
2,394 ( 0.08%)  		if (unlikely(size > SC_LOOKUP_MAXCLASS ||
    .           		    free_fastpath_nonfast_aligned(ptr,
    .           		    /* check_prof */ true))) {
    .           			return false;
    .           		}
    .           		alloc_ctx.szind = sz_size2index_lookup(size);
    .           		/* Max lookup class must be small. */
    .           		assert(alloc_ctx.szind < SC_NBINS);
    .           		/* This is a dead store, except when opt size checking is on. */
-- line 3105 ----------------------------------------
-- line 3111 ----------------------------------------
    .           	 * tcache szind upper limit (i.e. tcache_maxclass) as well.
    .           	 */
    .           	assert(alloc_ctx.slab);
    .           
    .           	uint64_t deallocated, threshold;
    .           	te_free_fastpath_ctx(tsd, &deallocated, &threshold);
    .           
    .           	size_t usize = sz_index2size(alloc_ctx.szind);
1,794 ( 0.06%)  	uint64_t deallocated_after = deallocated + usize;
    .           	/*
    .           	 * Check for events and tsd non-nominal (fast_threshold will be set to
    .           	 * 0) in a single branch.  Note that this handles the uninitialized case
    .           	 * as well (TSD init will be triggered on the non-fastpath).  Therefore
    .           	 * anything depends on a functional TSD (e.g. the alloc_ctx sanity check
    .           	 * below) needs to be after this branch.
    .           	 */
1,196 ( 0.04%)  	if (unlikely(deallocated_after >= threshold)) {
    .           		return false;
    .           	}
    .           	assert(tsd_fast(tsd));
    .           	bool fail = maybe_check_alloc_ctx(tsd, ptr, &alloc_ctx);
    .           	if (fail) {
    .           		/* See the comment in isfree. */
    .           		return true;
    .           	}
-- line 3135 ----------------------------------------
-- line 3144 ----------------------------------------
    .           	 * that to double-check.
    .           	 */
    .           	assert(!opt_junk_free);
    .           
    .           	if (!cache_bin_dalloc_easy(bin, ptr)) {
    .           		return false;
    .           	}
    .           
  598 ( 0.02%)  	*tsd_thread_deallocatedp_get(tsd) = deallocated_after;
    .           
    .           	return true;
    .           }
    .           
    .           JEMALLOC_EXPORT void JEMALLOC_NOTHROW
    .           je_free(void *ptr) {
    .           	LOG("core.free.entry", "ptr: %p", ptr);
    .           
-- line 3160 ----------------------------------------
-- line 3310 ----------------------------------------
    .            */
    .           /******************************************************************************/
    .           /*
    .            * Begin non-standard functions.
    .            */
    .           
    .           JEMALLOC_ALWAYS_INLINE unsigned
    .           mallocx_tcache_get(int flags) {
   90 ( 0.00%)  	if (likely((flags & MALLOCX_TCACHE_MASK) == 0)) {
    .           		return TCACHE_IND_AUTOMATIC;
    .           	} else if ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
    .           		return TCACHE_IND_NONE;
    .           	} else {
    .           		return MALLOCX_TCACHE_GET(flags);
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE unsigned
    .           mallocx_arena_get(int flags) {
   58 ( 0.00%)  	if (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {
    .           		return MALLOCX_ARENA_GET(flags);
    .           	} else {
    .           		return ARENA_IND_AUTOMATIC;
    .           	}
    .           }
    .           
    .           #ifdef JEMALLOC_EXPERIMENTAL_SMALLOCX_API
    .           
-- line 3337 ----------------------------------------
-- line 3485 ----------------------------------------
    .           	assert(usize == isalloc(tsd_tsdn(tsd), p));
    .           	prof_realloc(tsd, p, size, usize, tctx, prof_active, old_ptr,
    .           	    old_usize, &old_prof_info, sample_event);
    .           
    .           	return p;
    .           }
    .           
    .           static void *
  406 ( 0.01%)  do_rallocx(void *ptr, size_t size, int flags, bool is_realloc) {
    .           	void *p;
    .           	tsd_t *tsd;
    .           	size_t usize;
    .           	size_t old_usize;
  116 ( 0.00%)  	size_t alignment = MALLOCX_ALIGN_GET(flags);
    .           	arena_t *arena;
    .           
    .           	assert(ptr != NULL);
    .           	assert(size != 0);
    .           	assert(malloc_initialized() || IS_INITIALIZER);
    .           	tsd = tsd_fetch();
    .           	check_entry_exit_locking(tsd_tsdn(tsd));
    .           
   58 ( 0.00%)  	bool zero = zero_get(MALLOCX_ZERO_GET(flags), /* slow */ true);
    .           
    .           	unsigned arena_ind = mallocx_arena_get(flags);
    .           	if (arena_get_from_ind(tsd, arena_ind, &arena)) {
    .           		goto label_oom;
    .           	}
    .           
    .           	unsigned tcache_ind = mallocx_tcache_get(flags);
    .           	tcache_t *tcache = tcache_get_from_ind(tsd, tcache_ind,
-- line 3515 ----------------------------------------
-- line 3520 ----------------------------------------
    .           	    &alloc_ctx);
    .           	assert(alloc_ctx.szind != SC_NSIZES);
    .           	old_usize = sz_index2size(alloc_ctx.szind);
    .           	assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
    .           	if (aligned_usize_get(size, alignment, &usize, NULL, false)) {
    .           		goto label_oom;
    .           	}
    .           
  174 ( 0.01%)  	hook_ralloc_args_t hook_args = {is_realloc, {(uintptr_t)ptr, size,
    .           		flags, 0}};
    .           	if (config_prof && opt_prof) {
    .           		p = irallocx_prof(tsd, ptr, old_usize, size, alignment, usize,
    .           		    zero, tcache, arena, &alloc_ctx, &hook_args);
    .           		if (unlikely(p == NULL)) {
    .           			goto label_oom;
    .           		}
    .           	} else {
    .           		p = iralloct(tsd_tsdn(tsd), ptr, old_usize, size, alignment,
    .           		    zero, tcache, arena, &hook_args);
  116 ( 0.00%)  		if (unlikely(p == NULL)) {
    .           			goto label_oom;
    .           		}
    .           		assert(usize == isalloc(tsd_tsdn(tsd), p));
    .           	}
    .           	assert(alignment == 0 || ((uintptr_t)p & (alignment - 1)) == ZU(0));
    .           	thread_alloc_event(tsd, usize);
    .           	thread_dalloc_event(tsd, old_usize);
    .           
    .           	UTRACE(ptr, size, p);
    .           	check_entry_exit_locking(tsd_tsdn(tsd));
    .           
   87 ( 0.00%)  	if (config_fill && unlikely(opt_junk_alloc) && usize > old_usize
   29 ( 0.00%)  	    && !zero) {
    .           		size_t excess_len = usize - old_usize;
    .           		void *excess_start = (void *)((uintptr_t)p + old_usize);
    .           		junk_alloc_callback(excess_start, excess_len);
    .           	}
    .           
    .           	return p;
    .           label_oom:
    .           	if (config_xmalloc && unlikely(opt_xmalloc)) {
    .           		malloc_write("<jemalloc>: Error in rallocx(): out of memory\n");
    .           		abort();
    .           	}
    .           	UTRACE(ptr, size, 0);
    .           	check_entry_exit_locking(tsd_tsdn(tsd));
    .           
    .           	return NULL;
  348 ( 0.01%)  }
    .           
    .           JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
    .           void JEMALLOC_NOTHROW *
    .           JEMALLOC_ALLOC_SIZE(2)
    .           je_rallocx(void *ptr, size_t size, int flags) {
    .           	LOG("core.rallocx.entry", "ptr: %p, size: %zu, flags: %d", ptr,
    .           	    size, flags);
    .           	void *ret = do_rallocx(ptr, size, flags, false);
-- line 3576 ----------------------------------------
-- line 3585 ----------------------------------------
    .           	}
    .           	if (opt_zero_realloc_action == zero_realloc_action_alloc) {
    .           		/*
    .           		 * The user might have gotten an alloc setting while expecting a
    .           		 * free setting.  If that's the case, we at least try to
    .           		 * reduce the harm, and turn off the tcache while allocating, so
    .           		 * that we'll get a true first fit.
    .           		 */
   58 ( 0.00%)  		return do_rallocx(ptr, 1, MALLOCX_TCACHE_NONE, true);
47,789 ( 1.69%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:do_rallocx (29x)
    .           	} else if (opt_zero_realloc_action == zero_realloc_action_free) {
    .           		UTRACE(ptr, 0, 0);
    .           		tsd_t *tsd = tsd_fetch();
    .           		check_entry_exit_locking(tsd_tsdn(tsd));
    .           
    .           		tcache_t *tcache = tcache_get_from_ind(tsd,
    .           		    TCACHE_IND_AUTOMATIC, /* slow */ true,
    .           		    /* is_alloc */ false);
-- line 3601 ----------------------------------------
-- line 3615 ----------------------------------------
    .           		 */
    .           		return NULL;
    .           	}
    .           }
    .           
    .           JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
    .           void JEMALLOC_NOTHROW *
    .           JEMALLOC_ALLOC_SIZE(2)
  377 ( 0.01%)  je_realloc(void *ptr, size_t size) {
    .           	LOG("core.realloc.entry", "ptr: %p, size: %zu\n", ptr, size);
    .           
  174 ( 0.01%)  	if (likely(ptr != NULL && size != 0)) {
  145 ( 0.01%)  		void *ret = do_rallocx(ptr, size, 0, true);
    .           		LOG("core.realloc.exit", "result: %p", ret);
    .           		return ret;
    .           	} else if (ptr != NULL && size == 0) {
    .           		void *ret = do_realloc_nonnull_zero(ptr);
    .           		LOG("core.realloc.exit", "result: %p", ret);
    .           		return ret;
    .           	} else {
    .           		/* realloc(NULL, size) is equivalent to malloc(size). */
-- line 3635 ----------------------------------------
-- line 3654 ----------------------------------------
    .           		if (sopts.slow) {
    .           			uintptr_t args[3] = {(uintptr_t)ptr, size};
    .           			hook_invoke_alloc(hook_alloc_realloc, ret,
    .           			    (uintptr_t)ret, args);
    .           		}
    .           		LOG("core.realloc.exit", "result: %p", ret);
    .           		return ret;
    .           	}
  203 ( 0.01%)  }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           ixallocx_helper(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,
    .               size_t extra, size_t alignment, bool zero) {
    .           	size_t newsize;
    .           
    .           	if (ixalloc(tsdn, ptr, old_usize, size, extra, alignment, zero,
    .           	    &newsize)) {
-- line 3670 ----------------------------------------
-- line 3894 ----------------------------------------
    .           	LOG("core.dallocx.exit", "");
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           inallocx(tsdn_t *tsdn, size_t size, int flags) {
    .           	check_entry_exit_locking(tsdn);
    .           	size_t usize;
    .           	/* In case of out of range, let the user see it rather than fail. */
    4 ( 0.00%)  	aligned_usize_get(size, MALLOCX_ALIGN_GET(flags), &usize, NULL, false);
    .           	check_entry_exit_locking(tsdn);
    .           	return usize;
    .           }
    .           
    .           JEMALLOC_NOINLINE void
   14 ( 0.00%)  sdallocx_default(void *ptr, size_t size, int flags) {
    .           	assert(ptr != NULL);
    .           	assert(malloc_initialized() || IS_INITIALIZER);
    .           
    .           	tsd_t *tsd = tsd_fetch_min();
    .           	bool fast = tsd_fast(tsd);
    .           	size_t usize = inallocx(tsd_tsdn(tsd), size, flags);
    .           	check_entry_exit_locking(tsd_tsdn(tsd));
    .           
-- line 3916 ----------------------------------------
-- line 3923 ----------------------------------------
    .           		tsd_assert_fast(tsd);
    .           		isfree(tsd, ptr, usize, tcache, false);
    .           	} else {
    .           		uintptr_t args_raw[3] = {(uintptr_t)ptr, size, flags};
    .           		hook_invoke_dalloc(hook_dalloc_sdallocx, ptr, args_raw);
    .           		isfree(tsd, ptr, usize, tcache, true);
    .           	}
    .           	check_entry_exit_locking(tsd_tsdn(tsd));
   11 ( 0.00%)  }
    .           
    .           JEMALLOC_EXPORT void JEMALLOC_NOTHROW
1,198 ( 0.04%)  je_sdallocx(void *ptr, size_t size, int flags) {
    .           	LOG("core.sdallocx.entry", "ptr: %p, size: %zu, flags: %d", ptr,
    .           		size, flags);
    .           
    .           	if (flags != 0 || !free_fastpath(ptr, size, true)) {
    2 ( 0.00%)  		sdallocx_default(ptr, size, flags);
  110 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_sdallocx_default (1x)
    .           	}
    .           
    .           	LOG("core.sdallocx.exit", "");
  598 ( 0.02%)  }
    .           
    .           void JEMALLOC_NOTHROW
    .           je_sdallocx_noflags(void *ptr, size_t size) {
    .           	LOG("core.sdallocx.entry", "ptr: %p, size: %zu, flags: 0", ptr,
    .           		size);
    .           
    .           	if (!free_fastpath(ptr, size, true)) {
    .           		sdallocx_default(ptr, size, 0);
-- line 3951 ----------------------------------------
-- line 4313 ----------------------------------------
    .            * the allocator isn't fully initialized at fork time.  The following library
    .            * constructor is a partial solution to this problem.  It may still be possible
    .            * to trigger the deadlock described above, but doing so would involve forking
    .            * via a library constructor that runs before jemalloc's runs.
    .            */
    .           #ifndef JEMALLOC_JET
    .           JEMALLOC_ATTR(constructor)
    .           static void
    1 ( 0.00%)  jemalloc_constructor(void) {
    .           	malloc_init();
    .           }
    .           #endif
    .           
    .           #ifndef JEMALLOC_MUTEX_INIT_CB
    .           void
    .           jemalloc_prefork(void)
    .           #else
-- line 4329 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c
--------------------------------------------------------------------------------
Ir             

-- line 250 ----------------------------------------
    .           #if (! defined JEMALLOC_INTERNAL_POPCOUNTL) || (defined BITMAP_USE_TREE)
    .           	for (unsigned i = 0; i < cnt; i++) {
    .           		size_t regind = bitmap_sfu(slab_data->bitmap,
    .           					   &bin_info->bitmap_info);
    .           		*(ptrs + i) = (void *)((uintptr_t)edata_addr_get(slab) +
    .           		    (uintptr_t)(bin_info->reg_size * regind));
    .           	}
    .           #else
   93 ( 0.00%)  	unsigned group = 0;
   31 ( 0.00%)  	bitmap_t g = slab_data->bitmap[group];
   62 ( 0.00%)  	unsigned i = 0;
   78 ( 0.00%)  	while (i < cnt) {
  133 ( 0.00%)  		while (g == 0) {
   40 ( 0.00%)  			g = slab_data->bitmap[++group];
    .           		}
  234 ( 0.01%)  		size_t shift = group << LG_BITMAP_GROUP_NBITS;
    .           		size_t pop = popcount_lu(g);
  117 ( 0.00%)  		if (pop > (cnt - i)) {
   39 ( 0.00%)  			pop = cnt - i;
    .           		}
    .           
    .           		/*
    .           		 * Load from memory locations only once, outside the
    .           		 * hot loop below.
    .           		 */
   78 ( 0.00%)  		uintptr_t base = (uintptr_t)edata_addr_get(slab);
  282 ( 0.01%)  		uintptr_t regsize = (uintptr_t)bin_info->reg_size;
  797 ( 0.03%)  		while (pop--) {
    .           			size_t bit = cfs_lu(&g);
1,186 ( 0.04%)  			size_t regind = shift + bit;
6,054 ( 0.21%)  			*(ptrs + i) = (void *)(base + regsize * regind);
    .           
  662 ( 0.02%)  			i++;
    .           		}
   39 ( 0.00%)  		slab_data->bitmap[group] = g;
    .           	}
    .           #endif
   93 ( 0.00%)  	edata_nfree_sub(slab, cnt);
    .           }
    .           
    .           static void
    .           arena_large_malloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
    .           	szind_t index, hindex;
    .           
    .           	cassert(config_stats);
    .           
    3 ( 0.00%)  	if (usize < SC_LARGE_MINCLASS) {
    .           		usize = SC_LARGE_MINCLASS;
    .           	}
    .           	index = sz_size2index(usize);
    7 ( 0.00%)  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
    .           
    .           	locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
    .           	    &arena->stats.lstats[hindex].nmalloc, 1);
    .           }
    .           
    .           static void
    .           arena_large_dalloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
    .           	szind_t index, hindex;
-- line 308 ----------------------------------------
-- line 323 ----------------------------------------
    .           arena_large_ralloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t oldusize,
    .               size_t usize) {
    .           	arena_large_malloc_stats_update(tsdn, arena, usize);
    .           	arena_large_dalloc_stats_update(tsdn, arena, oldusize);
    .           }
    .           
    .           edata_t *
    .           arena_extent_alloc_large(tsdn_t *tsdn, arena_t *arena, size_t usize,
   16 ( 0.00%)      size_t alignment, bool zero) {
    1 ( 0.00%)  	bool deferred_work_generated = false;
    .           	szind_t szind = sz_size2index(usize);
    3 ( 0.00%)  	size_t esize = usize + sz_large_pad;
    .           
    .           	bool guarded = san_large_extent_decide_guard(tsdn,
    .           	    arena_get_ehooks(arena), esize, alignment);
   12 ( 0.00%)  	edata_t *edata = pa_alloc(tsdn, &arena->pa_shard, esize, alignment,
6,496 ( 0.23%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pa.c:_rjem_je_pa_alloc (1x)
    .           	    /* slab */ false, szind, zero, guarded, &deferred_work_generated);
    .           	assert(deferred_work_generated == false);
    .           
    3 ( 0.00%)  	if (edata != NULL) {
    .           		if (config_stats) {
    .           			LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
    .           			arena_large_malloc_stats_update(tsdn, arena, usize);
    .           			LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
    .           		}
    .           	}
    .           
    .           	if (edata != NULL && sz_large_pad != 0) {
    .           		arena_cache_oblivious_randomize(tsdn, arena, edata, alignment);
    .           	}
    .           
    .           	return edata;
   11 ( 0.00%)  }
    .           
    .           void
    .           arena_extent_dalloc_large_prep(tsdn_t *tsdn, arena_t *arena, edata_t *edata) {
    .           	if (config_stats) {
    .           		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
    .           		arena_large_dalloc_stats_update(tsdn, arena,
    .           		    edata_usize_get(edata));
    .           		LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
-- line 363 ----------------------------------------
-- line 587 ----------------------------------------
    .           	edata_heap_remove(&bin->slabs_nonfull, slab);
    .           	if (config_stats) {
    .           		bin->stats.nonfull_slabs--;
    .           	}
    .           }
    .           
    .           static edata_t *
    .           arena_bin_slabs_nonfull_tryget(bin_t *bin) {
  442 ( 0.02%)  	edata_t *slab = edata_heap_remove_first(&bin->slabs_nonfull);
  372 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_remove_first (62x)
  124 ( 0.00%)  	if (slab == NULL) {
    .           		return NULL;
    .           	}
    .           	if (config_stats) {
    .           		bin->stats.reslabs++;
    .           		bin->stats.nonfull_slabs--;
    .           	}
    .           	return slab;
    .           }
-- line 604 ----------------------------------------
-- line 606 ----------------------------------------
    .           static void
    .           arena_bin_slabs_full_insert(arena_t *arena, bin_t *bin, edata_t *slab) {
    .           	assert(edata_nfree_get(slab) == 0);
    .           	/*
    .           	 *  Tracking extents is required by arena_reset, which is not allowed
    .           	 *  for auto arenas.  Bypass this step to avoid touching the edata
    .           	 *  linkage (often results in cache misses) for auto arenas.
    .           	 */
   45 ( 0.00%)  	if (arena_is_auto(arena)) {
    .           		return;
    .           	}
    .           	edata_list_active_append(&bin->slabs_full, slab);
    .           }
    .           
    .           static void
    .           arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, edata_t *slab) {
    .           	if (arena_is_auto(arena)) {
-- line 622 ----------------------------------------
-- line 824 ----------------------------------------
    .           	 * the metadata in this base anymore.
    .           	 */
    .           	arena_prepare_base_deletion(tsd, arena->base);
    .           	base_delete(tsd_tsdn(tsd), arena->base);
    .           }
    .           
    .           static edata_t *
    .           arena_slab_alloc(tsdn_t *tsdn, arena_t *arena, szind_t binind, unsigned binshard,
  465 ( 0.02%)      const bin_info_t *bin_info) {
   31 ( 0.00%)  	bool deferred_work_generated = false;
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, 0);
    .           
    .           	bool guarded = san_slab_extent_decide_guard(tsdn,
    .           	    arena_get_ehooks(arena));
  403 ( 0.01%)  	edata_t *slab = pa_alloc(tsdn, &arena->pa_shard, bin_info->slab_size,
86,658 ( 3.07%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pa.c:_rjem_je_pa_alloc (31x)
    .           	    /* alignment */ PAGE, /* slab */ true, /* szind */ binind,
    .           	     /* zero */ false, guarded, &deferred_work_generated);
    .           
   93 ( 0.00%)  	if (deferred_work_generated) {
    .           		arena_handle_deferred_work(tsdn, arena);
    .           	}
    .           
   62 ( 0.00%)  	if (slab == NULL) {
    .           		return NULL;
    .           	}
    .           	assert(edata_slab_get(slab));
    .           
    .           	/* Initialize slab internals. */
    .           	slab_data_t *slab_data = edata_slab_data_get(slab);
    .           	edata_nfree_binshard_set(slab, bin_info->nregs, binshard);
  124 ( 0.00%)  	bitmap_init(slab_data->bitmap, &bin_info->bitmap_info, false);
  958 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bitmap.c:_rjem_je_bitmap_init (31x)
    .           
    .           	return slab;
  341 ( 0.01%)  }
    .           
    .           /*
    .            * Before attempting the _with_fresh_slab approaches below, the _no_fresh_slab
    .            * variants (i.e. through slabcur and nonfull) must be tried first.
    .            */
    .           static void
    .           arena_bin_refill_slabcur_with_fresh_slab(tsdn_t *tsdn, arena_t *arena,
    .               bin_t *bin, szind_t binind, edata_t *fresh_slab) {
-- line 866 ----------------------------------------
-- line 868 ----------------------------------------
    .           	/* Only called after slabcur and nonfull both failed. */
    .           	assert(bin->slabcur == NULL);
    .           	assert(edata_heap_first(&bin->slabs_nonfull) == NULL);
    .           	assert(fresh_slab != NULL);
    .           
    .           	/* A new slab from arena_slab_alloc() */
    .           	assert(edata_nfree_get(fresh_slab) == bin_infos[binind].nregs);
    .           	if (config_stats) {
   31 ( 0.00%)  		bin->stats.nslabs++;
   31 ( 0.00%)  		bin->stats.curslabs++;
    .           	}
   31 ( 0.00%)  	bin->slabcur = fresh_slab;
    .           }
    .           
    .           /* Refill slabcur and then alloc using the fresh slab */
    .           static void *
    .           arena_bin_malloc_with_fresh_slab(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
    .               szind_t binind, edata_t *fresh_slab) {
    .           	malloc_mutex_assert_owner(tsdn, &bin->lock);
    .           	arena_bin_refill_slabcur_with_fresh_slab(tsdn, arena, bin, binind,
-- line 887 ----------------------------------------
-- line 897 ----------------------------------------
    .           	/* Only called after arena_slab_reg_alloc[_batch] failed. */
    .           	assert(bin->slabcur == NULL || edata_nfree_get(bin->slabcur) == 0);
    .           
    .           	if (bin->slabcur != NULL) {
    .           		arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
    .           	}
    .           
    .           	/* Look for a usable slab. */
   62 ( 0.00%)  	bin->slabcur = arena_bin_slabs_nonfull_tryget(bin);
    .           	assert(bin->slabcur == NULL || edata_nfree_get(bin->slabcur) > 0);
    .           
    .           	return (bin->slabcur == NULL);
    .           }
    .           
    .           bin_t *
    .           arena_bin_choose(tsdn_t *tsdn, arena_t *arena, szind_t binind,
    .               unsigned *binshard_p) {
    .           	unsigned binshard;
   88 ( 0.00%)  	if (tsdn_null(tsdn) || tsd_arena_get(tsdn_tsd(tsdn)) == NULL) {
    .           		binshard = 0;
    .           	} else {
   22 ( 0.00%)  		binshard = tsd_binshardsp_get(tsdn_tsd(tsdn))->binshard[binind];
    .           	}
    .           	assert(binshard < bin_infos[binind].n_shards);
    .           	if (binshard_p != NULL) {
    .           		*binshard_p = binshard;
    .           	}
    .           	return arena_get_bin(arena, binind, binshard);
    .           }
    .           
    .           void
    .           arena_cache_bin_fill_small(tsdn_t *tsdn, arena_t *arena,
    .               cache_bin_t *cache_bin, cache_bin_info_t *cache_bin_info, szind_t binind,
  330 ( 0.01%)      const unsigned nfill) {
    .           	assert(cache_bin_ncached_get_local(cache_bin, cache_bin_info) == 0);
    .           
   88 ( 0.00%)  	const bin_info_t *bin_info = &bin_infos[binind];
    .           
    .           	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nfill);
    .           	cache_bin_init_ptr_array_for_fill(cache_bin, cache_bin_info, &ptrs,
    .           	    nfill);
    .           	/*
    .           	 * Bin-local resources are used first: 1) bin->slabcur, and 2) nonfull
    .           	 * slabs.  After both are exhausted, new slabs will be allocated through
    .           	 * arena_slab_alloc().
-- line 941 ----------------------------------------
-- line 956 ----------------------------------------
    .           	 * made_progress below, initialized to true to jump start the first
    .           	 * iteration.
    .           	 *
    .           	 * In other words (again), the loop will only terminate early (i.e. stop
    .           	 * with filled < nfill) after going through the three steps: a) bin
    .           	 * local exhausted, b) unlock and slab_alloc returns null, c) re-lock
    .           	 * and bin local fails again.
    .           	 */
   22 ( 0.00%)  	bool made_progress = true;
   22 ( 0.00%)  	edata_t *fresh_slab = NULL;
    .           	bool alloc_and_retry = false;
   22 ( 0.00%)  	unsigned filled = 0;
    .           	unsigned binshard;
    .           	bin_t *bin = arena_bin_choose(tsdn, arena, binind, &binshard);
    .           
    .           label_refill:
    .           	malloc_mutex_lock(tsdn, &bin->lock);
    .           
  455 ( 0.02%)  	while (filled < nfill) {
    .           		/* Try batch-fill from slabcur first. */
   93 ( 0.00%)  		edata_t *slabcur = bin->slabcur;
  266 ( 0.01%)  		if (slabcur != NULL && edata_nfree_get(slabcur) > 0) {
  124 ( 0.00%)  			unsigned tofill = nfill - filled;
    .           			unsigned nfree = edata_nfree_get(slabcur);
   93 ( 0.00%)  			unsigned cnt = tofill < nfree ? tofill : nfree;
    .           
    .           			arena_slab_reg_alloc_batch(slabcur, bin_info, cnt,
   31 ( 0.00%)  			    &ptrs.ptr[filled]);
   31 ( 0.00%)  			made_progress = true;
   31 ( 0.00%)  			filled += cnt;
    .           			continue;
    .           		}
    .           		/* Next try refilling slabcur from nonfull slabs. */
    .           		if (!arena_bin_refill_slabcur_no_fresh_slab(tsdn, arena, bin)) {
    .           			assert(bin->slabcur != NULL);
    .           			continue;
    .           		}
    .           
    .           		/* Then see if a new slab was reserved already. */
  186 ( 0.01%)  		if (fresh_slab != NULL) {
    .           			arena_bin_refill_slabcur_with_fresh_slab(tsdn, arena,
    .           			    bin, binind, fresh_slab);
    .           			assert(bin->slabcur != NULL);
   31 ( 0.00%)  			fresh_slab = NULL;
   31 ( 0.00%)  			continue;
    .           		}
    .           
    .           		/* Try slab_alloc if made progress (or never did slab_alloc). */
  155 ( 0.01%)  		if (made_progress) {
    .           			assert(bin->slabcur == NULL);
    .           			assert(fresh_slab == NULL);
    .           			alloc_and_retry = true;
    .           			/* Alloc a new slab then come back. */
    .           			break;
    .           		}
    .           
    .           		/* OOM. */
    .           
    .           		assert(fresh_slab == NULL);
    .           		assert(!alloc_and_retry);
    .           		break;
    .           	} /* while (filled < nfill) loop. */
    .           
    .           	if (config_stats && !alloc_and_retry) {
   44 ( 0.00%)  		bin->stats.nmalloc += filled;
  132 ( 0.00%)  		bin->stats.nrequests += cache_bin->tstats.nrequests;
    .           		bin->stats.curregs += filled;
   22 ( 0.00%)  		bin->stats.nfills++;
   22 ( 0.00%)  		cache_bin->tstats.nrequests = 0;
    .           	}
    .           
    .           	malloc_mutex_unlock(tsdn, &bin->lock);
    .           
    .           	if (alloc_and_retry) {
    .           		assert(fresh_slab == NULL);
    .           		assert(filled < nfill);
    .           		assert(made_progress);
    .           
  217 ( 0.01%)  		fresh_slab = arena_slab_alloc(tsdn, arena, binind, binshard,
89,631 ( 3.17%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:arena_slab_alloc (31x)
    .           		    bin_info);
    .           		/* fresh_slab NULL case handled in the for loop. */
    .           
    .           		alloc_and_retry = false;
   31 ( 0.00%)  		made_progress = false;
    .           		goto label_refill;
    .           	}
    .           	assert(filled == nfill || (fresh_slab == NULL && !made_progress));
    .           
    .           	/* Release if allocated but not used. */
   44 ( 0.00%)  	if (fresh_slab != NULL) {
    .           		assert(edata_nfree_get(fresh_slab) == bin_info->nregs);
    .           		arena_slab_dalloc(tsdn, arena, fresh_slab);
    .           		fresh_slab = NULL;
    .           	}
    .           
    .           	cache_bin_finish_fill(cache_bin, cache_bin_info, &ptrs, filled);
    .           	arena_decay_tick(tsdn, arena);
  242 ( 0.01%)  }
    .           
    .           size_t
    .           arena_fill_small_fresh(tsdn_t *tsdn, arena_t *arena, szind_t binind,
    .               void **ptrs, size_t nfill, bool zero) {
    .           	assert(binind < SC_NBINS);
    .           	const bin_info_t *bin_info = &bin_infos[binind];
    .           	const size_t nregs = bin_info->nregs;
    .           	assert(nregs > 0);
-- line 1061 ----------------------------------------
-- line 1202 ----------------------------------------
    .           	if (likely(size <= SC_SMALL_MAXCLASS)) {
    .           		return arena_malloc_small(tsdn, arena, ind, zero);
    .           	}
    .           	return large_malloc(tsdn, arena, sz_index2size(ind), zero);
    .           }
    .           
    .           void *
    .           arena_palloc(tsdn_t *tsdn, arena_t *arena, size_t usize, size_t alignment,
   14 ( 0.00%)      bool zero, tcache_t *tcache) {
    .           	void *ret;
    .           
    2 ( 0.00%)  	if (usize <= SC_SMALL_MAXCLASS) {
    .           		/* Small; alignment doesn't require special slab placement. */
    .           
    .           		/* usize should be a result of sz_sa2u() */
    .           		assert((usize & (alignment - 1)) == 0);
    .           
    .           		/*
    .           		 * Small usize can't come from an alignment larger than a page.
    .           		 */
    .           		assert(alignment <= PAGE);
    .           
    .           		ret = arena_malloc(tsdn, arena, usize, sz_size2index(usize),
    .           		    zero, tcache, true);
    .           	} else {
    2 ( 0.00%)  		if (likely(alignment <= CACHELINE)) {
    .           			ret = large_malloc(tsdn, arena, usize, zero);
    .           		} else {
    4 ( 0.00%)  			ret = large_palloc(tsdn, arena, usize, alignment, zero);
6,688 ( 0.24%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/large.c:_rjem_je_large_palloc (1x)
    .           		}
    .           	}
    .           	return ret;
    7 ( 0.00%)  }
    .           
    .           void
    .           arena_prof_promote(tsdn_t *tsdn, void *ptr, size_t usize) {
    .           	cassert(config_prof);
    .           	assert(ptr != NULL);
    .           	assert(isalloc(tsdn, ptr) == SC_LARGE_MINCLASS);
    .           	assert(usize <= SC_SMALL_MAXCLASS);
    .           
-- line 1242 ----------------------------------------
-- line 1389 ----------------------------------------
    .           	arena_t *arena = arena_get_from_edata(edata);
    .           
    .           	arena_dalloc_bin(tsdn, arena, edata, ptr);
    .           	arena_decay_tick(tsdn, arena);
    .           }
    .           
    .           bool
    .           arena_ralloc_no_move(tsdn_t *tsdn, void *ptr, size_t oldsize, size_t size,
  551 ( 0.02%)      size_t extra, bool zero, size_t *newsize) {
    .           	bool ret;
    .           	/* Calls with non-zero extra had to clamp extra. */
    .           	assert(extra == 0 || size + extra <= SC_LARGE_MAXCLASS);
    .           
    .           	edata_t *edata = emap_edata_lookup(tsdn, &arena_emap_global, ptr);
   87 ( 0.00%)  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
    .           		ret = true;
    .           		goto done;
    .           	}
    .           
    .           	size_t usize_min = sz_s2u(size);
   29 ( 0.00%)  	size_t usize_max = sz_s2u(size + extra);
  203 ( 0.01%)  	if (likely(oldsize <= SC_SMALL_MAXCLASS && usize_min
    .           	    <= SC_SMALL_MAXCLASS)) {
    .           		/*
    .           		 * Avoid moving the allocation if the size class can be left the
    .           		 * same.
    .           		 */
    .           		assert(bin_infos[sz_size2index(oldsize)].reg_size ==
    .           		    oldsize);
   58 ( 0.00%)  		if ((usize_max > SC_SMALL_MAXCLASS
   58 ( 0.00%)  		    || sz_size2index(usize_max) != sz_size2index(oldsize))
  174 ( 0.01%)  		    && (size > oldsize || usize_max < oldsize)) {
   58 ( 0.00%)  			ret = true;
    .           			goto done;
    .           		}
    .           
    .           		arena_t *arena = arena_get_from_edata(edata);
    .           		arena_decay_tick(tsdn, arena);
    .           		ret = false;
    .           	} else if (oldsize >= SC_LARGE_MINCLASS
    .           	    && usize_max >= SC_LARGE_MINCLASS) {
    .           		ret = large_ralloc_no_move(tsdn, edata, usize_min, usize_max,
    .           		    zero);
    .           	} else {
    .           		ret = true;
    .           	}
    .           done:
    .           	assert(edata == emap_edata_lookup(tsdn, &arena_emap_global, ptr));
   29 ( 0.00%)  	*newsize = edata_usize_get(edata);
    .           
    .           	return ret;
  319 ( 0.01%)  }
    .           
    .           static void *
    .           arena_ralloc_move_helper(tsdn_t *tsdn, arena_t *arena, size_t usize,
    .               size_t alignment, bool zero, tcache_t *tcache) {
   58 ( 0.00%)  	if (alignment == 0) {
    .           		return arena_malloc(tsdn, arena, usize, sz_size2index(usize),
    .           		    zero, tcache, true);
    .           	}
    .           	usize = sz_sa2u(usize, alignment);
    .           	if (unlikely(usize == 0 || usize > SC_LARGE_MAXCLASS)) {
    .           		return NULL;
    .           	}
    .           	return ipalloct(tsdn, usize, alignment, zero, tcache, arena);
    .           }
    .           
    .           void *
    .           arena_ralloc(tsdn_t *tsdn, arena_t *arena, void *ptr, size_t oldsize,
    .               size_t size, size_t alignment, bool zero, tcache_t *tcache,
  609 ( 0.02%)      hook_ralloc_args_t *hook_args) {
   58 ( 0.00%)  	size_t usize = alignment == 0 ? sz_s2u(size) : sz_sa2u(size, alignment);
  145 ( 0.01%)  	if (unlikely(usize == 0 || size > SC_LARGE_MAXCLASS)) {
    .           		return NULL;
    .           	}
    .           
   58 ( 0.00%)  	if (likely(usize <= SC_SMALL_MAXCLASS)) {
    .           		/* Try to avoid moving the allocation. */
    .           		UNUSED size_t newsize;
  464 ( 0.02%)  		if (!arena_ralloc_no_move(tsdn, ptr, oldsize, usize, 0, zero,
2,987 ( 0.11%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_ralloc_no_move (29x)
    .           		    &newsize)) {
    .           			hook_invoke_expand(hook_args->is_realloc
    .           			    ? hook_expand_realloc : hook_expand_rallocx,
    .           			    ptr, oldsize, usize, (uintptr_t)ptr,
    .           			    hook_args->args);
    .           			return ptr;
    .           		}
    .           	}
-- line 1476 ----------------------------------------
-- line 1482 ----------------------------------------
    .           	}
    .           
    .           	/*
    .           	 * size and oldsize are different enough that we need to move the
    .           	 * object.  In that case, fall back to allocating new space and copying.
    .           	 */
    .           	void *ret = arena_ralloc_move_helper(tsdn, arena, usize, alignment,
    .           	    zero, tcache);
   58 ( 0.00%)  	if (ret == NULL) {
    .           		return NULL;
    .           	}
    .           
  203 ( 0.01%)  	hook_invoke_alloc(hook_args->is_realloc
  696 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/hook.c:_rjem_je_hook_invoke_alloc (29x)
    .           	    ? hook_alloc_realloc : hook_alloc_rallocx, ret, (uintptr_t)ret,
   29 ( 0.00%)  	    hook_args->args);
  174 ( 0.01%)  	hook_invoke_dalloc(hook_args->is_realloc
  696 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/hook.c:_rjem_je_hook_invoke_dalloc (29x)
    .           	    ? hook_dalloc_realloc : hook_dalloc_rallocx, ptr, hook_args->args);
    .           
    .           	/*
    .           	 * Junk/zero-filling were already done by
    .           	 * ipalloc()/arena_malloc().
    .           	 */
   87 ( 0.00%)  	size_t copysize = (usize < oldsize) ? usize : oldsize;
    .           	memcpy(ret, ptr, copysize);
    .           	isdalloct(tsdn, ptr, oldsize, tcache, NULL, true);
    .           	return ret;
  348 ( 0.01%)  }
    .           
    .           ehooks_t *
    .           arena_get_ehooks(arena_t *arena) {
   65 ( 0.00%)  	return base_ehooks_get(arena->base);
   93 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_ehooks_get (31x)
    .           }
    .           
    .           extent_hooks_t *
    .           arena_set_extent_hooks(tsd_t *tsd, arena_t *arena,
    .               extent_hooks_t *extent_hooks) {
    .           	background_thread_info_t *info;
    .           	if (have_background_thread) {
    .           		info = arena_background_thread_info_get(arena);
-- line 1520 ----------------------------------------
-- line 1546 ----------------------------------------
    .           
    .           ssize_t
    .           arena_dirty_decay_ms_default_get(void) {
    .           	return atomic_load_zd(&dirty_decay_ms_default, ATOMIC_RELAXED);
    .           }
    .           
    .           bool
    .           arena_dirty_decay_ms_default_set(ssize_t decay_ms) {
    4 ( 0.00%)  	if (!decay_ms_valid(decay_ms)) {
    9 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/decay.c:_rjem_je_decay_ms_valid (1x)
    .           		return true;
    .           	}
    .           	atomic_store_zd(&dirty_decay_ms_default, decay_ms, ATOMIC_RELAXED);
    .           	return false;
    .           }
    .           
    .           ssize_t
    .           arena_muzzy_decay_ms_default_get(void) {
    .           	return atomic_load_zd(&muzzy_decay_ms_default, ATOMIC_RELAXED);
    .           }
    .           
    .           bool
    .           arena_muzzy_decay_ms_default_set(ssize_t decay_ms) {
    4 ( 0.00%)  	if (!decay_ms_valid(decay_ms)) {
    9 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/decay.c:_rjem_je_decay_ms_valid (1x)
    .           		return true;
    .           	}
    .           	atomic_store_zd(&muzzy_decay_ms_default, decay_ms, ATOMIC_RELAXED);
    .           	return false;
    .           }
    .           
    .           bool
    .           arena_retain_grow_limit_get_set(tsd_t *tsd, arena_t *arena, size_t *old_limit,
    .               size_t *new_limit) {
    .           	assert(opt_retain);
    .           	return pac_retain_grow_limit_get_set(tsd_tsdn(tsd),
    .           	    &arena->pa_shard.pac, old_limit, new_limit);
    .           }
    .           
    .           unsigned
    2 ( 0.00%)  arena_nthreads_get(arena_t *arena, bool internal) {
    .           	return atomic_load_u(&arena->nthreads[internal], ATOMIC_RELAXED);
    2 ( 0.00%)  }
    .           
    .           void
    2 ( 0.00%)  arena_nthreads_inc(arena_t *arena, bool internal) {
    .           	atomic_fetch_add_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
    2 ( 0.00%)  }
    .           
    .           void
    .           arena_nthreads_dec(arena_t *arena, bool internal) {
    .           	atomic_fetch_sub_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
    .           }
    .           
    .           arena_t *
   13 ( 0.00%)  arena_new(tsdn_t *tsdn, unsigned ind, const arena_config_t *config) {
    .           	arena_t *arena;
    .           	base_t *base;
    .           	unsigned i;
    .           
    2 ( 0.00%)  	if (ind == 0) {
    2 ( 0.00%)  		base = b0get();
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_b0get (1x)
    .           	} else {
    .           		base = base_new(tsdn, ind, config->extent_hooks,
    .           		    config->metadata_use_hooks);
    .           		if (base == NULL) {
    .           			return NULL;
    .           		}
    .           	}
    .           
    5 ( 0.00%)  	size_t arena_size = sizeof(arena_t) + sizeof(bin_t) * nbins_total;
    5 ( 0.00%)  	arena = (arena_t *)base_alloc(tsdn, base, arena_size, CACHELINE);
  465 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_alloc (1x)
    2 ( 0.00%)  	if (arena == NULL) {
    .           		goto label_error;
    .           	}
    .           
    .           	atomic_store_u(&arena->nthreads[0], 0, ATOMIC_RELAXED);
    .           	atomic_store_u(&arena->nthreads[1], 0, ATOMIC_RELAXED);
    1 ( 0.00%)  	arena->last_thd = NULL;
    .           
    .           	if (config_stats) {
    .           		if (arena_stats_init(tsdn, &arena->stats)) {
    .           			goto label_error;
    .           		}
    .           
    2 ( 0.00%)  		ql_new(&arena->tcache_ql);
    .           		ql_new(&arena->cache_bin_array_descriptor_ql);
    7 ( 0.00%)  		if (malloc_mutex_init(&arena->tcache_ql_mtx, "tcache_ql",
  127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
    .           		    WITNESS_RANK_TCACHE_QL, malloc_mutex_rank_exclusive)) {
    .           			goto label_error;
    .           		}
    .           	}
    .           
    1 ( 0.00%)  	atomic_store_u(&arena->dss_prec, (unsigned)extent_dss_prec_get(),
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent_dss.c:_rjem_je_extent_dss_prec_get (1x)
    .           	    ATOMIC_RELAXED);
    .           
    .           	edata_list_active_init(&arena->large);
    7 ( 0.00%)  	if (malloc_mutex_init(&arena->large_mtx, "arena_large",
  127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
    .           	    WITNESS_RANK_ARENA_LARGE, malloc_mutex_rank_exclusive)) {
    .           		goto label_error;
    .           	}
    .           
    .           	nstime_t cur_time;
    3 ( 0.00%)  	nstime_init_update(&cur_time);
   32 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/nstime.c:_rjem_je_nstime_init_update (1x)
   18 ( 0.00%)  	if (pa_shard_init(tsdn, &arena->pa_shard, &arena_pa_central_global,
12,298 ( 0.44%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pa.c:_rjem_je_pa_shard_init (1x)
    .           	    &arena_emap_global, base, ind, &arena->stats.pa_shard_stats,
    .           	    LOCKEDINT_MTX(arena->stats.mtx), &cur_time, oversize_threshold,
    .           	    arena_dirty_decay_ms_default_get(),
    .           	    arena_muzzy_decay_ms_default_get())) {
    .           		goto label_error;
    .           	}
    .           
    .           	/* Initialize bins. */
    .           	atomic_store_u(&arena->binshard_next, 0, ATOMIC_RELEASE);
  150 ( 0.01%)  	for (i = 0; i < nbins_total; i++) {
   72 ( 0.00%)  		bool err = bin_init(&arena->bins[i]);
5,688 ( 0.20%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin.c:_rjem_je_bin_init (36x)
   72 ( 0.00%)  		if (err) {
    .           			goto label_error;
    .           		}
    .           	}
    .           
    1 ( 0.00%)  	arena->base = base;
    .           	/* Set arena before creating background threads. */
    3 ( 0.00%)  	arena_set(ind, arena);
    5 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_arena_set (1x)
    1 ( 0.00%)  	arena->ind = ind;
    .           
    2 ( 0.00%)  	nstime_init_update(&arena->create_time);
   32 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/nstime.c:_rjem_je_nstime_init_update (1x)
    .           
    .           	/*
    .           	 * We turn on the HPA if set to.  There are two exceptions:
    .           	 * - Custom extent hooks (we should only return memory allocated from
    .           	 *   them in that case).
    .           	 * - Arena 0 initialization.  In this case, we're mid-bootstrapping, and
    .           	 *   so arena_hpa_global is not yet initialized.
    .           	 */
    3 ( 0.00%)  	if (opt_hpa && ehooks_are_default(base_ehooks_get(base)) && ind != 0) {
    .           		hpa_shard_opts_t hpa_shard_opts = opt_hpa_opts;
    .           		hpa_shard_opts.deferral_allowed = background_thread_enabled();
    .           		if (pa_shard_enable_hpa(tsdn, &arena->pa_shard,
    .           		    &hpa_shard_opts, &opt_hpa_sec_opts)) {
    .           			goto label_error;
    .           		}
    .           	}
    .           
    .           	/* We don't support reentrancy for arena 0 bootstrapping. */
    2 ( 0.00%)  	if (ind != 0) {
    .           		/*
    .           		 * If we're here, then arena 0 already exists, so bootstrapping
    .           		 * is done enough that we should have tsd.
    .           		 */
    .           		assert(!tsdn_null(tsdn));
    .           		pre_reentrancy(tsdn_tsd(tsdn), arena);
    .           		if (test_hooks_arena_new_hook) {
    .           			test_hooks_arena_new_hook();
-- line 1697 ----------------------------------------
-- line 1700 ----------------------------------------
    .           	}
    .           
    .           	return arena;
    .           label_error:
    .           	if (ind != 0) {
    .           		base_delete(tsdn, base);
    .           	}
    .           	return NULL;
   12 ( 0.00%)  }
    .           
    .           arena_t *
    .           arena_choose_huge(tsd_t *tsd) {
    .           	/* huge_arena_ind can be 0 during init (will use a0). */
    .           	if (huge_arena_ind == 0) {
    .           		assert(!malloc_initialized());
    .           	}
    .           
-- line 1716 ----------------------------------------
-- line 1737 ----------------------------------------
    .           			    extent_state_muzzy, 0);
    .           		}
    .           	}
    .           
    .           	return huge_arena;
    .           }
    .           
    .           bool
    2 ( 0.00%)  arena_init_huge(void) {
    .           	bool huge_enabled;
    .           
    .           	/* The threshold should be large size class. */
    5 ( 0.00%)  	if (opt_oversize_threshold > SC_LARGE_MAXCLASS ||
    .           	    opt_oversize_threshold < SC_LARGE_MINCLASS) {
    .           		opt_oversize_threshold = 0;
    1 ( 0.00%)  		oversize_threshold = SC_LARGE_MAXCLASS + PAGE;
    .           		huge_enabled = false;
    .           	} else {
    .           		/* Reserve the index for the huge arena. */
    2 ( 0.00%)  		huge_arena_ind = narenas_total_get();
    3 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_narenas_total_get (1x)
    1 ( 0.00%)  		oversize_threshold = opt_oversize_threshold;
    1 ( 0.00%)  		huge_enabled = true;
    .           	}
    .           
    .           	return huge_enabled;
    2 ( 0.00%)  }
    .           
    .           bool
    .           arena_is_huge(unsigned arena_ind) {
    .           	if (huge_arena_ind == 0) {
    .           		return false;
    .           	}
    .           	return (arena_ind == huge_arena_ind);
    .           }
    .           
    .           bool
   11 ( 0.00%)  arena_boot(sc_data_t *sc_data, base_t *base, bool hpa) {
    1 ( 0.00%)  	arena_dirty_decay_ms_default_set(opt_dirty_decay_ms);
    1 ( 0.00%)  	arena_muzzy_decay_ms_default_set(opt_muzzy_decay_ms);
   59 ( 0.00%)  	for (unsigned i = 0; i < SC_NBINS; i++) {
    .           		sc_t *sc = &sc_data->sc[i];
   78 ( 0.00%)  		div_init(&arena_binind_div_info[i],
  288 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/div.c:_rjem_je_div_init (36x)
  219 ( 0.01%)  		    (1U << sc->lg_base) + (sc->ndelta << sc->lg_delta));
    .           	}
    .           
    4 ( 0.00%)  	uint32_t cur_offset = (uint32_t)offsetof(arena_t, bins);
   17 ( 0.00%)  	for (szind_t i = 0; i < SC_NBINS; i++) {
   36 ( 0.00%)  		arena_bin_offsets[i] = cur_offset;
   72 ( 0.00%)  		nbins_total += bin_infos[i].n_shards;
   72 ( 0.00%)  		cur_offset += (uint32_t)(bin_infos[i].n_shards * sizeof(bin_t));
    .           	}
    5 ( 0.00%)  	return pa_central_init(&arena_pa_central_global, base, hpa,
    5 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pa.c:_rjem_je_pa_central_init (1x)
    .           	    &hpa_hooks_default);
    7 ( 0.00%)  }
    .           
    .           void
    .           arena_prefork0(tsdn_t *tsdn, arena_t *arena) {
    .           	pa_shard_prefork0(tsdn, &arena->pa_shard);
    .           }
    .           
    .           void
    .           arena_prefork1(tsdn_t *tsdn, arena_t *arena) {
-- line 1798 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pac.c
--------------------------------------------------------------------------------
Ir           

-- line 33 ----------------------------------------
  .           		unreachable();
  .           	}
  .           }
  .           
  .           bool
  .           pac_init(tsdn_t *tsdn, pac_t *pac, base_t *base, emap_t *emap,
  .               edata_cache_t *edata_cache, nstime_t *cur_time,
  .               size_t pac_oversize_threshold, ssize_t dirty_decay_ms,
 14 ( 0.00%)      ssize_t muzzy_decay_ms, pac_stats_t *pac_stats, malloc_mutex_t *stats_mtx) {
  .           	unsigned ind = base_ind_get(base);
  .           	/*
  .           	 * Delay coalescing for dirty extents despite the disruptive effect on
  .           	 * memory layout for best-fit extent allocation, since cached extents
  .           	 * are likely to be reused soon after deallocation, and the cost of
  .           	 * merging/splitting extents is non-trivial.
  .           	 */
  7 ( 0.00%)  	if (ecache_init(tsdn, &pac->ecache_dirty, extent_state_dirty, ind,
3,615 ( 0.13%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ecache.c:_rjem_je_ecache_init (1x)
  .           	    /* delay_coalesce */ true)) {
  .           		return true;
  .           	}
  .           	/*
  .           	 * Coalesce muzzy extents immediately, because operations on them are in
  .           	 * the critical path much less often than for dirty extents.
  .           	 */
  8 ( 0.00%)  	if (ecache_init(tsdn, &pac->ecache_muzzy, extent_state_muzzy, ind,
3,615 ( 0.13%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ecache.c:_rjem_je_ecache_init (1x)
  .           	    /* delay_coalesce */ false)) {
  .           		return true;
  .           	}
  .           	/*
  .           	 * Coalesce retained extents immediately, in part because they will
  .           	 * never be evicted (and therefore there's no opportunity for delayed
  .           	 * coalescing), but also because operations on retained extents are not
  .           	 * in the critical path.
  .           	 */
  8 ( 0.00%)  	if (ecache_init(tsdn, &pac->ecache_retained, extent_state_retained,
3,615 ( 0.13%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ecache.c:_rjem_je_ecache_init (1x)
  .           	    ind, /* delay_coalesce */ false)) {
  .           		return true;
  .           	}
  2 ( 0.00%)  	exp_grow_init(&pac->exp_grow);
  4 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/exp_grow.c:_rjem_je_exp_grow_init (1x)
  7 ( 0.00%)  	if (malloc_mutex_init(&pac->grow_mtx, "extent_grow",
127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
  .           	    WITNESS_RANK_EXTENT_GROW, malloc_mutex_rank_exclusive)) {
  .           		return true;
  .           	}
  .           	atomic_store_zu(&pac->oversize_threshold, pac_oversize_threshold,
  .           	    ATOMIC_RELAXED);
  6 ( 0.00%)  	if (decay_init(&pac->decay_dirty, cur_time, dirty_decay_ms)) {
468 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/decay.c:_rjem_je_decay_init (1x)
  .           		return true;
  .           	}
  6 ( 0.00%)  	if (decay_init(&pac->decay_muzzy, cur_time, muzzy_decay_ms)) {
409 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/decay.c:_rjem_je_decay_init (1x)
  .           		return true;
  .           	}
  .           	if (san_bump_alloc_init(&pac->sba)) {
  .           		return true;
  .           	}
  .           
  1 ( 0.00%)  	pac->base = base;
  1 ( 0.00%)  	pac->emap = emap;
  2 ( 0.00%)  	pac->edata_cache = edata_cache;
  2 ( 0.00%)  	pac->stats = pac_stats;
  2 ( 0.00%)  	pac->stats_mtx = stats_mtx;
  .           	atomic_store_zu(&pac->extent_sn_next, 0, ATOMIC_RELAXED);
  .           
 14 ( 0.00%)  	pac->pai.alloc = &pac_alloc_impl;
  .           	pac->pai.alloc_batch = &pai_alloc_batch_default;
  .           	pac->pai.expand = &pac_expand_impl;
  .           	pac->pai.shrink = &pac_shrink_impl;
  .           	pac->pai.dalloc = &pac_dalloc_impl;
  .           	pac->pai.dalloc_batch = &pai_dalloc_batch_default;
  2 ( 0.00%)  	pac->pai.time_until_deferred_work = &pac_time_until_deferred_work;
  .           
  1 ( 0.00%)  	return false;
  8 ( 0.00%)  }
  .           
  .           static inline bool
  .           pac_may_have_muzzy(pac_t *pac) {
  .           	return pac_decay_ms_get(pac, extent_state_muzzy) != 0;
  .           }
  .           
  .           static edata_t *
  .           pac_alloc_real(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, size_t size,
384 ( 0.01%)      size_t alignment, bool zero, bool guarded) {
  .           	assert(!guarded || alignment <= PAGE);
  .           
288 ( 0.01%)  	edata_t *edata = ecache_alloc(tsdn, pac, ehooks, &pac->ecache_dirty,
8,600 ( 0.30%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_ecache_alloc (32x)
  .           	    NULL, size, alignment, zero, guarded);
  .           
160 ( 0.01%)  	if (edata == NULL && pac_may_have_muzzy(pac)) {
  .           		edata = ecache_alloc(tsdn, pac, ehooks, &pac->ecache_muzzy,
  .           		    NULL, size, alignment, zero, guarded);
  .           	}
  .           	if (edata == NULL) {
352 ( 0.01%)  		edata = ecache_alloc_grow(tsdn, pac, ehooks,
74,588 ( 2.64%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_ecache_alloc_grow (32x)
  .           		    &pac->ecache_retained, NULL, size, alignment, zero,
  .           		    guarded);
 96 ( 0.00%)  		if (config_stats && edata != NULL) {
  .           			atomic_fetch_add_zu(&pac->stats->pac_mapped, size,
  .           			    ATOMIC_RELAXED);
  .           		}
  .           	}
  .           
  .           	return edata;
256 ( 0.01%)  }
  .           
  .           static edata_t *
  .           pac_alloc_new_guarded(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, size_t size,
  .               size_t alignment, bool zero, bool frequent_reuse) {
  .           	assert(alignment <= PAGE);
  .           
  .           	edata_t *edata;
  .           	if (san_bump_enabled() && frequent_reuse) {
-- line 142 ----------------------------------------
-- line 158 ----------------------------------------
  .           	    edata_size_get(edata) == size));
  .           
  .           	return edata;
  .           }
  .           
  .           static edata_t *
  .           pac_alloc_impl(tsdn_t *tsdn, pai_t *self, size_t size, size_t alignment,
  .               bool zero, bool guarded, bool frequent_reuse,
480 ( 0.02%)      bool *deferred_work_generated) {
  .           	pac_t *pac = (pac_t *)self;
  .           	ehooks_t *ehooks = pac_ehooks_get(pac);
  .           
  .           	edata_t *edata = NULL;
  .           	/*
  .           	 * The condition is an optimization - not frequently reused guarded
  .           	 * allocations are never put in the ecache.  pac_alloc_real also
  .           	 * doesn't grow retained for guarded allocations.  So pac_alloc_real
  .           	 * for such allocations would always return NULL.
  .           	 * */
 64 ( 0.00%)  	if (!guarded || frequent_reuse) {
352 ( 0.01%)  		edata =	pac_alloc_real(tsdn, pac, ehooks, size, alignment,
84,852 ( 3.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pac.c:pac_alloc_real (32x)
  .           		    zero, guarded);
  .           	}
128 ( 0.00%)  	if (edata == NULL && guarded) {
  .           		/* No cached guarded extents; creating a new one. */
  .           		edata = pac_alloc_new_guarded(tsdn, pac, ehooks, size,
  .           		    alignment, zero, frequent_reuse);
  .           	}
  .           
  .           	return edata;
256 ( 0.01%)  }
  .           
  .           static bool
  .           pac_expand_impl(tsdn_t *tsdn, pai_t *self, edata_t *edata, size_t old_size,
  .               size_t new_size, bool zero, bool *deferred_work_generated) {
  .           	pac_t *pac = (pac_t *)self;
  .           	ehooks_t *ehooks = pac_ehooks_get(pac);
  .           
  .           	size_t mapped_add = 0;
-- line 196 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/fb.h
--------------------------------------------------------------------------------
Ir           

-- line 53 ----------------------------------------
  .           	size_t group_ind = bit / FB_GROUP_BITS;
  .           	size_t bit_ind = bit % FB_GROUP_BITS;
  .           	return (bool)(fb[group_ind] & ((fb_group_t)1 << bit_ind));
  .           }
  .           
  .           static inline void
  .           fb_set(fb_group_t *fb, size_t nbits, size_t bit) {
  .           	assert(bit < nbits);
 64 ( 0.00%)  	size_t group_ind = bit / FB_GROUP_BITS;
  .           	size_t bit_ind = bit % FB_GROUP_BITS;
 96 ( 0.00%)  	fb[group_ind] |= ((fb_group_t)1 << bit_ind);
  .           }
  .           
  .           static inline void
  .           fb_unset(fb_group_t *fb, size_t nbits, size_t bit) {
  .           	assert(bit < nbits);
 31 ( 0.00%)  	size_t group_ind = bit / FB_GROUP_BITS;
  .           	size_t bit_ind = bit % FB_GROUP_BITS;
 93 ( 0.00%)  	fb[group_ind] &= ~((fb_group_t)1 << bit_ind);
  .           }
  .           
  .           
  .           /*
  .            * Some implementation details.  This visitation function lets us apply a group
  .            * visitor to each group in the bitmap (potentially modifying it).  The mask
  .            * indicates which bits are logically part of the visitation.
  .            */
-- line 79 ----------------------------------------
-- line 171 ----------------------------------------
  .            *
  .            * Returns the number of bits in the bitmap if no such bit exists.
  .            */
  .           JEMALLOC_ALWAYS_INLINE ssize_t
  .           fb_find_impl(fb_group_t *fb, size_t nbits, size_t start, bool val,
  .               bool forward) {
  .           	assert(start < nbits);
  .           	size_t ngroups = FB_NGROUPS(nbits);
 95 ( 0.00%)  	ssize_t group_ind = start / FB_GROUP_BITS;
  .           	size_t bit_ind = start % FB_GROUP_BITS;
  .           
  .           	fb_group_t maybe_invert = (val ? 0 : (fb_group_t)-1);
  .           
  .           	fb_group_t group = fb[group_ind];
  .           	group ^= maybe_invert;
  .           	if (forward) {
  .           		/* Only keep ones in bits bit_ind and above. */
380 ( 0.01%)  		group &= ~((1LU << bit_ind) - 1);
  .           	} else {
  .           		/*
  .           		 * Only keep ones in bits bit_ind and below.  You might more
  .           		 * naturally express this as (1 << (bit_ind + 1)) - 1, but
  .           		 * that shifts by an invalid amount if bit_ind is one less than
  .           		 * FB_GROUP_BITS.
  .           		 */
  .           		group &= ((2LU << bit_ind) - 1);
  .           	}
  .           	ssize_t group_ind_bound = forward ? (ssize_t)ngroups : -1;
766 ( 0.03%)  	while (group == 0) {
256 ( 0.01%)  		group_ind += forward ? 1 : -1;
896 ( 0.03%)  		if (group_ind == group_ind_bound) {
  .           			return forward ? (ssize_t)nbits : (ssize_t)-1;
  .           		}
192 ( 0.01%)  		group = fb[group_ind];
  .           		group ^= maybe_invert;
  .           	}
  .           	assert(group != 0);
 31 ( 0.00%)  	size_t bit = forward ? ffs_lu(group) : fls_lu(group);
 62 ( 0.00%)  	size_t pos = group_ind * FB_GROUP_BITS + bit;
  .           	/*
  .           	 * The high bits of a partially filled last group are zeros, so if we're
  .           	 * looking for zeros we don't want to report an invalid result.
  .           	 */
  .           	if (forward && !val && pos > nbits) {
  .           		return nbits;
  .           	}
  .           	return pos;
-- line 217 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/nstime.h
--------------------------------------------------------------------------------
Ir           

-- line 55 ----------------------------------------
  .           };
  .           typedef enum prof_time_res_e prof_time_res_t;
  .           
  .           extern prof_time_res_t opt_prof_time_res;
  .           extern const char *prof_time_res_mode_names[];
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           nstime_init_zero(nstime_t *time) {
356 ( 0.01%)  	nstime_copy(time, &nstime_zero);
464 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/nstime.c:_rjem_je_nstime_copy (116x)
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE bool
  .           nstime_equals_zero(nstime_t *time) {
  .           	int diff = nstime_compare(time, &nstime_zero);
  .           	assert(diff >= 0);
  .           	return diff == 0;
  .           }
-- line 71 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/div.c
--------------------------------------------------------------------------------
Ir           

-- line 22 ----------------------------------------
  .            * So that our initial expression is equal to the quantity we seek, so long as
  .            * (r / d) * (n / 2^k) < 1.
  .            *
  .            * r is a remainder mod d, so r < d and r / d < 1 always. We can make
  .            * n / 2 ^ k < 1 by setting k = 32. This gets us a value of magic that works.
  .            */
  .           
  .           void
 36 ( 0.00%)  div_init(div_info_t *div_info, size_t d) {
  .           	/* Nonsensical. */
  .           	assert(d != 0);
  .           	/*
  .           	 * This would make the value of magic too high to fit into a uint32_t
  .           	 * (we would want magic = 2^32 exactly). This would mess with code gen
  .           	 * on 32-bit machines.
  .           	 */
  .           	assert(d != 1);
  .           
  .           	uint64_t two_to_k = ((uint64_t)1 << 32);
108 ( 0.00%)  	uint32_t magic = (uint32_t)(two_to_k / d);
  .           
  .           	/*
  .           	 * We want magic = ceil(2^k / d), but C gives us floor. We have to
  .           	 * increment it unless the result was exact (i.e. unless d is a power of
  .           	 * two).
  .           	 */
  .           	if (two_to_k % d != 0) {
 72 ( 0.00%)  		magic++;
  .           	}
 36 ( 0.00%)  	div_info->magic = magic;
  .           #ifdef JEMALLOC_DEBUG
  .           	div_info->d = d;
  .           #endif
 36 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/thread_event.h
--------------------------------------------------------------------------------
Ir           

-- line 89 ----------------------------------------
  .            * allocation call.
  .            */
  .           #define C(counter)							\
  .           JEMALLOC_ALWAYS_INLINE void						\
  .           counter##_set(tsd_t *tsd, uint64_t v) {					\
  .           	*tsd_##counter##p_get(tsd) = v;					\
  .           }
  .           
592 ( 0.02%)  ITERATE_OVER_ALL_COUNTERS
  .           #undef C
  .           
  .           /*
  .            * For generating _event_wait getter / setter functions for each individual
  .            * event.
  .            */
  .           #undef E
  .           
-- line 105 ----------------------------------------
-- line 107 ----------------------------------------
  .            * The malloc and free fastpath getters -- use the unsafe getters since tsd may
  .            * be non-nominal, in which case the fast_threshold will be set to 0.  This
  .            * allows checking for events and tsd non-nominal in a single branch.
  .            *
  .            * Note that these can only be used on the fastpath.
  .            */
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_malloc_fastpath_ctx(tsd_t *tsd, uint64_t *allocated, uint64_t *threshold) {
601 ( 0.02%)  	*allocated = *tsd_thread_allocatedp_get_unsafe(tsd);
  .           	*threshold = *tsd_thread_allocated_next_event_fastp_get_unsafe(tsd);
  .           	assert(*threshold <= TE_NEXT_EVENT_FAST_MAX);
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_free_fastpath_ctx(tsd_t *tsd, uint64_t *deallocated, uint64_t *threshold) {
  .           	/* Unsafe getters since this may happen before tsd_init. */
598 ( 0.02%)  	*deallocated = *tsd_thread_deallocatedp_get_unsafe(tsd);
  .           	*threshold = *tsd_thread_deallocated_next_event_fastp_get_unsafe(tsd);
  .           	assert(*threshold <= TE_NEXT_EVENT_FAST_MAX);
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE bool
  .           te_ctx_is_alloc(te_ctx_t *ctx) {
  .           	return ctx->is_alloc;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE uint64_t
  .           te_ctx_current_bytes_get(te_ctx_t *ctx) {
 73 ( 0.00%)  	return *ctx->current;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_ctx_current_bytes_set(te_ctx_t *ctx, uint64_t v) {
  .           	*ctx->current = v;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE uint64_t
  .           te_ctx_last_event_get(te_ctx_t *ctx) {
  .           	return *ctx->last_event;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_ctx_last_event_set(te_ctx_t *ctx, uint64_t v) {
  4 ( 0.00%)  	*ctx->last_event = v;
  .           }
  .           
  .           /* Below 3 for next_event_fast. */
  .           JEMALLOC_ALWAYS_INLINE uint64_t
  .           te_ctx_next_event_fast_get(te_ctx_t *ctx) {
  .           	uint64_t v = *ctx->next_event_fast;
  .           	assert(v <= TE_NEXT_EVENT_FAST_MAX);
  .           	return v;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_ctx_next_event_fast_set(te_ctx_t *ctx, uint64_t v) {
  .           	assert(v <= TE_NEXT_EVENT_FAST_MAX);
  6 ( 0.00%)  	*ctx->next_event_fast = v;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_next_event_fast_set_non_nominal(tsd_t *tsd) {
  .           	/*
  .           	 * Set the fast thresholds to zero when tsd is non-nominal.  Use the
  .           	 * unsafe getter as this may get called during tsd init and clean up.
  .           	 */
  4 ( 0.00%)  	*tsd_thread_allocated_next_event_fastp_get_unsafe(tsd) = 0;
  4 ( 0.00%)  	*tsd_thread_deallocated_next_event_fastp_get_unsafe(tsd) = 0;
  .           }
  .           
  .           /* For next_event.  Setter also updates the fast threshold. */
  .           JEMALLOC_ALWAYS_INLINE uint64_t
  .           te_ctx_next_event_get(te_ctx_t *ctx) {
  6 ( 0.00%)  	return *ctx->next_event;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_ctx_next_event_set(tsd_t *tsd, te_ctx_t *ctx, uint64_t v) {
  2 ( 0.00%)  	*ctx->next_event = v;
  .           	te_recompute_fast_threshold(tsd);
  .           }
  .           
  .           /*
  .            * The function checks in debug mode whether the thread event counters are in
  .            * a consistent state, which forms the invariants before and after each round
  .            * of thread event handling that we can rely on and need to promise.
  .            * The invariants are only temporarily violated in the middle of
-- line 193 ----------------------------------------
-- line 198 ----------------------------------------
  .           te_assert_invariants(tsd_t *tsd) {
  .           	if (config_debug) {
  .           		te_assert_invariants_debug(tsd);
  .           	}
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_ctx_get(tsd_t *tsd, te_ctx_t *ctx, bool is_alloc) {
 73 ( 0.00%)  	ctx->is_alloc = is_alloc;
  .           	if (is_alloc) {
  .           		ctx->current = tsd_thread_allocatedp_get(tsd);
  .           		ctx->last_event = tsd_thread_allocated_last_eventp_get(tsd);
  .           		ctx->next_event = tsd_thread_allocated_next_eventp_get(tsd);
  .           		ctx->next_event_fast =
  .           		    tsd_thread_allocated_next_event_fastp_get(tsd);
  .           	} else {
  .           		ctx->current = tsd_thread_deallocatedp_get(tsd);
-- line 214 ----------------------------------------
-- line 273 ----------------------------------------
  .           JEMALLOC_ALWAYS_INLINE void
  .           te_event_advance(tsd_t *tsd, size_t usize, bool is_alloc) {
  .           	te_assert_invariants(tsd);
  .           
  .           	te_ctx_t ctx;
  .           	te_ctx_get(tsd, &ctx, is_alloc);
  .           
  .           	uint64_t bytes_before = te_ctx_current_bytes_get(&ctx);
146 ( 0.01%)  	te_ctx_current_bytes_set(&ctx, bytes_before + usize);
  .           
  .           	/* The subtraction is intentionally susceptible to underflow. */
292 ( 0.01%)  	if (likely(usize < te_ctx_next_event_get(&ctx) - bytes_before)) {
  .           		te_assert_invariants(tsd);
  .           	} else {
  .           		te_event_trigger(tsd, &ctx);
  .           	}
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           thread_dalloc_event(tsd_t *tsd, size_t usize) {
-- line 292 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c
--------------------------------------------------------------------------------
Ir             

-- line 72 ----------------------------------------
    .           	}
    .           	eset_insert(&ecache->eset, edata);
    .           	return false;
    .           }
    .           
    .           edata_t *
    .           ecache_alloc(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
    .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
  160 ( 0.01%)      bool guarded) {
    .           	assert(size != 0);
    .           	assert(alignment != 0);
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, 0);
    .           
   32 ( 0.00%)  	bool commit = true;
  288 ( 0.01%)  	edata_t *edata = extent_recycle(tsdn, pac, ehooks, ecache, expand_edata,
7,928 ( 0.28%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_recycle (32x)
    .           	    size, alignment, zero, &commit, false, guarded);
    .           	assert(edata == NULL || edata_pai_get(edata) == EXTENT_PAI_PAC);
    .           	assert(edata == NULL || edata_guarded_get(edata) == guarded);
   32 ( 0.00%)  	return edata;
  160 ( 0.01%)  }
    .           
    .           edata_t *
    .           ecache_alloc_grow(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
    .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
  640 ( 0.02%)      bool guarded) {
    .           	assert(size != 0);
    .           	assert(alignment != 0);
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, 0);
    .           
   32 ( 0.00%)  	bool commit = true;
   66 ( 0.00%)  	edata_t *edata = extent_alloc_retained(tsdn, pac, ehooks, expand_edata,
    .           	    size, alignment, zero, &commit, guarded);
    1 ( 0.00%)  	if (edata == NULL) {
    .           		if (opt_retain && expand_edata != NULL) {
    .           			/*
    .           			 * When retain is enabled and trying to expand, we do
    .           			 * not attempt extent_alloc_wrapper which does mmap that
    .           			 * is very unlikely to succeed (unless it happens to be
    .           			 * at the end).
    .           			 */
    .           			return NULL;
-- line 114 ----------------------------------------
-- line 125 ----------------------------------------
    .           		    edata_past_get(expand_edata);
    .           		edata = extent_alloc_wrapper(tsdn, pac, ehooks, new_addr,
    .           		    size, alignment, zero, &commit,
    .           		    /* growing_retained */ false);
    .           	}
    .           
    .           	assert(edata == NULL || edata_pai_get(edata) == EXTENT_PAI_PAC);
    .           	return edata;
  384 ( 0.01%)  }
    .           
    .           void
    .           ecache_dalloc(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
    .               edata_t *edata) {
    .           	assert(edata_base_get(edata) != NULL);
    .           	assert(edata_size_get(edata) != 0);
    .           	assert(edata_pai_get(edata) == EXTENT_PAI_PAC);
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
-- line 141 ----------------------------------------
-- line 245 ----------------------------------------
    .           }
    .           
    .           static void
    .           extent_deactivate_locked_impl(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    .               edata_t *edata) {
    .           	malloc_mutex_assert_owner(tsdn, &ecache->mtx);
    .           	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
    .           
  160 ( 0.01%)  	emap_update_edata_state(tsdn, pac->emap, edata, ecache->state);
2,325 ( 0.08%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_update_edata_state (31x)
  128 ( 0.00%)  	eset_t *eset = edata_guarded_get(edata) ? &ecache->guarded_eset :
    .           	    &ecache->eset;
   64 ( 0.00%)  	eset_insert(eset, edata);
4,712 ( 0.17%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_insert (31x)
    .           }
    .           
    .           static void
    .           extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    .               edata_t *edata) {
    .           	assert(edata_state_get(edata) == extent_state_active);
    .           	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
    .           }
-- line 264 ----------------------------------------
-- line 272 ----------------------------------------
    .           
    .           static void
    .           extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    .               edata_t *edata) {
    .           	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
    .           	assert(edata_state_get(edata) == ecache->state ||
    .           	    edata_state_get(edata) == extent_state_merging);
    .           
   93 ( 0.00%)  	eset_remove(eset, edata);
4,340 ( 0.15%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_remove (31x)
  155 ( 0.01%)  	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
2,325 ( 0.08%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_update_edata_state (31x)
    .           }
    .           
    .           void
    .           extent_gdump_add(tsdn_t *tsdn, const edata_t *edata) {
    .           	cassert(config_prof);
    .           	/* prof_gdump() requirement. */
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, 0);
-- line 289 ----------------------------------------
-- line 320 ----------------------------------------
    .           
    .           static bool
    .           extent_register_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata, bool gdump_add) {
    .           	assert(edata_state_get(edata) == extent_state_active);
    .           	/*
    .           	 * No locking needed, as the edata must be in active state, which
    .           	 * prevents other threads from accessing the edata.
    .           	 */
    7 ( 0.00%)  	if (emap_register_boundary(tsdn, pac->emap, edata, SC_NSIZES,
2,594 ( 0.09%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_register_boundary (1x)
    .           	    /* slab */ false)) {
    .           		return true;
    .           	}
    .           
    .           	if (config_prof && gdump_add) {
    .           		extent_gdump_add(tsdn, edata);
    .           	}
    .           
-- line 336 ----------------------------------------
-- line 395 ----------------------------------------
    .           		 * interior lookups (which of course cannot be recycled).
    .           		 */
    .           		void *new_addr = edata_past_get(expand_edata);
    .           		assert(PAGE_ADDR2BASE(new_addr) == new_addr);
    .           		assert(alignment <= PAGE);
    .           	}
    .           
    .           	edata_t *edata;
  320 ( 0.01%)  	eset_t *eset = guarded ? &ecache->guarded_eset : &ecache->eset;
  128 ( 0.00%)  	if (expand_edata != NULL) {
    .           		edata = emap_try_acquire_edata_neighbor_expand(tsdn, pac->emap,
    .           		    expand_edata, EXTENT_PAI_PAC, ecache->state);
    .           		if (edata != NULL) {
    .           			extent_assert_can_expand(expand_edata, edata);
    .           			if (edata_size_get(edata) < size) {
    .           				emap_release_edata(tsdn, pac->emap, edata,
    .           				    ecache->state);
    .           				edata = NULL;
-- line 412 ----------------------------------------
-- line 417 ----------------------------------------
    .           		 * A large extent might be broken up from its original size to
    .           		 * some small size to satisfy a small request.  When that small
    .           		 * request is freed, though, it won't merge back with the larger
    .           		 * extent if delayed coalescing is on.  The large extent can
    .           		 * then no longer satify a request for its original size.  To
    .           		 * limit this effect, when delayed coalescing is enabled, we
    .           		 * put a cap on how big an extent we can split for a request.
    .           		 */
  224 ( 0.01%)  		unsigned lg_max_fit = ecache->delay_coalesce
    .           		    ? (unsigned)opt_lg_extent_max_active_fit : SC_PTR_BITS;
    .           
    .           		/*
    .           		 * If split and merge are not allowed (Windows w/o retain), try
    .           		 * exact fit only.
    .           		 *
    .           		 * For simplicity purposes, splitting guarded extents is not
    .           		 * supported.  Hence, we do only exact fit for guarded
    .           		 * allocations.
    .           		 */
    .           		bool exact_only = (!maps_coalesce && !opt_retain) || guarded;
  384 ( 0.01%)  		edata = eset_fit(eset, size, alignment, exact_only,
10,119 ( 0.36%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c:_rjem_je_eset_fit (64x)
    .           		    lg_max_fit);
    .           	}
  161 ( 0.01%)  	if (edata == NULL) {
    .           		return NULL;
    .           	}
    .           	assert(!guarded || edata_guarded_get(edata));
    .           	extent_activate_locked(tsdn, pac, ecache, eset, edata);
    .           
    .           	return edata;
    .           }
    .           
-- line 448 ----------------------------------------
-- line 468 ----------------------------------------
    .           	 * In a potentially invalid state.  Must leak (if *to_leak is non-NULL),
    .           	 * and salvage what's still salvageable (if *to_salvage is non-NULL).
    .           	 * None of lead, edata, or trail are valid.
    .           	 */
    .           	extent_split_interior_error
    .           } extent_split_interior_result_t;
    .           
    .           static extent_split_interior_result_t
  416 ( 0.01%)  extent_split_interior(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               /* The result of splitting, in case of success. */
    .               edata_t **edata, edata_t **lead, edata_t **trail,
    .               /* The mess to clean up, in case of error. */
    .               edata_t **to_leak, edata_t **to_salvage,
    .               edata_t *expand_edata, size_t size, size_t alignment) {
  288 ( 0.01%)  	size_t leadsize = ALIGNMENT_CEILING((uintptr_t)edata_base_get(*edata),
    .           	    PAGE_CEILING(alignment)) - (uintptr_t)edata_base_get(*edata);
    .           	assert(expand_edata == NULL || leadsize == 0);
   96 ( 0.00%)  	if (edata_size_get(*edata) < leadsize + size) {
   32 ( 0.00%)  		return extent_split_interior_cant_alloc;
    .           	}
  128 ( 0.00%)  	size_t trailsize = edata_size_get(*edata) - leadsize - size;
    .           
   32 ( 0.00%)  	*lead = NULL;
   32 ( 0.00%)  	*trail = NULL;
   64 ( 0.00%)  	*to_leak = NULL;
   64 ( 0.00%)  	*to_salvage = NULL;
    .           
    .           	/* Split the lead. */
   64 ( 0.00%)  	if (leadsize != 0) {
    .           		assert(!edata_guarded_get(*edata));
    .           		*lead = *edata;
    .           		*edata = extent_split_impl(tsdn, pac, ehooks, *lead, leadsize,
    .           		    size + trailsize, /* holding_core_locks*/ true);
    .           		if (*edata == NULL) {
    .           			*to_leak = *lead;
    .           			*lead = NULL;
    .           			return extent_split_interior_error;
    .           		}
    .           	}
    .           
    .           	/* Split the trail. */
   64 ( 0.00%)  	if (trailsize != 0) {
    .           		assert(!edata_guarded_get(*edata));
  224 ( 0.01%)  		*trail = extent_split_impl(tsdn, pac, ehooks, *edata, size,
38,266 ( 1.36%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_split_impl.isra.0 (32x)
    .           		    trailsize, /* holding_core_locks */ true);
   64 ( 0.00%)  		if (*trail == NULL) {
    .           			*to_leak = *edata;
    .           			*to_salvage = *lead;
    .           			*lead = NULL;
    .           			*edata = NULL;
    .           			return extent_split_interior_error;
    .           		}
    .           	}
    .           
   32 ( 0.00%)  	return extent_split_interior_ok;
  288 ( 0.01%)  }
    .           
    .           /*
    .            * This fulfills the indicated allocation request out of the given extent (which
    .            * the caller should have ensured was big enough).  If there's any unused space
    .            * before or after the resulting allocation, that space is given its own extent
    .            * and put back into ecache.
    .            */
    .           static edata_t *
-- line 531 ----------------------------------------
-- line 532 ----------------------------------------
    .           extent_recycle_split(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               ecache_t *ecache, edata_t *expand_edata, size_t size, size_t alignment,
    .               edata_t *edata, bool growing_retained) {
    .           	assert(!edata_guarded_get(edata) || size == edata_size_get(edata));
    .           	malloc_mutex_assert_owner(tsdn, &ecache->mtx);
    .           
    .           	edata_t *lead;
    .           	edata_t *trail;
   31 ( 0.00%)  	edata_t *to_leak JEMALLOC_CC_SILENCE_INIT(NULL);
   31 ( 0.00%)  	edata_t *to_salvage JEMALLOC_CC_SILENCE_INIT(NULL);
    .           
  403 ( 0.01%)  	extent_split_interior_result_t result = extent_split_interior(
38,998 ( 1.38%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_split_interior.constprop.0 (31x)
    .           	    tsdn, pac, ehooks, &edata, &lead, &trail, &to_leak, &to_salvage,
    .           	    expand_edata, size, alignment);
    .           
    .           	if (!maps_coalesce && result != extent_split_interior_ok
    .           	    && !opt_retain) {
    .           		/*
    .           		 * Split isn't supported (implies Windows w/o retain).  Avoid
    .           		 * leaking the extent.
    .           		 */
    .           		assert(to_leak != NULL && lead == NULL && trail == NULL);
    .           		extent_deactivate_locked(tsdn, pac, ecache, to_leak);
    .           		return NULL;
    .           	}
    .           
   93 ( 0.00%)  	if (result == extent_split_interior_ok) {
   93 ( 0.00%)  		if (lead != NULL) {
    .           			extent_deactivate_locked(tsdn, pac, ecache, lead);
    .           		}
   93 ( 0.00%)  		if (trail != NULL) {
    .           			extent_deactivate_locked(tsdn, pac, ecache, trail);
    .           		}
   31 ( 0.00%)  		return edata;
    .           	} else {
    .           		/*
    .           		 * We should have picked an extent that was large enough to
    .           		 * fulfill our allocation request.
    .           		 */
    .           		assert(result == extent_split_interior_error);
    .           		if (to_salvage != NULL) {
    .           			extent_deregister(tsdn, pac, to_salvage);
-- line 573 ----------------------------------------
-- line 590 ----------------------------------------
    .           
    .           /*
    .            * Tries to satisfy the given allocation request by reusing one of the extents
    .            * in the given ecache_t.
    .            */
    .           static edata_t *
    .           extent_recycle(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
    .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
1,472 ( 0.05%)      bool *commit, bool growing_retained, bool guarded) {
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
    .           	assert(!guarded || expand_edata == NULL);
    .           	assert(!guarded || alignment <= PAGE);
    .           
    .           	malloc_mutex_lock(tsdn, &ecache->mtx);
    .           
    .           	edata_t *edata = extent_recycle_extract(tsdn, pac, ehooks, ecache,
    .           	    expand_edata, size, alignment, guarded);
    .           	if (edata == NULL) {
    .           		malloc_mutex_unlock(tsdn, &ecache->mtx);
   66 ( 0.00%)  		return NULL;
    .           	}
    .           
   62 ( 0.00%)  	edata = extent_recycle_split(tsdn, pac, ehooks, ecache, expand_edata,
    .           	    size, alignment, edata, growing_retained);
    .           	malloc_mutex_unlock(tsdn, &ecache->mtx);
   62 ( 0.00%)  	if (edata == NULL) {
    .           		return NULL;
    .           	}
    .           
    .           	assert(edata_state_get(edata) == extent_state_active);
  310 ( 0.01%)  	if (extent_commit_zero(tsdn, ehooks, edata, *commit, zero,
  434 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_extent_commit_zero (31x)
    .           	    growing_retained)) {
    .           		extent_record(tsdn, pac, ehooks, ecache, edata);
    .           		return NULL;
    .           	}
   62 ( 0.00%)  	if (edata_committed_get(edata)) {
    .           		/*
    .           		 * This reverses the purpose of this variable - previously it
    .           		 * was treated as an input parameter, now it turns into an
    .           		 * output parameter, reporting if the edata has actually been
    .           		 * committed.
    .           		 */
   62 ( 0.00%)  		*commit = true;
    .           	}
    .           	return edata;
  768 ( 0.03%)  }
    .           
    .           /*
    .            * If virtual memory is retained, create increasingly larger extents from which
    .            * to split requested extents in order to limit the total number of disjoint
    .            * virtual memory ranges retained by each shard.
    .            */
    .           static edata_t *
    .           extent_grow_retained(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               size_t size, size_t alignment, bool zero, bool *commit) {
    .           	malloc_mutex_assert_owner(tsdn, &pac->grow_mtx);
    .           
    5 ( 0.00%)  	size_t alloc_size_min = size + PAGE_CEILING(alignment) - PAGE;
    .           	/* Beware size_t wrap-around. */
    2 ( 0.00%)  	if (alloc_size_min < size) {
    .           		goto label_err;
    .           	}
    .           	/*
    .           	 * Find the next extent size in the series that would be large enough to
    .           	 * satisfy this request.
    .           	 */
    .           	size_t alloc_size;
    .           	pszind_t exp_grow_skip;
    .           	bool err = exp_grow_size_prepare(&pac->exp_grow, alloc_size_min,
    .           	    &alloc_size, &exp_grow_skip);
    .           	if (err) {
    .           		goto label_err;
    .           	}
    .           
    5 ( 0.00%)  	edata_t *edata = edata_cache_get(tsdn, pac->edata_cache);
  926 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata_cache.c:_rjem_je_edata_cache_get (1x)
    2 ( 0.00%)  	if (edata == NULL) {
    .           		goto label_err;
    .           	}
    1 ( 0.00%)  	bool zeroed = false;
    1 ( 0.00%)  	bool committed = false;
    .           
    .           	void *ptr = ehooks_alloc(tsdn, ehooks, NULL, alloc_size, PAGE, &zeroed,
    .           	    &committed);
    .           
    2 ( 0.00%)  	if (ptr == NULL) {
    .           		edata_cache_put(tsdn, pac->edata_cache, edata);
    .           		goto label_err;
    .           	}
    .           
    3 ( 0.00%)  	edata_init(edata, ecache_ind_get(&pac->ecache_retained), ptr,
    .           	    alloc_size, false, SC_NSIZES, extent_sn_next(pac),
    .           	    extent_state_active, zeroed, committed, EXTENT_PAI_PAC,
    .           	    EXTENT_IS_HEAD);
    .           
    3 ( 0.00%)  	if (extent_register_no_gdump_add(tsdn, pac, edata)) {
    .           		edata_cache_put(tsdn, pac->edata_cache, edata);
    .           		goto label_err;
    .           	}
    .           
    2 ( 0.00%)  	if (edata_committed_get(edata)) {
    1 ( 0.00%)  		*commit = true;
    .           	}
    .           
    .           	edata_t *lead;
    .           	edata_t *trail;
    1 ( 0.00%)  	edata_t *to_leak JEMALLOC_CC_SILENCE_INIT(NULL);
    1 ( 0.00%)  	edata_t *to_salvage JEMALLOC_CC_SILENCE_INIT(NULL);
    .           
   14 ( 0.00%)  	extent_split_interior_result_t result = extent_split_interior(tsdn,
1,284 ( 0.05%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_split_interior.constprop.0 (1x)
    .           	    pac, ehooks, &edata, &lead, &trail, &to_leak, &to_salvage, NULL,
    .           	    size, alignment);
    .           
    3 ( 0.00%)  	if (result == extent_split_interior_ok) {
    3 ( 0.00%)  		if (lead != NULL) {
    .           			extent_record(tsdn, pac, ehooks, &pac->ecache_retained,
    .           			    lead);
    .           		}
    3 ( 0.00%)  		if (trail != NULL) {
    5 ( 0.00%)  			extent_record(tsdn, pac, ehooks, &pac->ecache_retained,
  535 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:_rjem_je_extent_record (1x)
    .           			    trail);
    .           		}
    .           	} else {
    .           		/*
    .           		 * We should have allocated a sufficiently large extent; the
    .           		 * cant_alloc case should not occur.
    .           		 */
    .           		assert(result == extent_split_interior_error);
-- line 717 ----------------------------------------
-- line 725 ----------------------------------------
    .           		if (to_leak != NULL) {
    .           			extent_deregister_no_gdump_sub(tsdn, pac, to_leak);
    .           			extents_abandon_vm(tsdn, pac, ehooks,
    .           			    &pac->ecache_retained, to_leak, true);
    .           		}
    .           		goto label_err;
    .           	}
    .           
    5 ( 0.00%)  	if (*commit && !edata_committed_get(edata)) {
    .           		if (extent_commit_impl(tsdn, ehooks, edata, 0,
    .           		    edata_size_get(edata), true)) {
    .           			extent_record(tsdn, pac, ehooks,
    .           			    &pac->ecache_retained, edata);
    .           			goto label_err;
    .           		}
    .           		/* A successful commit should return zeroed memory. */
    .           		if (config_debug) {
-- line 741 ----------------------------------------
-- line 755 ----------------------------------------
    .           	/* All opportunities for failure are past. */
    .           	exp_grow_size_commit(&pac->exp_grow, exp_grow_skip);
    .           	malloc_mutex_unlock(tsdn, &pac->grow_mtx);
    .           
    .           	if (config_prof) {
    .           		/* Adjust gdump stats now that extent is final size. */
    .           		extent_gdump_add(tsdn, edata);
    .           	}
    5 ( 0.00%)  	if (zero && !edata_zeroed_get(edata)) {
    .           		ehooks_zero(tsdn, ehooks, edata_base_get(edata),
    .           		    edata_size_get(edata));
    .           	}
    .           	return edata;
    .           label_err:
    .           	malloc_mutex_unlock(tsdn, &pac->grow_mtx);
    .           	return NULL;
    .           }
-- line 771 ----------------------------------------
-- line 774 ----------------------------------------
    .           extent_alloc_retained(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               edata_t *expand_edata, size_t size, size_t alignment, bool zero,
    .               bool *commit, bool guarded) {
    .           	assert(size != 0);
    .           	assert(alignment != 0);
    .           
    .           	malloc_mutex_lock(tsdn, &pac->grow_mtx);
    .           
  608 ( 0.02%)  	edata_t *edata = extent_recycle(tsdn, pac, ehooks,
64,998 ( 2.30%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_recycle (32x)
    .           	    &pac->ecache_retained, expand_edata, size, alignment, zero, commit,
    .           	    /* growing_retained */ true, guarded);
   96 ( 0.00%)  	if (edata != NULL) {
    .           		malloc_mutex_unlock(tsdn, &pac->grow_mtx);
    .           		if (config_prof) {
    .           			extent_gdump_add(tsdn, edata);
    .           		}
    7 ( 0.00%)  	} else if (opt_retain && expand_edata == NULL && !guarded) {
    .           		edata = extent_grow_retained(tsdn, pac, ehooks, size,
    .           		    alignment, zero, commit);
    .           		/* extent_grow_retained() always releases pac->grow_mtx. */
    .           	} else {
    .           		malloc_mutex_unlock(tsdn, &pac->grow_mtx);
    .           	}
    .           	malloc_mutex_assert_not_owner(tsdn, &pac->grow_mtx);
    .           
-- line 798 ----------------------------------------
-- line 813 ----------------------------------------
    .           		    extent_state_merging);
    .           	}
    .           
    .           	return err;
    .           }
    .           
    .           static edata_t *
    .           extent_try_coalesce_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
   15 ( 0.00%)      ecache_t *ecache, edata_t *edata, bool *coalesced) {
    .           	assert(!edata_guarded_get(edata));
    .           	/*
    .           	 * We avoid checking / locking inactive neighbors for large size
    .           	 * classes, since they are eagerly coalesced on deallocation which can
    .           	 * cause lock contention.
    .           	 */
    .           	/*
    .           	 * Continue attempting to coalesce until failure, to protect against
    .           	 * races with other threads that are thwarted by this one.
    .           	 */
    .           	bool again;
    .           	do {
    .           		again = false;
    .           
    .           		/* Try to coalesce forward. */
    8 ( 0.00%)  		edata_t *next = emap_try_acquire_edata_neighbor(tsdn, pac->emap,
   66 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_try_acquire_edata_neighbor (1x)
    .           		    edata, EXTENT_PAI_PAC, ecache->state, /* forward */ true);
    2 ( 0.00%)  		if (next != NULL) {
    .           			if (!extent_coalesce(tsdn, pac, ehooks, ecache, edata,
    .           			    next, true)) {
    .           				if (ecache->delay_coalesce) {
    .           					/* Do minimal coalescing. */
    .           					*coalesced = true;
    1 ( 0.00%)  					return edata;
    .           				}
    .           				again = true;
    .           			}
    .           		}
    .           
    .           		/* Try to coalesce backward. */
    8 ( 0.00%)  		edata_t *prev = emap_try_acquire_edata_neighbor(tsdn, pac->emap,
   79 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_try_acquire_edata_neighbor (1x)
    .           		    edata, EXTENT_PAI_PAC, ecache->state, /* forward */ false);
    2 ( 0.00%)  		if (prev != NULL) {
    .           			if (!extent_coalesce(tsdn, pac, ehooks, ecache, edata,
    .           			    prev, false)) {
    .           				edata = prev;
    .           				if (ecache->delay_coalesce) {
    .           					/* Do minimal coalescing. */
    .           					*coalesced = true;
    .           					return edata;
    .           				}
    .           				again = true;
    .           			}
    .           		}
    .           	} while (again);
    .           
    2 ( 0.00%)  	if (ecache->delay_coalesce) {
    .           		*coalesced = false;
    .           	}
    .           	return edata;
    9 ( 0.00%)  }
    .           
    .           static edata_t *
    .           extent_try_coalesce(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               ecache_t *ecache, edata_t *edata, bool *coalesced) {
    9 ( 0.00%)  	return extent_try_coalesce_impl(tsdn, pac, ehooks, ecache, edata,
  192 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent.c:extent_try_coalesce_impl (1x)
    .           	    coalesced);
    .           }
    .           
    .           static edata_t *
    .           extent_try_coalesce_large(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               ecache_t *ecache, edata_t *edata, bool *coalesced) {
    .           	return extent_try_coalesce_impl(tsdn, pac, ehooks, ecache, edata,
    .           	    coalesced);
-- line 885 ----------------------------------------
-- line 908 ----------------------------------------
    .           }
    .           
    .           /*
    .            * Does the metadata management portions of putting an unused extent into the
    .            * given ecache_t (coalesces and inserts into the eset).
    .            */
    .           void
    .           extent_record(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, ecache_t *ecache,
   16 ( 0.00%)      edata_t *edata) {
    .           	assert((ecache->state != extent_state_dirty &&
    .           	    ecache->state != extent_state_muzzy) ||
    .           	    !edata_zeroed_get(edata));
    .           
    .           	malloc_mutex_lock(tsdn, &ecache->mtx);
    .           
    .           	emap_assert_mapped(tsdn, pac->emap, edata);
    .           
    2 ( 0.00%)  	if (edata_guarded_get(edata)) {
    .           		goto label_skip_coalesce;
    .           	}
    2 ( 0.00%)  	if (!ecache->delay_coalesce) {
    .           		edata = extent_try_coalesce(tsdn, pac, ehooks, ecache, edata,
    .           		    NULL);
    .           	} else if (edata_size_get(edata) >= SC_LARGE_MINCLASS) {
    .           		assert(ecache == &pac->ecache_dirty);
    .           		/* Always coalesce large extents eagerly. */
    .           		bool coalesced;
    .           		do {
    .           			assert(edata_state_get(edata) == extent_state_active);
-- line 936 ----------------------------------------
-- line 945 ----------------------------------------
    .           			extent_maximally_purge(tsdn, pac, ehooks, edata);
    .           			return;
    .           		}
    .           	}
    .           label_skip_coalesce:
    .           	extent_deactivate_locked(tsdn, pac, ecache, edata);
    .           
    .           	malloc_mutex_unlock(tsdn, &ecache->mtx);
    7 ( 0.00%)  }
    .           
    .           void
    .           extent_dalloc_gap(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               edata_t *edata) {
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, 0);
    .           
    .           	if (extent_register(tsdn, pac, edata)) {
-- line 961 ----------------------------------------
-- line 1166 ----------------------------------------
    .           /*
    .            * Accepts the extent to split, and the characteristics of each side of the
    .            * split.  The 'a' parameters go with the 'lead' of the resulting pair of
    .            * extents (the lower addressed portion of the split), and the 'b' parameters go
    .            * with the trail (the higher addressed portion).  This makes 'extent' the lead,
    .            * and returns the trail (except in case of error).
    .            */
    .           static edata_t *
  256 ( 0.01%)  extent_split_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               edata_t *edata, size_t size_a, size_t size_b, bool holding_core_locks) {
    .           	assert(edata_size_get(edata) == size_a + size_b);
    .           	/* Only the shrink path may split w/o holding core locks. */
    .           	if (holding_core_locks) {
    .           		witness_assert_positive_depth_to_rank(
    .           		    tsdn_witness_tsdp_get(tsdn), WITNESS_RANK_CORE);
    .           	} else {
    .           		witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           		    WITNESS_RANK_CORE, 0);
    .           	}
    .           
   96 ( 0.00%)  	if (ehooks_split_will_fail(ehooks)) {
    .           		return NULL;
    .           	}
    .           
  224 ( 0.01%)  	edata_t *trail = edata_cache_get(tsdn, pac->edata_cache);
30,042 ( 1.06%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata_cache.c:_rjem_je_edata_cache_get (32x)
   64 ( 0.00%)  	if (trail == NULL) {
    .           		goto label_error_a;
    .           	}
    .           
    .           	edata_init(trail, edata_arena_ind_get(edata),
   96 ( 0.00%)  	    (void *)((uintptr_t)edata_base_get(edata) + size_a), size_b,
    .           	    /* slab */ false, SC_NSIZES, edata_sn_get(edata),
    .           	    edata_state_get(edata), edata_zeroed_get(edata),
    .           	    edata_committed_get(edata), EXTENT_PAI_PAC, EXTENT_NOT_HEAD);
    .           	emap_prepare_t prepare;
  288 ( 0.01%)  	bool err = emap_split_prepare(tsdn, pac->emap, &prepare, edata,
3,520 ( 0.12%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_split_prepare (32x)
    .           	    size_a, trail, size_b);
  128 ( 0.00%)  	if (err) {
    .           		goto label_error_b;
    .           	}
    .           
    .           	/*
    .           	 * No need to acquire trail or edata, because: 1) trail was new (just
    .           	 * allocated); and 2) edata is either an active allocation (the shrink
    .           	 * path), or in an acquired state (extracted from the ecache on the
    .           	 * extent_recycle_split path).
    .           	 */
    .           	assert(emap_edata_is_acquired(tsdn, pac->emap, edata));
    .           	assert(emap_edata_is_acquired(tsdn, pac->emap, trail));
    .           
    .           	err = ehooks_split(tsdn, ehooks, edata_base_get(edata), size_a + size_b,
    .           	    size_a, size_b, edata_committed_get(edata));
    .           
   64 ( 0.00%)  	if (err) {
    .           		goto label_error_b;
    .           	}
    .           
    .           	edata_size_set(edata, size_a);
  256 ( 0.01%)  	emap_split_commit(tsdn, pac->emap, &prepare, edata, size_a, trail,
1,440 ( 0.05%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_split_commit (32x)
    .           	    size_b);
    .           
   96 ( 0.00%)  	return trail;
    .           label_error_b:
    .           	edata_cache_put(tsdn, pac->edata_cache, trail);
    .           label_error_a:
    .           	return NULL;
  384 ( 0.01%)  }
    .           
    .           edata_t *
    .           extent_split_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, edata_t *edata,
    .               size_t size_a, size_t size_b, bool holding_core_locks) {
    .           	return extent_split_impl(tsdn, pac, ehooks, edata, size_a, size_b,
    .           	    holding_core_locks);
    .           }
    .           
-- line 1240 ----------------------------------------
-- line 1291 ----------------------------------------
    .           extent_merge_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    .               edata_t *a, edata_t *b) {
    .           	return extent_merge_impl(tsdn, pac, ehooks, a, b,
    .           	    /* holding_core_locks */ false);
    .           }
    .           
    .           bool
    .           extent_commit_zero(tsdn_t *tsdn, ehooks_t *ehooks, edata_t *edata,
  124 ( 0.00%)      bool commit, bool zero, bool growing_retained) {
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
    .           
  124 ( 0.00%)  	if (commit && !edata_committed_get(edata)) {
    .           		if (extent_commit_impl(tsdn, ehooks, edata, 0,
    .           		    edata_size_get(edata), growing_retained)) {
    .           			return true;
    .           		}
    .           	}
   62 ( 0.00%)  	if (zero && !edata_zeroed_get(edata)) {
    .           		void *addr = edata_base_get(edata);
    .           		size_t size = edata_size_get(edata);
    .           		ehooks_zero(tsdn, ehooks, addr, size);
    .           	}
   31 ( 0.00%)  	return false;
   93 ( 0.00%)  }
    .           
    .           bool
    2 ( 0.00%)  extent_boot(void) {
    .           	assert(sizeof(slab_data_t) >= sizeof(e_prof_info_t));
    .           
    .           	if (have_dss) {
    1 ( 0.00%)  		extent_dss_boot();
   47 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent_dss.c:_rjem_je_extent_dss_boot (1x)
    .           	}
    .           
    .           	return false;
    3 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/tcache_inlines.h
--------------------------------------------------------------------------------
Ir           

-- line 43 ----------------------------------------
  .           
  .           JEMALLOC_ALWAYS_INLINE void *
  .           tcache_alloc_small(tsd_t *tsd, arena_t *arena, tcache_t *tcache,
  .               size_t size, szind_t binind, bool zero, bool slow_path) {
  .           	void *ret;
  .           	bool tcache_success;
  .           
  .           	assert(binind < SC_NBINS);
 30 ( 0.00%)  	cache_bin_t *bin = &tcache->bins[binind];
  .           	ret = cache_bin_alloc(bin, &tcache_success);
  .           	assert(tcache_success == (ret != NULL));
  .           	if (unlikely(!tcache_success)) {
  .           		bool tcache_hard_success;
  .           		arena = arena_choose(tsd, arena);
  .           		if (unlikely(arena == NULL)) {
  .           			return NULL;
  .           		}
 44 ( 0.00%)  		if (unlikely(tcache_small_bin_disabled(binind, bin))) {
  .           			/* stats and zero are handled directly by the arena. */
  1 ( 0.00%)  			return arena_malloc_hard(tsd_tsdn(tsd), arena, size,
  .           			    binind, zero);
  .           		}
168 ( 0.01%)  		tcache_bin_flush_stashed(tsd, tcache, bin, binind,
312 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_bin_flush_stashed (8x)
  .           		    /* is_small */ true);
  .           
176 ( 0.01%)  		ret = tcache_alloc_small_hard(tsd_tsdn(tsd), arena, tcache,
34,030 ( 1.21%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_alloc_small_hard (8x)
  .           		    bin, binind, &tcache_hard_success);
 52 ( 0.00%)  		if (tcache_hard_success == false) {
  .           			return NULL;
  .           		}
  .           	}
  .           
  .           	assert(ret);
 58 ( 0.00%)  	if (unlikely(zero)) {
  .           		size_t usize = sz_index2size(binind);
  .           		assert(tcache_salloc(tsd_tsdn(tsd), ret) == usize);
  .           		memset(ret, 0, usize);
  .           	}
  .           	if (config_stats) {
 14 ( 0.00%)  		bin->tstats.nrequests++;
  .           	}
  .           	return ret;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void *
  .           tcache_alloc_large(tsd_t *tsd, arena_t *arena, tcache_t *tcache, size_t size,
  .               szind_t binind, bool zero, bool slow_path) {
  .           	void *ret;
-- line 90 ----------------------------------------
-- line 113 ----------------------------------------
  .           	} else {
  .           		if (unlikely(zero)) {
  .           			size_t usize = sz_index2size(binind);
  .           			assert(usize <= tcache_maxclass);
  .           			memset(ret, 0, usize);
  .           		}
  .           
  .           		if (config_stats) {
 29 ( 0.00%)  			bin->tstats.nrequests++;
  .           		}
  .           	}
  .           
  .           	return ret;
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           tcache_dalloc_small(tsd_t *tsd, tcache_t *tcache, void *ptr, szind_t binind,
-- line 129 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/util.h
--------------------------------------------------------------------------------
Ir           

-- line 43 ----------------------------------------
  .           #define unreachable() JEMALLOC_INTERNAL_UNREACHABLE()
  .           
  .           /* Set error code. */
  .           UTIL_INLINE void
  .           set_errno(int errnum) {
  .           #ifdef _WIN32
  .           	SetLastError(errnum);
  .           #else
  1 ( 0.00%)  	errno = errnum;
  .           #endif
  .           }
  .           
  .           /* Get last error code. */
  .           UTIL_INLINE int
  .           get_errno(void) {
  .           #ifdef _WIN32
  .           	return GetLastError();
  .           #else
  .           	return errno;
  .           #endif
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           util_assume(bool b) {
646 ( 0.02%)  	if (!b) {
  .           		unreachable();
  .           	}
  .           }
  .           
  .           /* ptr should be valid. */
  .           JEMALLOC_ALWAYS_INLINE void
  .           util_prefetch_read(void *ptr) {
  .           	/*
-- line 75 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c
--------------------------------------------------------------------------------
Ir              

     .           #include "jemalloc/internal/jemalloc_preamble.h"
     .           #include "jemalloc/internal/jemalloc_internal_includes.h"
     .           
   101 ( 0.00%)  ph_gen(, edata_avail, edata_t, avail_link,
     .               edata_esnead_comp)
14,749 ( 0.52%)  ph_gen(, edata_heap, edata_t, heap_link, edata_snad_comp)

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/rtree.h
--------------------------------------------------------------------------------
Ir             

-- line 140 ----------------------------------------
    .           	unsigned cumbits = (rtree_levels[RTREE_HEIGHT-1].cumbits -
    .           	    rtree_levels[RTREE_HEIGHT-1].bits);
    .           	return ptrbits - cumbits;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE uintptr_t
    .           rtree_leafkey(uintptr_t key) {
    .           	uintptr_t mask = ~((ZU(1) << rtree_leaf_maskbits()) - 1);
  851 ( 0.03%)  	return (key & mask);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           rtree_cache_direct_map(uintptr_t key) {
  791 ( 0.03%)  	return (size_t)((key >> rtree_leaf_maskbits()) &
    .           	    (RTREE_CTX_NCACHE - 1));
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE uintptr_t
    .           rtree_subkey(uintptr_t key, unsigned level) {
    .           	unsigned ptrbits = ZU(1) << (LG_SIZEOF_PTR+3);
    .           	unsigned cumbits = rtree_levels[level].cumbits;
    .           	unsigned shiftbits = ptrbits - cumbits;
    .           	unsigned maskbits = rtree_levels[level].bits;
    .           	uintptr_t mask = (ZU(1) << maskbits) - 1;
  511 ( 0.02%)  	return ((key >> shiftbits) & mask);
    .           }
    .           
    .           /*
    .            * Atomic getters.
    .            *
    .            * dependent: Reading a value on behalf of a pointer to a valid allocation
    .            *            is guaranteed to be a clean read even without synchronization,
    .            *            because the rtree update became visible in memory before the
-- line 172 ----------------------------------------
-- line 181 ----------------------------------------
    .               rtree_leaf_elm_t *elm, bool dependent) {
    .           	return (uintptr_t)atomic_load_p(&elm->le_bits, dependent
    .           	    ? ATOMIC_RELAXED : ATOMIC_ACQUIRE);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE uintptr_t
    .           rtree_leaf_elm_bits_encode(rtree_contents_t contents) {
    .           	assert((uintptr_t)contents.edata % (uintptr_t)EDATA_ALIGNMENT == 0);
  286 ( 0.01%)  	uintptr_t edata_bits = (uintptr_t)contents.edata
    .           	    & (((uintptr_t)1 << LG_VADDR) - 1);
    .           
  111 ( 0.00%)  	uintptr_t szind_bits = (uintptr_t)contents.metadata.szind << LG_VADDR;
   33 ( 0.00%)  	uintptr_t slab_bits = (uintptr_t)contents.metadata.slab;
  239 ( 0.01%)  	uintptr_t is_head_bits = (uintptr_t)contents.metadata.is_head << 1;
  186 ( 0.01%)  	uintptr_t state_bits = (uintptr_t)contents.metadata.state <<
    .           	    RTREE_LEAF_STATE_SHIFT;
    .           	uintptr_t metadata_bits = szind_bits | state_bits | is_head_bits |
    .           	    slab_bits;
    .           	assert((edata_bits & metadata_bits) == 0);
    .           
  462 ( 0.02%)  	return edata_bits | metadata_bits;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE rtree_contents_t
    .           rtree_leaf_elm_bits_decode(uintptr_t bits) {
    .           	rtree_contents_t contents;
    .           	/* Do the easy things first. */
    .           	contents.metadata.szind = bits >> LG_VADDR;
    .           	contents.metadata.slab = (bool)(bits & 1);
    .           	contents.metadata.is_head = (bool)(bits & (1 << 1));
    .           
    3 ( 0.00%)  	uintptr_t state_bits = (bits & RTREE_LEAF_STATE_MASK) >>
    .           	    RTREE_LEAF_STATE_SHIFT;
    .           	assert(state_bits <= extent_state_max);
    .           	contents.metadata.state = (extent_state_t)state_bits;
    .           
    .           	uintptr_t low_bit_mask = ~((uintptr_t)EDATA_ALIGNMENT - 1);
    .           #    ifdef __aarch64__
    .           	/*
    .           	 * aarch64 doesn't sign extend the highest virtual address bit to set
-- line 220 ----------------------------------------
-- line 221 ----------------------------------------
    .           	 * the higher ones.  Instead, the high bits get zeroed.
    .           	 */
    .           	uintptr_t high_bit_mask = ((uintptr_t)1 << LG_VADDR) - 1;
    .           	/* Mask off metadata. */
    .           	uintptr_t mask = high_bit_mask & low_bit_mask;
    .           	contents.edata = (edata_t *)(bits & mask);
    .           #    else
    .           	/* Restore sign-extended high bits, mask metadata bits. */
   62 ( 0.00%)  	contents.edata = (edata_t *)((uintptr_t)((intptr_t)(bits << RTREE_NHIB)
   31 ( 0.00%)  	    >> RTREE_NHIB) & low_bit_mask);
    .           #    endif
    .           	assert((uintptr_t)contents.edata % (uintptr_t)EDATA_ALIGNMENT == 0);
    .           	return contents;
    .           }
    .           
    .           #  endif /* RTREE_LEAF_COMPACT */
    .           
    .           JEMALLOC_ALWAYS_INLINE rtree_contents_t
-- line 238 ----------------------------------------
-- line 307 ----------------------------------------
    .           /* The state field can be updated independently (and more frequently). */
    .           JEMALLOC_ALWAYS_INLINE void
    .           rtree_leaf_elm_state_update(tsdn_t *tsdn, rtree_t *rtree,
    .               rtree_leaf_elm_t *elm1, rtree_leaf_elm_t *elm2, extent_state_t state) {
    .           	assert(elm1 != NULL);
    .           #ifdef RTREE_LEAF_COMPACT
    .           	uintptr_t bits = rtree_leaf_elm_bits_read(tsdn, rtree, elm1,
    .           	    /* dependent */ true);
   63 ( 0.00%)  	bits &= ~RTREE_LEAF_STATE_MASK;
  126 ( 0.00%)  	bits |= state << RTREE_LEAF_STATE_SHIFT;
    .           	atomic_store_p(&elm1->le_bits, (void *)bits, ATOMIC_RELEASE);
  126 ( 0.00%)  	if (elm2 != NULL) {
    .           		atomic_store_p(&elm2->le_bits, (void *)bits, ATOMIC_RELEASE);
    .           	}
    .           #else
    .           	unsigned bits = atomic_load_u(&elm1->le_metadata, ATOMIC_RELAXED);
    .           	bits &= ~RTREE_LEAF_STATE_MASK;
    .           	bits |= state << RTREE_LEAF_STATE_SHIFT;
    .           	atomic_store_u(&elm1->le_metadata, bits, ATOMIC_RELEASE);
    .           	if (elm2 != NULL) {
-- line 326 ----------------------------------------
-- line 359 ----------------------------------------
    .           	assert(key != 0);
    .           	assert(!dependent || !init_missing);
    .           
    .           	size_t slot = rtree_cache_direct_map(key);
    .           	uintptr_t leafkey = rtree_leafkey(key);
    .           	assert(leafkey != RTREE_LEAFKEY_INVALID);
    .           
    .           	/* Fast path: L1 direct mapped cache. */
1,819 ( 0.06%)  	if (likely(rtree_ctx->cache[slot].leafkey == leafkey)) {
    .           		rtree_leaf_elm_t *leaf = rtree_ctx->cache[slot].leaf;
    .           		assert(leaf != NULL);
    .           		uintptr_t subkey = rtree_subkey(key, RTREE_HEIGHT-1);
  947 ( 0.03%)  		return &leaf[subkey];
    .           	}
    .           	/*
    .           	 * Search the L2 LRU cache.  On hit, swap the matching element into the
    .           	 * slot in L1 cache, and move the position in L2 up by 1.
    .           	 */
    .           #define RTREE_CACHE_CHECK_L2(i) do {					\
    .           	if (likely(rtree_ctx->l2_cache[i].leafkey == leafkey)) {	\
    .           		rtree_leaf_elm_t *leaf = rtree_ctx->l2_cache[i].leaf;	\
-- line 379 ----------------------------------------
-- line 396 ----------------------------------------
    .           		}							\
    .           		rtree_ctx->cache[slot].leafkey = leafkey;		\
    .           		rtree_ctx->cache[slot].leaf = leaf;			\
    .           		uintptr_t subkey = rtree_subkey(key, RTREE_HEIGHT-1);	\
    .           		return &leaf[subkey];					\
    .           	}								\
    .           } while (0)
    .           	/* Check the first cache entry. */
    2 ( 0.00%)  	RTREE_CACHE_CHECK_L2(0);
    .           	/* Search the remaining cache elements. */
    6 ( 0.00%)  	for (unsigned i = 1; i < RTREE_CTX_NCACHE_L2; i++) {
   14 ( 0.00%)  		RTREE_CACHE_CHECK_L2(i);
    .           	}
    .           #undef RTREE_CACHE_CHECK_L2
    .           
   10 ( 0.00%)  	return rtree_leaf_elm_lookup_hard(tsdn, rtree, rtree_ctx, key,
2,484 ( 0.09%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/rtree.c:_rjem_je_rtree_leaf_elm_lookup_hard (1x)
    .           	    dependent, init_missing);
    .           }
    .           
    .           /*
    .            * Returns true on lookup failure.
    .            */
    .           static inline bool
    .           rtree_read_independent(tsdn_t *tsdn, rtree_t *rtree, rtree_ctx_t *rtree_ctx,
-- line 419 ----------------------------------------
-- line 478 ----------------------------------------
    .           	 * boundaries have been registered already.  Therefore all the lookups
    .           	 * are dependent w/o init_missing, assuming the range spans across at
    .           	 * most 2 rtree leaf nodes (each covers 1 GiB of vaddr).
    .           	 */
    .           	void *bits;
    .           	unsigned additional;
    .           	rtree_contents_encode(contents, &bits, &additional);
    .           
   23 ( 0.00%)  	rtree_leaf_elm_t *elm = NULL; /* Dead store. */
  227 ( 0.01%)  	for (uintptr_t addr = base; addr <= end; addr += PAGE) {
  134 ( 0.00%)  		if (addr == base ||
    .           		    (addr & ((ZU(1) << rtree_leaf_maskbits()) - 1)) == 0) {
    .           			elm = rtree_leaf_elm_lookup(tsdn, rtree, rtree_ctx, addr,
    .           			    /* dependent */ true, /* init_missing */ false);
    .           			assert(elm != NULL);
    .           		}
    .           		assert(elm == rtree_leaf_elm_lookup(tsdn, rtree, rtree_ctx, addr,
    .           		    /* dependent */ true, /* init_missing */ false));
    .           		assert(!clearing || rtree_leaf_elm_read(tsdn, rtree, elm,
    .           		    /* dependent */ true).edata != NULL);
    .           		rtree_leaf_elm_write_commit(tsdn, rtree, elm, bits, additional);
   45 ( 0.00%)  		elm++;
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           rtree_write_range(tsdn_t *tsdn, rtree_t *rtree, rtree_ctx_t *rtree_ctx,
    .               uintptr_t base, uintptr_t end, rtree_contents_t contents) {
    .           	rtree_write_range_impl(tsdn, rtree, rtree_ctx, base, end, contents,
    .           	    /* clearing */ false);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           rtree_write(tsdn_t *tsdn, rtree_t *rtree, rtree_ctx_t *rtree_ctx, uintptr_t key,
    .               rtree_contents_t contents) {
    .           	rtree_leaf_elm_t *elm = rtree_leaf_elm_lookup(tsdn, rtree, rtree_ctx,
    .           	    key, /* dependent */ false, /* init_missing */ true);
  110 ( 0.00%)  	if (elm == NULL) {
    .           		return true;
    .           	}
    .           
    .           	rtree_leaf_elm_write(tsdn, rtree, elm, contents);
    .           
    .           	return false;
    .           }
    .           
-- line 523 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/sz.h
--------------------------------------------------------------------------------
Ir             

-- line 51 ----------------------------------------
    .            */
    .           extern size_t sz_large_pad;
    .           
    .           extern void sz_boot(const sc_data_t *sc_data, bool cache_oblivious);
    .           
    .           JEMALLOC_ALWAYS_INLINE pszind_t
    .           sz_psz2ind(size_t psz) {
    .           	assert(psz > 0);
  832 ( 0.03%)  	if (unlikely(psz > SC_LARGE_MAXCLASS)) {
    .           		return SC_NPSIZES;
    .           	}
    .           	/* x is the lg of the first base >= psz. */
    .           	pszind_t x = lg_ceil(psz);
    .           	/*
    .           	 * sc.h introduces a lot of size classes. These size classes are divided
    .           	 * into different size class groups. There is a very special size class
    .           	 * group, each size class in or after it is an integer multiple of PAGE.
    .           	 * We call it first_ps_rg. It means first page size regular group. The
    .           	 * range of first_ps_rg is (base, base * 2], and base == PAGE *
    .           	 * SC_NGROUP. off_to_first_ps_rg begins from 1, instead of 0. e.g.
    .           	 * off_to_first_ps_rg is 1 when psz is (PAGE * SC_NGROUP + 1).
    .           	 */
    .           	pszind_t off_to_first_ps_rg = (x < SC_LG_NGROUP + LG_PAGE) ?
  768 ( 0.03%)  	    0 : x - (SC_LG_NGROUP + LG_PAGE);
    .           
    .           	/*
    .           	 * Same as sc_s::lg_delta.
    .           	 * Delta for off_to_first_ps_rg == 1 is PAGE,
    .           	 * for each increase in offset, it's multiplied by two.
    .           	 * Therefore, lg_delta = LG_PAGE + (off_to_first_ps_rg - 1).
    .           	 */
    .           	pszind_t lg_delta = (off_to_first_ps_rg == 0) ?
  978 ( 0.03%)  	    LG_PAGE : LG_PAGE + (off_to_first_ps_rg - 1);
    .           
    .           	/*
    .           	 * Let's write psz in binary, e.g. 0011 for 0x3, 0111 for 0x7.
    .           	 * The leftmost bits whose len is lg_base decide the base of psz.
    .           	 * The rightmost bits whose len is lg_delta decide (pgz % PAGE).
    .           	 * The middle bits whose len is SC_LG_NGROUP decide ndelta.
    .           	 * ndelta is offset to the first size class in the size class group,
    .           	 * starts from 1.
-- line 91 ----------------------------------------
-- line 92 ----------------------------------------
    .           	 * If you don't know lg_base, ndelta or lg_delta, see sc.h.
    .           	 * |xxxxxxxxxxxxxxxxxxxx|------------------------|yyyyyyyyyyyyyyyyyyyyy|
    .           	 * |<-- len: lg_base -->|<-- len: SC_LG_NGROUP-->|<-- len: lg_delta -->|
    .           	 *                      |<--      ndelta      -->|
    .           	 * rg_inner_off = ndelta - 1
    .           	 * Why use (psz - 1)?
    .           	 * To handle case: psz % (1 << lg_delta) == 0.
    .           	 */
  466 ( 0.02%)  	pszind_t rg_inner_off = (((psz - 1)) >> lg_delta) & (SC_NGROUP - 1);
    .           
    .           	pszind_t base_ind = off_to_first_ps_rg << SC_LG_NGROUP;
  382 ( 0.01%)  	pszind_t ind = base_ind + rg_inner_off;
  189 ( 0.01%)  	return ind;
    .           }
    .           
    .           static inline size_t
    .           sz_pind2sz_compute(pszind_t pind) {
    .           	if (unlikely(pind == SC_NPSIZES)) {
    .           		return SC_LARGE_MAXCLASS + PAGE;
    .           	}
    .           	size_t grp = pind >> SC_LG_NGROUP;
-- line 112 ----------------------------------------
-- line 121 ----------------------------------------
    .           	size_t mod_size = (mod+1) << lg_delta;
    .           
    .           	size_t sz = grp_size + mod_size;
    .           	return sz;
    .           }
    .           
    .           static inline size_t
    .           sz_pind2sz_lookup(pszind_t pind) {
  129 ( 0.00%)  	size_t ret = (size_t)sz_pind2sz_tab[pind];
    .           	assert(ret == sz_pind2sz_compute(pind));
    .           	return ret;
    .           }
    .           
    .           static inline size_t
    .           sz_pind2sz(pszind_t pind) {
    .           	assert(pind < SC_NPSIZES + 1);
    .           	return sz_pind2sz_lookup(pind);
    .           }
    .           
    .           static inline size_t
    .           sz_psz2u(size_t psz) {
    6 ( 0.00%)  	if (unlikely(psz > SC_LARGE_MAXCLASS)) {
    .           		return SC_LARGE_MAXCLASS + PAGE;
    .           	}
    4 ( 0.00%)  	size_t x = lg_floor((psz<<1)-1);
    2 ( 0.00%)  	size_t lg_delta = (x < SC_LG_NGROUP + LG_PAGE + 1) ?
    6 ( 0.00%)  	    LG_PAGE : x - SC_LG_NGROUP - 1;
    6 ( 0.00%)  	size_t delta = ZU(1) << lg_delta;
    .           	size_t delta_mask = delta - 1;
    6 ( 0.00%)  	size_t usize = (psz + delta_mask) & ~delta_mask;
    .           	return usize;
    .           }
    .           
    .           static inline szind_t
    .           sz_size2index_compute(size_t size) {
  137 ( 0.00%)  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
    1 ( 0.00%)  		return SC_NSIZES;
    .           	}
    .           
    .           	if (size == 0) {
    .           		return 0;
    .           	}
    .           #if (SC_NTINY != 0)
    .           	if (size <= (ZU(1) << SC_LG_TINY_MAXCLASS)) {
    .           		szind_t lg_tmin = SC_LG_TINY_MAXCLASS - SC_NTINY + 1;
    .           		szind_t lg_ceil = lg_floor(pow2_ceil_zu(size));
    .           		return (lg_ceil < lg_tmin ? 0 : lg_ceil - lg_tmin);
    .           	}
    .           #endif
    .           	{
   45 ( 0.00%)  		szind_t x = lg_floor((size<<1)-1);
  249 ( 0.01%)  		szind_t shift = (x < SC_LG_NGROUP + LG_QUANTUM) ? 0 :
    .           		    x - (SC_LG_NGROUP + LG_QUANTUM);
    .           		szind_t grp = shift << SC_LG_NGROUP;
    .           
   45 ( 0.00%)  		szind_t lg_delta = (x < SC_LG_NGROUP + LG_QUANTUM + 1)
  135 ( 0.00%)  		    ? LG_QUANTUM : x - SC_LG_NGROUP - 1;
    .           
   90 ( 0.00%)  		size_t delta_inverse_mask = ZU(-1) << lg_delta;
  142 ( 0.01%)  		szind_t mod = ((((size-1) & delta_inverse_mask) >> lg_delta)) &
    .           		    ((ZU(1) << SC_LG_NGROUP) - 1);
    .           
    6 ( 0.00%)  		szind_t index = SC_NTINY + grp + mod;
   43 ( 0.00%)  		return index;
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE szind_t
    .           sz_size2index_lookup_impl(size_t size) {
    .           	assert(size <= SC_LOOKUP_MAXCLASS);
1,807 ( 0.06%)  	return sz_size2index_tab[(size + (ZU(1) << SC_LG_TINY_MIN) - 1)
1,479 ( 0.05%)  	    >> SC_LG_TINY_MIN];
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE szind_t
    .           sz_size2index_lookup(size_t size) {
    .           	szind_t ret = sz_size2index_lookup_impl(size);
    .           	assert(ret == sz_size2index_compute(size));
    .           	return ret;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE szind_t
    .           sz_size2index(size_t size) {
  417 ( 0.01%)  	if (likely(size <= SC_LOOKUP_MAXCLASS)) {
    .           		return sz_size2index_lookup(size);
    .           	}
    .           	return sz_size2index_compute(size);
    .           }
    .           
    .           static inline size_t
    .           sz_index2size_compute(szind_t index) {
    .           #if (SC_NTINY > 0)
-- line 212 ----------------------------------------
-- line 230 ----------------------------------------
    .           
    .           		size_t usize = grp_size + mod_size;
    .           		return usize;
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           sz_index2size_lookup_impl(szind_t index) {
3,030 ( 0.11%)  	return sz_index2size_tab[index];
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           sz_index2size_lookup(szind_t index) {
    .           	size_t ret = sz_index2size_lookup_impl(index);
    .           	assert(ret == sz_index2size_compute(index));
    .           	return ret;
    .           }
-- line 246 ----------------------------------------
-- line 254 ----------------------------------------
    .           JEMALLOC_ALWAYS_INLINE void
    .           sz_size2index_usize_fastpath(size_t size, szind_t *ind, size_t *usize) {
    .           	*ind = sz_size2index_lookup_impl(size);
    .           	*usize = sz_index2size_lookup_impl(*ind);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           sz_s2u_compute(size_t size) {
   10 ( 0.00%)  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
    .           		return 0;
    .           	}
    .           
    .           	if (size == 0) {
    .           		size++;
    .           	}
    .           #if (SC_NTINY > 0)
    .           	if (size <= (ZU(1) << SC_LG_TINY_MAXCLASS)) {
    .           		size_t lg_tmin = SC_LG_TINY_MAXCLASS - SC_NTINY + 1;
    .           		size_t lg_ceil = lg_floor(pow2_ceil_zu(size));
    .           		return (lg_ceil < lg_tmin ? (ZU(1) << lg_tmin) :
    .           		    (ZU(1) << lg_ceil));
    .           	}
    .           #endif
    .           	{
    8 ( 0.00%)  		size_t x = lg_floor((size<<1)-1);
    4 ( 0.00%)  		size_t lg_delta = (x < SC_LG_NGROUP + LG_QUANTUM + 1)
   12 ( 0.00%)  		    ?  LG_QUANTUM : x - SC_LG_NGROUP - 1;
    8 ( 0.00%)  		size_t delta = ZU(1) << lg_delta;
    .           		size_t delta_mask = delta - 1;
   12 ( 0.00%)  		size_t usize = (size + delta_mask) & ~delta_mask;
    2 ( 0.00%)  		return usize;
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           sz_s2u_lookup(size_t size) {
    .           	size_t ret = sz_index2size_lookup(sz_size2index_lookup(size));
    .           
    .           	assert(ret == sz_s2u_compute(size));
-- line 292 ----------------------------------------
-- line 294 ----------------------------------------
    .           }
    .           
    .           /*
    .            * Compute usable size that would result from allocating an object with the
    .            * specified size.
    .            */
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           sz_s2u(size_t size) {
  236 ( 0.01%)  	if (likely(size <= SC_LOOKUP_MAXCLASS)) {
    .           		return sz_s2u_lookup(size);
    .           	}
    .           	return sz_s2u_compute(size);
    .           }
    .           
    .           /*
    .            * Compute usable size that would result from allocating an object with the
    .            * specified size and alignment.
-- line 310 ----------------------------------------
-- line 311 ----------------------------------------
    .            */
    .           JEMALLOC_ALWAYS_INLINE size_t
    .           sz_sa2u(size_t size, size_t alignment) {
    .           	size_t usize;
    .           
    .           	assert(alignment != 0 && ((alignment - 1) & alignment) == 0);
    .           
    .           	/* Try for a small size class. */
    6 ( 0.00%)  	if (size <= SC_SMALL_MAXCLASS && alignment <= PAGE) {
    .           		/*
    .           		 * Round size up to the nearest multiple of alignment.
    .           		 *
    .           		 * This done, we can take advantage of the fact that for each
    .           		 * small size class, every object is aligned at the smallest
    .           		 * power of two that is non-zero in the base two representation
    .           		 * of the size.  For example:
    .           		 *
-- line 327 ----------------------------------------
-- line 334 ----------------------------------------
    .           		usize = sz_s2u(ALIGNMENT_CEILING(size, alignment));
    .           		if (usize < SC_LARGE_MINCLASS) {
    .           			return usize;
    .           		}
    .           	}
    .           
    .           	/* Large size class.  Beware of overflow. */
    .           
    6 ( 0.00%)  	if (unlikely(alignment > SC_LARGE_MAXCLASS)) {
    4 ( 0.00%)  		return 0;
    .           	}
    .           
    .           	/* Make sure result is a large size class. */
    4 ( 0.00%)  	if (size <= SC_LARGE_MINCLASS) {
    .           		usize = SC_LARGE_MINCLASS;
    .           	} else {
    .           		usize = sz_s2u(size);
    4 ( 0.00%)  		if (usize < size) {
    .           			/* size_t overflow. */
    .           			return 0;
    .           		}
    .           	}
    .           
    .           	/*
    .           	 * Calculate the multi-page mapping that large_palloc() would need in
    .           	 * order to guarantee the alignment.
    .           	 */
   12 ( 0.00%)  	if (usize + sz_large_pad + PAGE_CEILING(alignment) - PAGE < usize) {
    .           		/* size_t overflow. */
    .           		return 0;
    .           	}
    .           	return usize;
    .           }
    .           
    .           size_t sz_psz_quantize_floor(size_t size);
    .           size_t sz_psz_quantize_ceil(size_t size);
-- line 369 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata_cache.c
--------------------------------------------------------------------------------
Ir           

  .           #include "jemalloc/internal/jemalloc_preamble.h"
  .           #include "jemalloc/internal/jemalloc_internal_includes.h"
  .           
  .           bool
  6 ( 0.00%)  edata_cache_init(edata_cache_t *edata_cache, base_t *base) {
  1 ( 0.00%)  	edata_avail_new(&edata_cache->avail);
  4 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_avail_new (1x)
  .           	/*
  .           	 * This is not strictly necessary, since the edata_cache_t is only
  .           	 * created inside an arena, which is zeroed on creation.  But this is
  .           	 * handy as a safety measure.
  .           	 */
  .           	atomic_store_zu(&edata_cache->count, 0, ATOMIC_RELAXED);
  7 ( 0.00%)  	if (malloc_mutex_init(&edata_cache->mtx, "edata_cache",
127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
  .           	    WITNESS_RANK_EDATA_CACHE, malloc_mutex_rank_exclusive)) {
  .           		return true;
  .           	}
  1 ( 0.00%)  	edata_cache->base = base;
  .           	return false;
  4 ( 0.00%)  }
  .           
  .           edata_t *
264 ( 0.01%)  edata_cache_get(tsdn_t *tsdn, edata_cache_t *edata_cache) {
  .           	malloc_mutex_lock(tsdn, &edata_cache->mtx);
 99 ( 0.00%)  	edata_t *edata = edata_avail_first(&edata_cache->avail);
198 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_avail_first (33x)
 66 ( 0.00%)  	if (edata == NULL) {
  .           		malloc_mutex_unlock(tsdn, &edata_cache->mtx);
 99 ( 0.00%)  		return base_alloc_edata(tsdn, edata_cache->base);
27,864 ( 0.99%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_alloc_edata (33x)
  .           	}
  .           	edata_avail_remove(&edata_cache->avail, edata);
  .           	atomic_load_sub_store_zu(&edata_cache->count, 1);
  .           	malloc_mutex_unlock(tsdn, &edata_cache->mtx);
  .           	return edata;
165 ( 0.01%)  }
  .           
  .           void
  .           edata_cache_put(tsdn_t *tsdn, edata_cache_t *edata_cache, edata_t *edata) {
  .           	malloc_mutex_lock(tsdn, &edata_cache->mtx);
  .           	edata_avail_insert(&edata_cache->avail, edata);
  .           	atomic_load_add_store_zu(&edata_cache->count, 1);
  .           	malloc_mutex_unlock(tsdn, &edata_cache->mtx);
  .           }
-- line 41 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/pai.h
--------------------------------------------------------------------------------
Ir           

-- line 35 ----------------------------------------
  .            * These are just simple convenience functions to avoid having to reference the
  .            * same pai_t twice on every invocation.
  .            */
  .           
  .           static inline edata_t *
  .           pai_alloc(tsdn_t *tsdn, pai_t *self, size_t size, size_t alignment,
  .               bool zero, bool guarded, bool frequent_reuse,
  .               bool *deferred_work_generated) {
256 ( 0.01%)  	return self->alloc(tsdn, self, size, alignment, zero, guarded,
86,324 ( 3.06%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pac.c:pac_alloc_impl (32x)
  .           	    frequent_reuse, deferred_work_generated);
  .           }
  .           
  .           static inline size_t
  .           pai_alloc_batch(tsdn_t *tsdn, pai_t *self, size_t size, size_t nallocs,
  .               edata_list_active_t *results, bool *deferred_work_generated) {
  .           	return self->alloc_batch(tsdn, self, size, nallocs, results,
  .           	    deferred_work_generated);
-- line 51 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/eset.c
--------------------------------------------------------------------------------
Ir             

-- line 2 ----------------------------------------
    .           #include "jemalloc/internal/jemalloc_internal_includes.h"
    .           
    .           #include "jemalloc/internal/eset.h"
    .           
    .           #define ESET_NPSIZES (SC_NPSIZES + 1)
    .           
    .           static void
    .           eset_bin_init(eset_bin_t *bin) {
1,350 ( 0.05%)  	edata_heap_new(&bin->heap);
4,800 ( 0.17%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_new (1,200x)
    .           	/*
    .           	 * heap_min doesn't need initialization; it gets filled in when the bin
    .           	 * goes from non-empty to empty.
    .           	 */
    .           }
    .           
    .           static void
    .           eset_bin_stats_init(eset_bin_stats_t *bin_stats) {
    .           	atomic_store_zu(&bin_stats->nextents, 0, ATOMIC_RELAXED);
    .           	atomic_store_zu(&bin_stats->nbytes, 0, ATOMIC_RELAXED);
    .           }
    .           
    .           void
   84 ( 0.00%)  eset_init(eset_t *eset, extent_state_t state) {
1,650 ( 0.06%)  	for (unsigned i = 0; i < ESET_NPSIZES; i++) {
    .           		eset_bin_init(&eset->bins[i]);
    .           		eset_bin_stats_init(&eset->bin_stats[i]);
    .           	}
    .           	fb_init(eset->bitmap, ESET_NPSIZES);
    .           	edata_list_inactive_init(&eset->lru);
    6 ( 0.00%)  	eset->state = state;
   48 ( 0.00%)  }
    .           
    .           size_t
    .           eset_npages_get(eset_t *eset) {
    .           	return atomic_load_zu(&eset->npages, ATOMIC_RELAXED);
    .           }
    .           
    .           size_t
    .           eset_nextents_get(eset_t *eset, pszind_t pind) {
-- line 40 ----------------------------------------
-- line 45 ----------------------------------------
    .           eset_nbytes_get(eset_t *eset, pszind_t pind) {
    .           	return atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
    .           }
    .           
    .           static void
    .           eset_stats_add(eset_t *eset, pszind_t pind, size_t sz) {
    .           	size_t cur = atomic_load_zu(&eset->bin_stats[pind].nextents,
    .           	    ATOMIC_RELAXED);
   32 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nextents, cur + 1,
    .           	    ATOMIC_RELAXED);
    .           	cur = atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
   32 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nbytes, cur + sz,
    .           	    ATOMIC_RELAXED);
    .           }
    .           
    .           static void
    .           eset_stats_sub(eset_t *eset, pszind_t pind, size_t sz) {
    .           	size_t cur = atomic_load_zu(&eset->bin_stats[pind].nextents,
    .           	    ATOMIC_RELAXED);
   31 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nextents, cur - 1,
    .           	    ATOMIC_RELAXED);
    .           	cur = atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
   31 ( 0.00%)  	atomic_store_zu(&eset->bin_stats[pind].nbytes, cur - sz,
    .           	    ATOMIC_RELAXED);
    .           }
    .           
    .           void
  384 ( 0.01%)  eset_insert(eset_t *eset, edata_t *edata) {
    .           	assert(edata_state_get(edata) == eset->state);
    .           
    .           	size_t size = edata_size_get(edata);
   96 ( 0.00%)  	size_t psz = sz_psz_quantize_floor(size);
  832 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_psz_quantize_floor (32x)
    .           	pszind_t pind = sz_psz2ind(psz);
    .           
    .           	edata_cmp_summary_t edata_cmp_summary = edata_cmp_summary_get(edata);
  352 ( 0.01%)  	if (edata_heap_empty(&eset->bins[pind].heap)) {
  128 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_empty (32x)
    .           		fb_set(eset->bitmap, ESET_NPSIZES, (size_t)pind);
    .           		/* Only element is automatically the min element. */
   96 ( 0.00%)  		eset->bins[pind].heap_min = edata_cmp_summary;
    .           	} else {
    .           		/*
    .           		 * There's already a min element; update the summary if we're
    .           		 * about to insert a lower one.
    .           		 */
    .           		if (edata_cmp_summary_comp(edata_cmp_summary,
    .           		    eset->bins[pind].heap_min) < 0) {
    .           			eset->bins[pind].heap_min = edata_cmp_summary;
    .           		}
    .           	}
   64 ( 0.00%)  	edata_heap_insert(&eset->bins[pind].heap, edata);
  896 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_insert (32x)
    .           
    .           	if (config_stats) {
    .           		eset_stats_add(eset, pind, size);
    .           	}
    .           
    .           	edata_list_inactive_append(&eset->lru, edata);
   32 ( 0.00%)  	size_t npages = size >> LG_PAGE;
    .           	/*
    .           	 * All modifications to npages hold the mutex (as asserted above), so we
    .           	 * don't need an atomic fetch-add; we can get by with a load followed by
    .           	 * a store.
    .           	 */
    .           	size_t cur_eset_npages =
    .           	    atomic_load_zu(&eset->npages, ATOMIC_RELAXED);
   32 ( 0.00%)  	atomic_store_zu(&eset->npages, cur_eset_npages + npages,
    .           	    ATOMIC_RELAXED);
  256 ( 0.01%)  }
    .           
    .           void
  310 ( 0.01%)  eset_remove(eset_t *eset, edata_t *edata) {
    .           	assert(edata_state_get(edata) == eset->state ||
    .           	    edata_state_in_transition(edata_state_get(edata)));
    .           
    .           	size_t size = edata_size_get(edata);
   93 ( 0.00%)  	size_t psz = sz_psz_quantize_floor(size);
  806 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_psz_quantize_floor (31x)
    .           	pszind_t pind = sz_psz2ind(psz);
    .           	if (config_stats) {
    .           		eset_stats_sub(eset, pind, size);
    .           	}
    .           
    .           	edata_cmp_summary_t edata_cmp_summary = edata_cmp_summary_get(edata);
  124 ( 0.00%)  	edata_heap_remove(&eset->bins[pind].heap, edata);
  837 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_remove (31x)
  155 ( 0.01%)  	if (edata_heap_empty(&eset->bins[pind].heap)) {
  124 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_empty (31x)
    .           		fb_unset(eset->bitmap, ESET_NPSIZES, (size_t)pind);
    .           	} else {
    .           		/*
    .           		 * This is a little weird; we compare if the summaries are
    .           		 * equal, rather than if the edata we removed was the heap
    .           		 * minimum.  The reason why is that getting the heap minimum
    .           		 * can cause a pairing heap merge operation.  We can avoid this
    .           		 * if we only update the min if it's changed, in which case the
-- line 135 ----------------------------------------
-- line 146 ----------------------------------------
    .           	size_t npages = size >> LG_PAGE;
    .           	/*
    .           	 * As in eset_insert, we hold eset->mtx and so don't need atomic
    .           	 * operations for updating eset->npages.
    .           	 */
    .           	size_t cur_extents_npages =
    .           	    atomic_load_zu(&eset->npages, ATOMIC_RELAXED);
    .           	assert(cur_extents_npages >= npages);
   31 ( 0.00%)  	atomic_store_zu(&eset->npages,
   31 ( 0.00%)  	    cur_extents_npages - (size >> LG_PAGE), ATOMIC_RELAXED);
  248 ( 0.01%)  }
    .           
    .           /*
    .            * Find an extent with size [min_size, max_size) to satisfy the alignment
    .            * requirement.  For each size, try only the first extent in the heap.
    .            */
    .           static edata_t *
    .           eset_fit_alignment(eset_t *eset, size_t min_size, size_t max_size,
    .               size_t alignment) {
-- line 164 ----------------------------------------
-- line 203 ----------------------------------------
    .            * for others.
    .            */
    .           static edata_t *
    .           eset_first_fit(eset_t *eset, size_t size, bool exact_only,
    .               unsigned lg_max_fit) {
    .           	edata_t *ret = NULL;
    .           	edata_cmp_summary_t ret_summ JEMALLOC_CC_SILENCE_INIT({0});
    .           
  256 ( 0.01%)  	pszind_t pind = sz_psz2ind(sz_psz_quantize_ceil(size));
1,900 ( 0.07%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sz.c:_rjem_je_sz_psz_quantize_ceil (64x)
    .           
  128 ( 0.00%)  	if (exact_only) {
   66 ( 0.00%)  		return edata_heap_empty(&eset->bins[pind].heap) ? NULL :
    .           		    edata_heap_first(&eset->bins[pind].heap);
    .           	}
    .           
   31 ( 0.00%)  	for (pszind_t i =
    .           	    (pszind_t)fb_ffs(eset->bitmap, ESET_NPSIZES, (size_t)pind);
   62 ( 0.00%)  	    i < ESET_NPSIZES;
   31 ( 0.00%)  	    i = (pszind_t)fb_ffs(eset->bitmap, ESET_NPSIZES, (size_t)i + 1)) {
    .           		assert(!edata_heap_empty(&eset->bins[i].heap));
  186 ( 0.01%)  		if (lg_max_fit == SC_PTR_BITS) {
    .           			/*
    .           			 * We'll shift by this below, and shifting out all the
    .           			 * bits is undefined.  Decreasing is safe, since the
    .           			 * page size is larger than 1 byte.
    .           			 */
   64 ( 0.00%)  			lg_max_fit = SC_PTR_BITS - 1;
    .           		}
  155 ( 0.01%)  		if ((sz_pind2sz(i) >> lg_max_fit) > size) {
    .           			break;
    .           		}
   62 ( 0.00%)  		if (ret == NULL || edata_cmp_summary_comp(
    .           		    eset->bins[i].heap_min, ret_summ) < 0) {
    .           			/*
    .           			 * We grab the edata as early as possible, even though
    .           			 * we might change it later.  Practically, a large
    .           			 * portion of eset_fit calls succeed at the first valid
    .           			 * index, so this doesn't cost much, and we get the
    .           			 * effect of prefetching the edata as early as possible.
    .           			 */
  186 ( 0.01%)  			edata_t *edata = edata_heap_first(&eset->bins[i].heap);
  744 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_first (31x)
    .           			assert(edata_size_get(edata) >= size);
    .           			assert(ret == NULL || edata_snad_comp(edata, ret) < 0);
    .           			assert(ret == NULL || edata_cmp_summary_comp(
    .           			    eset->bins[i].heap_min,
    .           			    edata_cmp_summary_get(edata)) == 0);
    .           			ret = edata;
  217 ( 0.01%)  			ret_summ = eset->bins[i].heap_min;
    .           		}
   62 ( 0.00%)  		if (i == SC_NPSIZES) {
    .           			break;
    .           		}
    .           		assert(i < SC_NPSIZES);
    .           	}
    .           
    .           	return ret;
    .           }
    .           
    .           edata_t *
    .           eset_fit(eset_t *eset, size_t esize, size_t alignment, bool exact_only,
  576 ( 0.02%)      unsigned lg_max_fit) {
  256 ( 0.01%)  	size_t max_size = esize + PAGE_CEILING(alignment) - PAGE;
    .           	/* Beware size_t wrap-around. */
  192 ( 0.01%)  	if (max_size < esize) {
    .           		return NULL;
    .           	}
    .           
  128 ( 0.00%)  	edata_t *edata = eset_first_fit(eset, max_size, exact_only, lg_max_fit);
    .           
  190 ( 0.01%)  	if (alignment > PAGE && edata == NULL) {
    .           		/*
    .           		 * max_size guarantees the alignment requirement but is rather
    .           		 * pessimistic.  Next we try to satisfy the aligned allocation
    .           		 * with sizes in [esize, max_size).
    .           		 */
    .           		edata = eset_fit_alignment(eset, esize, max_size, alignment);
    .           	}
    .           
    .           	return edata;
  576 ( 0.02%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/tsd.h
--------------------------------------------------------------------------------
Ir           

-- line 261 ----------------------------------------
  .           JEMALLOC_ALWAYS_INLINE uint8_t
  .           tsd_state_get(tsd_t *tsd) {
  .           	/*
  .           	 * This should be atomic.  Unfortunately, compilers right now can't tell
  .           	 * that this can be done as a memory comparison, and forces a load into
  .           	 * a register that hurts fast-path performance.
  .           	 */
  .           	/* return atomic_load_u8(&tsd->state, ATOMIC_RELAXED); */
 81 ( 0.00%)  	return *(uint8_t *)&tsd->state;
  .           }
  .           
  .           /*
  .            * Wrapper around tsd_t that makes it possible to avoid implicit conversion
  .            * between tsd_t and tsdn_t, where tsdn_t is "nullable" and has to be
  .            * explicitly converted to tsd_t, which is non-nullable.
  .            */
  .           struct tsdn_s {
-- line 277 ----------------------------------------
-- line 315 ----------------------------------------
  .            * foo.  This omits some safety checks, and so can be used during tsd
  .            * initialization and cleanup.
  .            */
  .           #define O(n, t, nt)							\
  .           JEMALLOC_ALWAYS_INLINE t *						\
  .           tsd_##n##p_get_unsafe(tsd_t *tsd) {					\
  .           	return &tsd->TSD_MANGLE(n);					\
  .           }
328 ( 0.01%)  TSD_DATA_SLOW
366 ( 0.01%)  TSD_DATA_FAST
  .           TSD_DATA_SLOWER
  .           #undef O
  .           
  .           /* tsd_foop_get(tsd) returns a pointer to the thread-local instance of foo. */
  .           #define O(n, t, nt)							\
  .           JEMALLOC_ALWAYS_INLINE t *						\
  .           tsd_##n##p_get(tsd_t *tsd) {						\
  .           	/*								\
-- line 332 ----------------------------------------
-- line 365 ----------------------------------------
  .           #undef O
  .           
  .           /* tsd_foo_get(tsd) returns the value of the thread-local instance of foo. */
  .           #define O(n, t, nt)							\
  .           JEMALLOC_ALWAYS_INLINE t						\
  .           tsd_##n##_get(tsd_t *tsd) {						\
  .           	return *tsd_##n##p_get(tsd);					\
  .           }
 30 ( 0.00%)  TSD_DATA_SLOW
  .           TSD_DATA_FAST
  .           TSD_DATA_SLOWER
  .           #undef O
  .           
  .           /* tsd_foo_set(tsd, val) updates the thread-local instance of foo to be val. */
  .           #define O(n, t, nt)							\
  .           JEMALLOC_ALWAYS_INLINE void						\
  .           tsd_##n##_set(tsd_t *tsd, t val) {					\
  .           	assert(tsd_state_get(tsd) != tsd_state_reincarnated &&		\
  .           	    tsd_state_get(tsd) != tsd_state_minimal_initialized);	\
  .           	*tsd_##n##p_get(tsd) = val;					\
  .           }
  5 ( 0.00%)  TSD_DATA_SLOW
  .           TSD_DATA_FAST
  .           TSD_DATA_SLOWER
  .           #undef O
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           tsd_assert_fast(tsd_t *tsd) {
  .           	/*
  .           	 * Note that our fastness assertion does *not* include global slowness
-- line 394 ----------------------------------------
-- line 413 ----------------------------------------
  .           tsd_fetch_impl(bool init, bool minimal) {
  .           	tsd_t *tsd = tsd_get(init);
  .           
  .           	if (!init && tsd_get_allocates() && tsd == NULL) {
  .           		return NULL;
  .           	}
  .           	assert(tsd != NULL);
  .           
 94 ( 0.00%)  	if (unlikely(tsd_state_get(tsd) != tsd_state_nominal)) {
  3 ( 0.00%)  		return tsd_fetch_slow(tsd, minimal);
9,015 ( 0.32%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tsd.c:_rjem_je_tsd_fetch_slow (1x)
  .           	}
  .           	assert(tsd_fast(tsd));
  .           	tsd_assert_fast(tsd);
  .           
 92 ( 0.00%)  	return tsd;
  .           }
  .           
  .           /* Get a minimal TSD that requires no cleanup.  See comments in free(). */
  .           JEMALLOC_ALWAYS_INLINE tsd_t *
  .           tsd_fetch_min(void) {
  .           	return tsd_fetch_impl(true, true);
  .           }
  .           
-- line 435 ----------------------------------------
-- line 471 ----------------------------------------
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE rtree_ctx_t *
  .           tsdn_rtree_ctx(tsdn_t *tsdn, rtree_ctx_t *fallback) {
  .           	/*
  .           	 * If tsd cannot be accessed, initialize the fallback rtree_ctx and
  .           	 * return a pointer to it.
  .           	 */
368 ( 0.01%)  	if (unlikely(tsdn_null(tsdn))) {
  .           		rtree_ctx_data_init(fallback);
  .           		return fallback;
  .           	}
  .           	return tsd_rtree_ctx(tsdn_tsd(tsdn));
  .           }
  .           
  .           static inline bool
  .           tsd_state_nocleanup(tsd_t *tsd) {
-- line 487 ----------------------------------------
-- line 493 ----------------------------------------
  .            * These "raw" tsd reentrancy functions don't have any debug checking to make
  .            * sure that we're not touching arena 0.  Better is to call pre_reentrancy and
  .            * post_reentrancy if this is possible.
  .            */
  .           static inline void
  .           tsd_pre_reentrancy_raw(tsd_t *tsd) {
  .           	bool fast = tsd_fast(tsd);
  .           	assert(tsd_reentrancy_level_get(tsd) < INT8_MAX);
  1 ( 0.00%)  	++*tsd_reentrancy_levelp_get(tsd);
  2 ( 0.00%)  	if (fast) {
  .           		/* Prepare slow path for reentrancy. */
  .           		tsd_slow_update(tsd);
  .           		assert(tsd_state_get(tsd) == tsd_state_nominal_slow);
  .           	}
  .           }
  .           
  .           static inline void
  .           tsd_post_reentrancy_raw(tsd_t *tsd) {
  .           	int8_t *reentrancy_level = tsd_reentrancy_levelp_get(tsd);
  .           	assert(*reentrancy_level > 0);
  2 ( 0.00%)  	if (--*reentrancy_level == 0) {
  3 ( 0.00%)  		tsd_slow_update(tsd);
 38 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tsd.c:_rjem_je_tsd_slow_update (1x)
  .           	}
  .           }
  .           
  .           #endif /* JEMALLOC_INTERNAL_TSD_H */

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/mutex.h
--------------------------------------------------------------------------------
Ir           

-- line 149 ----------------------------------------
  .           static inline void
  .           malloc_mutex_lock_final(malloc_mutex_t *mutex) {
  .           	MALLOC_MUTEX_LOCK(mutex);
  .           	atomic_store_b(&mutex->locked, true, ATOMIC_RELAXED);
  .           }
  .           
  .           static inline bool
  .           malloc_mutex_trylock_final(malloc_mutex_t *mutex) {
940 ( 0.03%)  	return MALLOC_MUTEX_TRYLOCK(mutex);
2,048 ( 0.07%)  => ./nptl/./nptl/pthread_mutex_trylock.c:pthread_mutex_trylock@@GLIBC_2.34 (64x)
  .           }
  .           
  .           static inline void
  .           mutex_owner_stats_update(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	if (config_stats) {
  .           		mutex_prof_data_t *data = &mutex->prof_data;
232 ( 0.01%)  		data->n_lock_ops++;
465 ( 0.02%)  		if (data->prev_owner != tsdn) {
 36 ( 0.00%)  			data->prev_owner = tsdn;
 36 ( 0.00%)  			data->n_owner_switches++;
  .           		}
  .           	}
  .           }
  .           
  .           /* Trylock: return false if the lock is successfully acquired. */
  .           static inline bool
  .           malloc_mutex_trylock(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	witness_assert_not_owner(tsdn_witness_tsdp_get(tsdn), &mutex->witness);
-- line 175 ----------------------------------------
-- line 208 ----------------------------------------
  .           	sum->n_owner_switches += data->n_owner_switches;
  .           	sum->n_lock_ops += data->n_lock_ops;
  .           }
  .           
  .           static inline void
  .           malloc_mutex_lock(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	witness_assert_not_owner(tsdn_witness_tsdp_get(tsdn), &mutex->witness);
  .           	if (isthreaded) {
489 ( 0.02%)  		if (malloc_mutex_trylock_final(mutex)) {
  .           			malloc_mutex_lock_slow(mutex);
  .           			atomic_store_b(&mutex->locked, true, ATOMIC_RELAXED);
  .           		}
  .           		mutex_owner_stats_update(tsdn, mutex);
  .           	}
  .           	witness_lock(tsdn_witness_tsdp_get(tsdn), &mutex->witness);
  .           }
  .           
  .           static inline void
  .           malloc_mutex_unlock(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	atomic_store_b(&mutex->locked, false, ATOMIC_RELAXED);
  .           	witness_unlock(tsdn_witness_tsdp_get(tsdn), &mutex->witness);
  .           	if (isthreaded) {
699 ( 0.02%)  		MALLOC_MUTEX_UNLOCK(mutex);
1,344 ( 0.05%)  => ./nptl/./nptl/pthread_mutex_unlock.c:pthread_mutex_unlock@@GLIBC_2.2.5 (64x)
  .           	}
  .           }
  .           
  .           static inline void
  .           malloc_mutex_assert_owner(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	witness_assert_owner(tsdn_witness_tsdp_get(tsdn), &mutex->witness);
  .           }
  .           
-- line 238 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/arena_inlines_b.h
--------------------------------------------------------------------------------
Ir           

-- line 14 ----------------------------------------
  .           static inline arena_t *
  .           arena_get_from_edata(edata_t *edata) {
  .           	return (arena_t *)atomic_load_p(&arenas[edata_arena_ind_get(edata)],
  .           	    ATOMIC_RELAXED);
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE arena_t *
  .           arena_choose_maybe_huge(tsd_t *tsd, arena_t *arena, size_t size) {
  2 ( 0.00%)  	if (arena != NULL) {
  .           		return arena;
  .           	}
  .           
  .           	/*
  .           	 * For huge allocations, use the dedicated huge arena if both are true:
  .           	 * 1) is using auto arena selection (i.e. arena == NULL), and 2) the
  .           	 * thread is not assigned to a manual arena.
  .           	 */
-- line 30 ----------------------------------------
-- line 111 ----------------------------------------
  .           	cassert(config_prof);
  .           
  .           	assert(!edata_slab_get(edata));
  .           	large_prof_info_set(edata, tctx, size);
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void
  .           arena_decay_ticks(tsdn_t *tsdn, arena_t *arena, unsigned nticks) {
 46 ( 0.00%)  	if (unlikely(tsdn_null(tsdn))) {
  .           		return;
  .           	}
  .           	tsd_t *tsd = tsdn_tsd(tsdn);
  .           	/*
  .           	 * We use the ticker_geom_t to avoid having per-arena state in the tsd.
  .           	 * Instead of having a countdown-until-decay timer running for every
  .           	 * arena in every thread, we flip a coin once per tick, whose
  .           	 * probability of coming up heads is 1/nticks; this is effectively the
-- line 127 ----------------------------------------
-- line 141 ----------------------------------------
  .           	arena_decay_ticks(tsdn, arena, 1);
  .           }
  .           
  .           JEMALLOC_ALWAYS_INLINE void *
  .           arena_malloc(tsdn_t *tsdn, arena_t *arena, size_t size, szind_t ind, bool zero,
  .               tcache_t *tcache, bool slow_path) {
  .           	assert(!tsdn_null(tsdn) || tcache == NULL);
  .           
 58 ( 0.00%)  	if (likely(tcache != NULL)) {
  2 ( 0.00%)  		if (likely(size <= SC_SMALL_MAXCLASS)) {
  .           			return tcache_alloc_small(tsdn_tsd(tsdn), arena,
  .           			    tcache, size, ind, zero, slow_path);
  .           		}
  .           		if (likely(size <= tcache_maxclass)) {
  .           			return tcache_alloc_large(tsdn_tsd(tsdn), arena,
  .           			    tcache, size, ind, zero, slow_path);
  .           		}
  .           		/* (size > tcache_maxclass) case falls through. */
-- line 158 ----------------------------------------
-- line 408 ----------------------------------------
  .           
  .           	if (config_debug) {
  .           		edata_t *edata = emap_edata_lookup(tsdn, &arena_emap_global,
  .           		    ptr);
  .           		assert(alloc_ctx.szind == edata_szind_get(edata));
  .           		assert(alloc_ctx.slab == edata_slab_get(edata));
  .           	}
  .           
 60 ( 0.00%)  	if (likely(alloc_ctx.slab)) {
  .           		/* Small allocation. */
  .           		tcache_dalloc_small(tsdn_tsd(tsdn), tcache, ptr,
  .           		    alloc_ctx.szind, slow_path);
  .           	} else {
  .           		arena_dalloc_large(tsdn, ptr, tcache, alloc_ctx.szind,
  .           		    slow_path);
  .           	}
  .           }
  .           
  .           static inline void
  .           arena_cache_oblivious_randomize(tsdn_t *tsdn, arena_t *arena, edata_t *edata,
  .               size_t alignment) {
  .           	assert(edata_base_get(edata) == edata_addr_get(edata));
  .           
  5 ( 0.00%)  	if (alignment < PAGE) {
  .           		unsigned lg_range = LG_PAGE -
  .           		    lg_floor(CACHELINE_CEILING(alignment));
  .           		size_t r;
  .           		if (!tsdn_null(tsdn)) {
  .           			tsd_t *tsd = tsdn_tsd(tsdn);
  .           			r = (size_t)prng_lg_range_u64(
  .           			    tsd_prng_statep_get(tsd), lg_range);
  .           		} else {
-- line 439 ----------------------------------------
-- line 538 ----------------------------------------
  .           		bin->stats.ndalloc += info->ndalloc;
  .           		assert(bin->stats.curregs >= (size_t)info->ndalloc);
  .           		bin->stats.curregs -= (size_t)info->ndalloc;
  .           	}
  .           }
  .           
  .           static inline bin_t *
  .           arena_get_bin(arena_t *arena, szind_t binind, unsigned binshard) {
 88 ( 0.00%)  	bin_t *shard0 = (bin_t *)((uintptr_t)arena + arena_bin_offsets[binind]);
110 ( 0.00%)  	return shard0 + binshard;
  .           }
  .           
  .           #endif /* JEMALLOC_INTERNAL_ARENA_INLINES_B_H */

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c
--------------------------------------------------------------------------------
Ir           

-- line 2 ----------------------------------------
  .           #include "jemalloc/internal/jemalloc_internal_includes.h"
  .           
  .           #include "jemalloc/internal/bit_util.h"
  .           #include "jemalloc/internal/cache_bin.h"
  .           #include "jemalloc/internal/safety_check.h"
  .           
  .           void
  .           cache_bin_info_init(cache_bin_info_t *info,
 41 ( 0.00%)      cache_bin_sz_t ncached_max) {
  .           	assert(ncached_max <= CACHE_BIN_NCACHED_MAX);
  .           	size_t stack_size = (size_t)ncached_max * sizeof(void *);
  .           	assert(stack_size < ((size_t)1 << (sizeof(cache_bin_sz_t) * 8)));
 41 ( 0.00%)  	info->ncached_max = (cache_bin_sz_t)ncached_max;
 41 ( 0.00%)  }
  .           
  .           void
  .           cache_bin_info_compute_alloc(cache_bin_info_t *infos, szind_t ninfos,
  2 ( 0.00%)      size_t *size, size_t *alignment) {
  .           	/* For the total bin stack region (per tcache), reserve 2 more slots so
  .           	 * that
  .           	 * 1) the empty position can be safely read on the fast path before
  .           	 *    checking "is_empty"; and
  .           	 * 2) the cur_ptr can go beyond the empty position by 1 step safely on
  .           	 * the fast path (i.e. no overflow).
  .           	 */
  1 ( 0.00%)  	*size = sizeof(void *) * 2;
 24 ( 0.00%)  	for (szind_t i = 0; i < ninfos; i++) {
  .           		assert(infos[i].ncached_max > 0);
104 ( 0.00%)  		*size += infos[i].ncached_max * sizeof(void *);
  .           	}
  .           
  .           	/*
  .           	 * Align to at least PAGE, to minimize the # of TLBs needed by the
  .           	 * smaller sizes; also helps if the larger sizes don't get used at all.
  .           	 */
  1 ( 0.00%)  	*alignment = PAGE;
  1 ( 0.00%)  }
  .           
  .           void
  .           cache_bin_preincrement(cache_bin_info_t *infos, szind_t ninfos, void *alloc,
  1 ( 0.00%)      size_t *cur_offset) {
  .           	if (config_debug) {
  .           		size_t computed_size;
  .           		size_t computed_alignment;
  .           
  .           		/* Pointer should be as aligned as we asked for. */
  .           		cache_bin_info_compute_alloc(infos, ninfos, &computed_size,
  .           		    &computed_alignment);
  .           		assert(((uintptr_t)alloc & (computed_alignment - 1)) == 0);
  .           	}
  .           
  3 ( 0.00%)  	*(uintptr_t *)((uintptr_t)alloc + *cur_offset) =
  .           	    cache_bin_preceding_junk;
  1 ( 0.00%)  	*cur_offset += sizeof(void *);
  1 ( 0.00%)  }
  .           
  .           void
  .           cache_bin_postincrement(cache_bin_info_t *infos, szind_t ninfos, void *alloc,
  1 ( 0.00%)      size_t *cur_offset) {
  3 ( 0.00%)  	*(uintptr_t *)((uintptr_t)alloc + *cur_offset) =
  .           	    cache_bin_trailing_junk;
  1 ( 0.00%)  	*cur_offset += sizeof(void *);
  1 ( 0.00%)  }
  .           
  .           void
  .           cache_bin_init(cache_bin_t *bin, cache_bin_info_t *info, void *alloc,
 82 ( 0.00%)      size_t *cur_offset) {
  .           	/*
  .           	 * The full_position points to the lowest available space.  Allocations
  .           	 * will access the slots toward higher addresses (for the benefit of
  .           	 * adjacent prefetch).
  .           	 */
 82 ( 0.00%)  	void *stack_cur = (void *)((uintptr_t)alloc + *cur_offset);
  .           	void *full_position = stack_cur;
 82 ( 0.00%)  	uint16_t bin_stack_size = info->ncached_max * sizeof(void *);
  .           
123 ( 0.00%)  	*cur_offset += bin_stack_size;
 41 ( 0.00%)  	void *empty_position = (void *)((uintptr_t)alloc + *cur_offset);
  .           
  .           	/* Init to the empty position. */
 41 ( 0.00%)  	bin->stack_head = (void **)empty_position;
 41 ( 0.00%)  	bin->low_bits_low_water = (uint16_t)(uintptr_t)bin->stack_head;
 41 ( 0.00%)  	bin->low_bits_full = (uint16_t)(uintptr_t)full_position;
 41 ( 0.00%)  	bin->low_bits_empty = (uint16_t)(uintptr_t)empty_position;
  .           	cache_bin_sz_t free_spots = cache_bin_diff(bin,
  .           	    bin->low_bits_full, (uint16_t)(uintptr_t)bin->stack_head,
  .           	    /* racy */ false);
  .           	assert(free_spots == bin_stack_size);
  .           	assert(cache_bin_ncached_get_local(bin, info) == 0);
  .           	assert(cache_bin_empty_position_get(bin) == empty_position);
  .           
  .           	assert(bin_stack_size > 0 || empty_position == full_position);
 41 ( 0.00%)  }
  .           
  .           bool
  .           cache_bin_still_zero_initialized(cache_bin_t *bin) {
  .           	return bin->stack_head == NULL;
  .           }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c
--------------------------------------------------------------------------------
Ir           

-- line 82 ----------------------------------------
  .           /******************************************************************************/
  .           
  .           size_t
  .           tcache_salloc(tsdn_t *tsdn, const void *ptr) {
  .           	return arena_salloc(tsdn, ptr);
  .           }
  .           
  .           uint64_t
  1 ( 0.00%)  tcache_gc_new_event_wait(tsd_t *tsd) {
  .           	return opt_tcache_gc_incr_bytes;
  2 ( 0.00%)  }
  .           
  .           uint64_t
  .           tcache_gc_postponed_event_wait(tsd_t *tsd) {
  .           	return TE_MIN_START_WAIT;
  .           }
  .           
  .           uint64_t
  3 ( 0.00%)  tcache_gc_dalloc_new_event_wait(tsd_t *tsd) {
  .           	return opt_tcache_gc_incr_bytes;
  .           }
  .           
  .           uint64_t
  .           tcache_gc_dalloc_postponed_event_wait(tsd_t *tsd) {
  .           	return TE_MIN_START_WAIT;
  .           }
  .           
  .           static uint8_t
  .           tcache_gc_item_delay_compute(szind_t szind) {
  .           	assert(szind < SC_NBINS);
  .           	size_t sz = sz_index2size(szind);
108 ( 0.00%)  	size_t item_delay = opt_tcache_gc_delay_bytes / sz;
  .           	size_t delay_max = ZU(1)
  .           	    << (sizeof(((tcache_slow_t *)NULL)->bin_flush_delay_items[0]) * 8);
108 ( 0.00%)  	if (item_delay >= delay_max) {
  .           		item_delay = delay_max - 1;
  .           	}
 36 ( 0.00%)  	return (uint8_t)item_delay;
  .           }
  .           
  .           static void
  .           tcache_gc_small(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
  .               szind_t szind) {
  .           	/* Aim to flush 3/4 of items below low-water. */
  .           	assert(szind < SC_NBINS);
  .           
-- line 127 ----------------------------------------
-- line 223 ----------------------------------------
  .           tcache_gc_dalloc_event_handler(tsd_t *tsd, uint64_t elapsed) {
  .           	assert(elapsed == TE_INVALID_ELAPSED);
  .           	tcache_event(tsd);
  .           }
  .           
  .           void *
  .           tcache_alloc_small_hard(tsdn_t *tsdn, arena_t *arena,
  .               tcache_t *tcache, cache_bin_t *cache_bin, szind_t binind,
198 ( 0.01%)      bool *tcache_success) {
 22 ( 0.00%)  	tcache_slow_t *tcache_slow = tcache->tcache_slow;
  .           	void *ret;
  .           
  .           	assert(tcache_slow->arena != NULL);
 88 ( 0.00%)  	unsigned nfill = cache_bin_info_ncached_max(&tcache_bin_info[binind])
 22 ( 0.00%)  	    >> tcache_slow->lg_fill_div[binind];
 88 ( 0.00%)  	arena_cache_bin_fill_small(tsdn, arena, cache_bin,
114,772 ( 4.07%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_cache_bin_fill_small (22x)
  .           	    &tcache_bin_info[binind], binind, nfill);
 22 ( 0.00%)  	tcache_slow->bin_refilled[binind] = true;
  .           	ret = cache_bin_alloc(cache_bin, tcache_success);
  .           
  .           	return ret;
154 ( 0.01%)  }
  .           
  .           static const void *
  .           tcache_bin_flush_ptr_getter(void *arr_ctx, size_t ind) {
  .           	cache_bin_ptr_array_t *arr = (cache_bin_ptr_array_t *)arr_ctx;
  .           	return arr->ptr[ind];
  .           }
  .           
  .           static void
-- line 252 ----------------------------------------
-- line 542 ----------------------------------------
  .            *
  .            * The downside is, the time between stash and flush may be relatively short,
  .            * especially when the request rate is high.  It lowers the chance of detecting
  .            * write-after-free -- however that is a delayed detection anyway, and is less
  .            * of a focus than the memory overhead.
  .            */
  .           void
  .           tcache_bin_flush_stashed(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
374 ( 0.01%)      szind_t binind, bool is_small) {
 66 ( 0.00%)  	cache_bin_info_t *info = &tcache_bin_info[binind];
  .           	/*
  .           	 * The two below are for assertion only.  The content of original cached
  .           	 * items remain unchanged -- the stashed items reside on the other end
  .           	 * of the stack.  Checking the stack head and ncached to verify.
  .           	 */
  .           	void *head_content = *cache_bin->stack_head;
  .           	cache_bin_sz_t orig_cached = cache_bin_ncached_get_local(cache_bin,
  .           	    info);
  .           
  .           	cache_bin_sz_t nstashed = cache_bin_nstashed_get_local(cache_bin, info);
  .           	assert(orig_cached + nstashed <= cache_bin_info_ncached_max(info));
 44 ( 0.00%)  	if (nstashed == 0) {
  .           		return;
  .           	}
  .           
  .           	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nstashed);
  .           	cache_bin_init_ptr_array_for_stashed(cache_bin, binind, info, &ptrs,
  .           	    nstashed);
  .           	san_check_stashed_ptrs(ptrs.ptr, nstashed, sz_index2size(binind));
  .           	tcache_bin_flush_impl(tsd, tcache, cache_bin, binind, &ptrs, nstashed,
  .           	    is_small);
  .           	cache_bin_finish_flush_stashed(cache_bin, info);
  .           
  .           	assert(cache_bin_nstashed_get_local(cache_bin, info) == 0);
  .           	assert(cache_bin_ncached_get_local(cache_bin, info) == orig_cached);
  .           	assert(head_content == *cache_bin->stack_head);
242 ( 0.01%)  }
  .           
  .           void
  .           tcache_arena_associate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
 14 ( 0.00%)      tcache_t *tcache, arena_t *arena) {
  .           	assert(tcache_slow->arena == NULL);
  2 ( 0.00%)  	tcache_slow->arena = arena;
  .           
  .           	if (config_stats) {
  .           		/* Link into list of extant tcaches. */
  .           		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
  .           
  3 ( 0.00%)  		ql_elm_new(tcache_slow, link);
  4 ( 0.00%)  		ql_tail_insert(&arena->tcache_ql, tcache_slow, link);
  1 ( 0.00%)  		cache_bin_array_descriptor_init(
  2 ( 0.00%)  		    &tcache_slow->cache_bin_array_descriptor, tcache->bins);
  4 ( 0.00%)  		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
  .           		    &tcache_slow->cache_bin_array_descriptor, link);
  .           
  .           		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
  .           	}
  7 ( 0.00%)  }
  .           
  .           static void
  .           tcache_arena_dissociate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
  .               tcache_t *tcache) {
  .           	arena_t *arena = tcache_slow->arena;
  .           	assert(arena != NULL);
  .           	if (config_stats) {
  .           		/* Unlink from list of extant tcaches. */
-- line 607 ----------------------------------------
-- line 629 ----------------------------------------
  .           void
  .           tcache_arena_reassociate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
  .               tcache_t *tcache, arena_t *arena) {
  .           	tcache_arena_dissociate(tsdn, tcache_slow, tcache);
  .           	tcache_arena_associate(tsdn, tcache_slow, tcache, arena);
  .           }
  .           
  .           bool
  3 ( 0.00%)  tsd_tcache_enabled_data_init(tsd_t *tsd) {
  .           	/* Called upon tsd initialization. */
  1 ( 0.00%)  	tsd_tcache_enabled_set(tsd, opt_tcache);
  1 ( 0.00%)  	tsd_slow_update(tsd);
 19 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tsd.c:_rjem_je_tsd_slow_update (1x)
  .           
  2 ( 0.00%)  	if (opt_tcache) {
  .           		/* Trigger tcache init. */
  2 ( 0.00%)  		tsd_tcache_data_init(tsd);
8,643 ( 0.31%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tsd_tcache_data_init (1x)
  .           	}
  .           
  .           	return false;
  3 ( 0.00%)  }
  .           
  .           static void
 13 ( 0.00%)  tcache_init(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
  .               void *mem) {
  1 ( 0.00%)  	tcache->tcache_slow = tcache_slow;
  1 ( 0.00%)  	tcache_slow->tcache = tcache;
  .           
  .           	memset(&tcache_slow->link, 0, sizeof(ql_elm(tcache_t)));
  1 ( 0.00%)  	tcache_slow->next_gc_bin = 0;
  1 ( 0.00%)  	tcache_slow->arena = NULL;
  1 ( 0.00%)  	tcache_slow->dyn_alloc = mem;
  .           
  .           	/*
  .           	 * We reserve cache bins for all small size classes, even if some may
  .           	 * not get used (i.e. bins higher than nhbins).  This allows the fast
  .           	 * and common paths to access cache bin metadata safely w/o worrying
  .           	 * about which ones are disabled.
  .           	 */
  4 ( 0.00%)  	unsigned n_reserved_bins = nhbins < SC_NBINS ? SC_NBINS : nhbins;
  3 ( 0.00%)  	memset(tcache->bins, 0, sizeof(cache_bin_t) * n_reserved_bins);
  .           
  1 ( 0.00%)  	size_t cur_offset = 0;
  6 ( 0.00%)  	cache_bin_preincrement(tcache_bin_info, nhbins, mem,
  6 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c:_rjem_je_cache_bin_preincrement (1x)
  .           	    &cur_offset);
210 ( 0.01%)  	for (unsigned i = 0; i < nhbins; i++) {
 82 ( 0.00%)  		if (i < SC_NBINS) {
 72 ( 0.00%)  			tcache_slow->lg_fill_div[i] = 1;
 36 ( 0.00%)  			tcache_slow->bin_refilled[i] = false;
  .           			tcache_slow->bin_flush_delay_items[i]
  .           			    = tcache_gc_item_delay_compute(i);
  .           		}
  .           		cache_bin_t *cache_bin = &tcache->bins[i];
246 ( 0.01%)  		cache_bin_init(cache_bin, &tcache_bin_info[i], mem,
615 ( 0.02%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c:_rjem_je_cache_bin_init (41x)
  .           		    &cur_offset);
  .           	}
  .           	/*
  .           	 * For small size classes beyond tcache_maxclass (i.e. nhbins < NBINS),
  .           	 * their cache bins are initialized to a state to safely and efficiently
  .           	 * fail all fastpath alloc / free, so that no additional check around
  .           	 * nhbins is needed on fastpath.
  .           	 */
  2 ( 0.00%)  	for (unsigned i = nhbins; i < SC_NBINS; i++) {
  .           		/* Disabled small bins. */
  .           		cache_bin_t *cache_bin = &tcache->bins[i];
  .           		void *fake_stack = mem;
  .           		size_t fake_offset = 0;
  .           
  .           		cache_bin_init(cache_bin, &tcache_bin_info[i], fake_stack,
  .           		    &fake_offset);
  .           		assert(tcache_small_bin_disabled(i, cache_bin));
  .           	}
  .           
  5 ( 0.00%)  	cache_bin_postincrement(tcache_bin_info, nhbins, mem,
  6 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c:_rjem_je_cache_bin_postincrement (1x)
  .           	    &cur_offset);
  .           	/* Sanity check that the whole stack is used. */
  .           	assert(cur_offset == tcache_bin_alloc_size);
 11 ( 0.00%)  }
  .           
  .           /* Initialize auto tcache (embedded in TSD). */
  .           bool
 12 ( 0.00%)  tsd_tcache_data_init(tsd_t *tsd) {
  .           	tcache_slow_t *tcache_slow = tsd_tcache_slowp_get_unsafe(tsd);
  .           	tcache_t *tcache = tsd_tcachep_get_unsafe(tsd);
  .           
  .           	assert(cache_bin_still_zero_initialized(&tcache->bins[0]));
  1 ( 0.00%)  	size_t alignment = tcache_bin_alloc_alignment;
  1 ( 0.00%)  	size_t size = sz_sa2u(tcache_bin_alloc_size, alignment);
  .           
  .           	void *mem = ipallocztm(tsd_tsdn(tsd), size, alignment, true, NULL,
  .           	    true, arena_get(TSDN_NULL, 0, true));
  .           	if (mem == NULL) {
  .           		return true;
  .           	}
  .           
  4 ( 0.00%)  	tcache_init(tsd, tcache_slow, tcache, mem);
1,688 ( 0.06%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:tcache_init.isra.0 (1x)
  .           	/*
  .           	 * Initialization is a bit tricky here.  After malloc init is done, all
  .           	 * threads can rely on arena_choose and associate tcache accordingly.
  .           	 * However, the thread that does actual malloc bootstrapping relies on
  .           	 * functional tsd, and it can only rely on a0.  In that case, we
  .           	 * associate its tcache to a0 temporarily, and later on
  .           	 * arena_choose_hard() will re-associate properly.
  .           	 */
  1 ( 0.00%)  	tcache_slow->arena = NULL;
  .           	arena_t *arena;
  3 ( 0.00%)  	if (!malloc_initialized()) {
  .           		/* If in initialization, assign to a0. */
  .           		arena = arena_get(tsd_tsdn(tsd), 0, false);
  5 ( 0.00%)  		tcache_arena_associate(tsd_tsdn(tsd), tcache_slow, tcache,
109 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/tcache.c:_rjem_je_tcache_arena_associate (1x)
  .           		    arena);
  .           	} else {
  .           		arena = arena_choose(tsd, NULL);
  .           		/* This may happen if thread.tcache.enabled is used. */
  .           		if (tcache_slow->arena == NULL) {
  .           			tcache_arena_associate(tsd_tsdn(tsd), tcache_slow,
  .           			    tcache, arena);
  .           		}
  .           	}
  .           	assert(arena == tcache_slow->arena);
  .           
  1 ( 0.00%)  	return false;
 11 ( 0.00%)  }
  .           
  .           /* Created manual tcache for tcache.create mallctl. */
  .           tcache_t *
  .           tcache_create_explicit(tsd_t *tsd) {
  .           	/*
  .           	 * We place the cache bin stacks, then the tcache_t, then a pointer to
  .           	 * the beginning of the whole allocation (for freeing).  The makes sure
  .           	 * the cache bins have the requested alignment.
-- line 758 ----------------------------------------
-- line 984 ----------------------------------------
  .           	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
  .           	if (tcache != NULL) {
  .           		tcache_destroy(tsd, tcache, false);
  .           	}
  .           }
  .           
  .           static unsigned
  .           tcache_ncached_max_compute(szind_t szind) {
 82 ( 0.00%)  	if (szind >= SC_NBINS) {
  .           		assert(szind < nhbins);
 10 ( 0.00%)  		return opt_tcache_nslots_large;
  .           	}
180 ( 0.01%)  	unsigned slab_nregs = bin_infos[szind].nregs;
  .           
  .           	/* We may modify these values; start with the opt versions. */
 36 ( 0.00%)  	unsigned nslots_small_min = opt_tcache_nslots_small_min;
  .           	unsigned nslots_small_max = opt_tcache_nslots_small_max;
  .           
  .           	/*
  .           	 * Clamp values to meet our constraints -- even, nonzero, min < max, and
  .           	 * suitable for a cache bin size.
  .           	 */
144 ( 0.01%)  	if (opt_tcache_nslots_small_max > CACHE_BIN_NCACHED_MAX) {
  .           		nslots_small_max = CACHE_BIN_NCACHED_MAX;
  .           	}
 72 ( 0.00%)  	if (nslots_small_min % 2 != 0) {
 72 ( 0.00%)  		nslots_small_min++;
  .           	}
  .           	if (nslots_small_max % 2 != 0) {
 36 ( 0.00%)  		nslots_small_max--;
  .           	}
 72 ( 0.00%)  	if (nslots_small_min < 2) {
  .           		nslots_small_min = 2;
  .           	}
108 ( 0.00%)  	if (nslots_small_max < 2) {
  .           		nslots_small_max = 2;
  .           	}
 72 ( 0.00%)  	if (nslots_small_min > nslots_small_max) {
  .           		nslots_small_min = nslots_small_max;
  .           	}
  .           
  .           	unsigned candidate;
 36 ( 0.00%)  	if (opt_lg_tcache_nslots_mul < 0) {
288 ( 0.01%)  		candidate = slab_nregs >> (-opt_lg_tcache_nslots_mul);
  .           	} else {
  .           		candidate = slab_nregs << opt_lg_tcache_nslots_mul;
  .           	}
 72 ( 0.00%)  	if (candidate % 2 != 0) {
  .           		/*
  .           		 * We need the candidate size to be even -- we assume that we
  .           		 * can divide by two and get a positive number (e.g. when
  .           		 * flushing).
  .           		 */
 72 ( 0.00%)  		++candidate;
  .           	}
  .           	if (candidate <= nslots_small_min) {
  .           		return nslots_small_min;
144 ( 0.01%)  	} else if (candidate <= nslots_small_max) {
  .           		return candidate;
  .           	} else {
  .           		return nslots_small_max;
  .           	}
  .           }
  .           
  .           bool
  6 ( 0.00%)  tcache_boot(tsdn_t *tsdn, base_t *base) {
  2 ( 0.00%)  	tcache_maxclass = sz_s2u(opt_tcache_max);
  .           	assert(tcache_maxclass <= TCACHE_MAXCLASS_LIMIT);
  2 ( 0.00%)  	nhbins = sz_size2index(tcache_maxclass) + 1;
  .           
  8 ( 0.00%)  	if (malloc_mutex_init(&tcaches_mtx, "tcaches", WITNESS_RANK_TCACHES,
127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
  .           	    malloc_mutex_rank_exclusive)) {
  .           		return true;
  .           	}
  .           
  .           	/* Initialize tcache_bin_info.  See comments in tcache_init(). */
  4 ( 0.00%)  	unsigned n_reserved_bins = nhbins < SC_NBINS ? SC_NBINS : nhbins;
  1 ( 0.00%)  	size_t size = n_reserved_bins * sizeof(cache_bin_info_t);
  6 ( 0.00%)  	tcache_bin_info = (cache_bin_info_t *)base_alloc(tsdn, base, size,
832 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_alloc (1x)
  .           	    CACHELINE);
  2 ( 0.00%)  	if (tcache_bin_info == NULL) {
  .           		return true;
  .           	}
  .           
169 ( 0.01%)  	for (szind_t i = 0; i < nhbins; i++) {
  .           		unsigned ncached_max = tcache_ncached_max_compute(i);
 82 ( 0.00%)  		cache_bin_info_init(&tcache_bin_info[i], ncached_max);
123 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c:_rjem_je_cache_bin_info_init (41x)
  .           	}
  2 ( 0.00%)  	for (szind_t i = nhbins; i < SC_NBINS; i++) {
  .           		/* Disabled small bins. */
  .           		cache_bin_info_init(&tcache_bin_info[i], 0);
  .           		assert(tcache_small_bin_disabled(i, NULL));
  .           	}
  .           
 47 ( 0.00%)  	cache_bin_info_compute_alloc(tcache_bin_info, nhbins,
133 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/cache_bin.c:_rjem_je_cache_bin_info_compute_alloc (1x)
  .           	    &tcache_bin_alloc_size, &tcache_bin_alloc_alignment);
  .           
  .           	return false;
  5 ( 0.00%)  }
  .           
  .           void
  .           tcache_prefork(tsdn_t *tsdn) {
  .           	malloc_mutex_prefork(tsdn, &tcaches_mtx);
  .           }
  .           
  .           void
  .           tcache_postfork_parent(tsdn_t *tsdn) {
-- line 1090 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/edata.h
--------------------------------------------------------------------------------
Ir           

-- line 245 ----------------------------------------
  .           		/* Small region slab metadata. */
  .           		slab_data_t	e_slab_data;
  .           
  .           		/* Profiling data, used for large objects. */
  .           		e_prof_info_t	e_prof_info;
  .           	};
  .           };
  .           
 37 ( 0.00%)  TYPED_LIST(edata_list_active, edata_t, ql_link_active)
415 ( 0.01%)  TYPED_LIST(edata_list_inactive, edata_t, ql_link_inactive)
  .           
  .           static inline unsigned
  .           edata_arena_ind_get(const edata_t *edata) {
  1 ( 0.00%)  	unsigned arena_ind = (unsigned)((edata->e_bits &
  .           	    EDATA_BITS_ARENA_MASK) >> EDATA_BITS_ARENA_SHIFT);
  .           	assert(arena_ind < MALLOCX_ARENA_LIMIT);
  .           
  .           	return arena_ind;
  .           }
  .           
  .           static inline szind_t
  .           edata_szind_get_maybe_invalid(const edata_t *edata) {
 29 ( 0.00%)  	szind_t szind = (szind_t)((edata->e_bits & EDATA_BITS_SZIND_MASK) >>
  .           	    EDATA_BITS_SZIND_SHIFT);
  .           	assert(szind <= SC_NSIZES);
  .           	return szind;
  .           }
  .           
  .           static inline szind_t
  .           edata_szind_get(const edata_t *edata) {
  .           	szind_t szind = edata_szind_get_maybe_invalid(edata);
-- line 275 ----------------------------------------
-- line 287 ----------------------------------------
  .           	unsigned binshard = (unsigned)((edata->e_bits &
  .           	    EDATA_BITS_BINSHARD_MASK) >> EDATA_BITS_BINSHARD_SHIFT);
  .           	assert(binshard < bin_infos[edata_szind_get(edata)].n_shards);
  .           	return binshard;
  .           }
  .           
  .           static inline uint64_t
  .           edata_sn_get(const edata_t *edata) {
161 ( 0.01%)  	return edata->e_sn;
  .           }
  .           
  .           static inline extent_state_t
  .           edata_state_get(const edata_t *edata) {
 96 ( 0.00%)  	return (extent_state_t)((edata->e_bits & EDATA_BITS_STATE_MASK) >>
  .           	    EDATA_BITS_STATE_SHIFT);
  .           }
  .           
  .           static inline bool
  .           edata_guarded_get(const edata_t *edata) {
  .           	return (bool)((edata->e_bits & EDATA_BITS_GUARDED_MASK) >>
  .           	    EDATA_BITS_GUARDED_SHIFT);
  .           }
-- line 308 ----------------------------------------
-- line 310 ----------------------------------------
  .           static inline bool
  .           edata_zeroed_get(const edata_t *edata) {
  .           	return (bool)((edata->e_bits & EDATA_BITS_ZEROED_MASK) >>
  .           	    EDATA_BITS_ZEROED_SHIFT);
  .           }
  .           
  .           static inline bool
  .           edata_committed_get(const edata_t *edata) {
 65 ( 0.00%)  	return (bool)((edata->e_bits & EDATA_BITS_COMMITTED_MASK) >>
  .           	    EDATA_BITS_COMMITTED_SHIFT);
  .           }
  .           
  .           static inline extent_pai_t
  .           edata_pai_get(const edata_t *edata) {
  .           	return (extent_pai_t)((edata->e_bits & EDATA_BITS_PAI_MASK) >>
  .           	    EDATA_BITS_PAI_SHIFT);
  .           }
-- line 326 ----------------------------------------
-- line 329 ----------------------------------------
  .           edata_slab_get(const edata_t *edata) {
  .           	return (bool)((edata->e_bits & EDATA_BITS_SLAB_MASK) >>
  .           	    EDATA_BITS_SLAB_SHIFT);
  .           }
  .           
  .           static inline unsigned
  .           edata_nfree_get(const edata_t *edata) {
  .           	assert(edata_slab_get(edata));
120 ( 0.00%)  	return (unsigned)((edata->e_bits & EDATA_BITS_NFREE_MASK) >>
  .           	    EDATA_BITS_NFREE_SHIFT);
  .           }
  .           
  .           static inline void *
  .           edata_base_get(const edata_t *edata) {
  .           	assert(edata->e_addr == PAGE_ADDR2BASE(edata->e_addr) ||
  .           	    !edata_slab_get(edata));
838 ( 0.03%)  	return PAGE_ADDR2BASE(edata->e_addr);
  .           }
  .           
  .           static inline void *
  .           edata_addr_get(const edata_t *edata) {
  .           	assert(edata->e_addr == PAGE_ADDR2BASE(edata->e_addr) ||
  .           	    !edata_slab_get(edata));
 63 ( 0.00%)  	return edata->e_addr;
  .           }
  .           
  .           static inline size_t
  .           edata_size_get(const edata_t *edata) {
556 ( 0.02%)  	return (edata->e_size_esn & EDATA_SIZE_MASK);
  .           }
  .           
  .           static inline size_t
  .           edata_esn_get(const edata_t *edata) {
  .           	return (edata->e_size_esn & EDATA_ESN_MASK);
  .           }
  .           
  .           static inline size_t
  .           edata_bsize_get(const edata_t *edata) {
 38 ( 0.00%)  	return edata->e_bsize;
  .           }
  .           
  .           static inline hpdata_t *
  .           edata_ps_get(const edata_t *edata) {
  .           	assert(edata_pai_get(edata) == EXTENT_PAI_HPA);
  .           	return edata->e_ps;
  .           }
  .           
-- line 375 ----------------------------------------
-- line 376 ----------------------------------------
  .           static inline void *
  .           edata_before_get(const edata_t *edata) {
  .           	return (void *)((uintptr_t)edata_base_get(edata) - PAGE);
  .           }
  .           
  .           static inline void *
  .           edata_last_get(const edata_t *edata) {
  .           	return (void *)((uintptr_t)edata_base_get(edata) +
128 ( 0.00%)  	    edata_size_get(edata) - PAGE);
  .           }
  .           
  .           static inline void *
  .           edata_past_get(const edata_t *edata) {
  1 ( 0.00%)  	return (void *)((uintptr_t)edata_base_get(edata) +
  .           	    edata_size_get(edata));
  .           }
  .           
  .           static inline slab_data_t *
  .           edata_slab_data_get(edata_t *edata) {
  .           	assert(edata_slab_get(edata));
  .           	return &edata->e_slab_data;
  .           }
-- line 397 ----------------------------------------
-- line 421 ----------------------------------------
  .           static inline prof_recent_t *
  .           edata_prof_recent_alloc_get_dont_call_directly(const edata_t *edata) {
  .           	return (prof_recent_t *)atomic_load_p(
  .           	    &edata->e_prof_info.e_prof_recent_alloc, ATOMIC_RELAXED);
  .           }
  .           
  .           static inline void
  .           edata_arena_ind_set(edata_t *edata, unsigned arena_ind) {
  3 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_ARENA_MASK) |
  1 ( 0.00%)  	    ((uint64_t)arena_ind << EDATA_BITS_ARENA_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_binshard_set(edata_t *edata, unsigned binshard) {
  .           	/* The assertion assumes szind is set already. */
  .           	assert(binshard < bin_infos[edata_szind_get(edata)].n_shards);
  .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_BINSHARD_MASK) |
  .           	    ((uint64_t)binshard << EDATA_BITS_BINSHARD_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_addr_set(edata_t *edata, void *addr) {
  1 ( 0.00%)  	edata->e_addr = addr;
  .           }
  .           
  .           static inline void
  .           edata_size_set(edata_t *edata, size_t size) {
  .           	assert((size & ~EDATA_SIZE_MASK) == 0);
292 ( 0.01%)  	edata->e_size_esn = size | (edata->e_size_esn & ~EDATA_SIZE_MASK);
  .           }
  .           
  .           static inline void
  .           edata_esn_set(edata_t *edata, size_t esn) {
198 ( 0.01%)  	edata->e_size_esn = (edata->e_size_esn & ~EDATA_ESN_MASK) | (esn &
  .           	    EDATA_ESN_MASK);
  .           }
  .           
  .           static inline void
  .           edata_bsize_set(edata_t *edata, size_t bsize) {
 38 ( 0.00%)  	edata->e_bsize = bsize;
  .           }
  .           
  .           static inline void
  .           edata_ps_set(edata_t *edata, hpdata_t *ps) {
  .           	assert(edata_pai_get(edata) == EXTENT_PAI_HPA);
  .           	edata->e_ps = ps;
  .           }
  .           
  .           static inline void
  .           edata_szind_set(edata_t *edata, szind_t szind) {
  .           	assert(szind <= SC_NSIZES); /* SC_NSIZES means "invalid". */
 96 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_SZIND_MASK) |
 64 ( 0.00%)  	    ((uint64_t)szind << EDATA_BITS_SZIND_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_nfree_set(edata_t *edata, unsigned nfree) {
  .           	assert(edata_slab_get(edata));
  .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_NFREE_MASK) |
  .           	    ((uint64_t)nfree << EDATA_BITS_NFREE_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_nfree_binshard_set(edata_t *edata, unsigned nfree, unsigned binshard) {
  .           	/* The assertion assumes szind is set already. */
  .           	assert(binshard < bin_infos[edata_szind_get(edata)].n_shards);
 62 ( 0.00%)  	edata->e_bits = (edata->e_bits &
  .           	    (~EDATA_BITS_NFREE_MASK & ~EDATA_BITS_BINSHARD_MASK)) |
124 ( 0.00%)  	    ((uint64_t)binshard << EDATA_BITS_BINSHARD_SHIFT) |
 62 ( 0.00%)  	    ((uint64_t)nfree << EDATA_BITS_NFREE_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_nfree_inc(edata_t *edata) {
  .           	assert(edata_slab_get(edata));
  .           	edata->e_bits += ((uint64_t)1U << EDATA_BITS_NFREE_SHIFT);
  .           }
  .           
-- line 498 ----------------------------------------
-- line 500 ----------------------------------------
  .           edata_nfree_dec(edata_t *edata) {
  .           	assert(edata_slab_get(edata));
  .           	edata->e_bits -= ((uint64_t)1U << EDATA_BITS_NFREE_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_nfree_sub(edata_t *edata, uint64_t n) {
  .           	assert(edata_slab_get(edata));
 93 ( 0.00%)  	edata->e_bits -= (n << EDATA_BITS_NFREE_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_sn_set(edata_t *edata, uint64_t sn) {
 35 ( 0.00%)  	edata->e_sn = sn;
  .           }
  .           
  .           static inline void
  .           edata_state_set(edata_t *edata, extent_state_t state) {
378 ( 0.01%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_STATE_MASK) |
 63 ( 0.00%)  	    ((uint64_t)state << EDATA_BITS_STATE_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_guarded_set(edata_t *edata, bool guarded) {
  .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_GUARDED_MASK) |
  .           	    ((uint64_t)guarded << EDATA_BITS_GUARDED_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_zeroed_set(edata_t *edata, bool zeroed) {
  .           	edata->e_bits = (edata->e_bits & ~EDATA_BITS_ZEROED_MASK) |
 65 ( 0.00%)  	    ((uint64_t)zeroed << EDATA_BITS_ZEROED_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_committed_set(edata_t *edata, bool committed) {
160 ( 0.01%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_COMMITTED_MASK) |
 33 ( 0.00%)  	    ((uint64_t)committed << EDATA_BITS_COMMITTED_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_pai_set(edata_t *edata, extent_pai_t pai) {
160 ( 0.01%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_PAI_MASK) |
  .           	    ((uint64_t)pai << EDATA_BITS_PAI_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_slab_set(edata_t *edata, bool slab) {
 96 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_SLAB_MASK) |
 64 ( 0.00%)  	    ((uint64_t)slab << EDATA_BITS_SLAB_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_prof_tctx_set(edata_t *edata, prof_tctx_t *tctx) {
  .           	atomic_store_p(&edata->e_prof_info.e_prof_tctx, tctx, ATOMIC_RELEASE);
  .           }
  .           
  .           static inline void
-- line 557 ----------------------------------------
-- line 568 ----------------------------------------
  .           edata_prof_recent_alloc_set_dont_call_directly(edata_t *edata,
  .               prof_recent_t *recent_alloc) {
  .           	atomic_store_p(&edata->e_prof_info.e_prof_recent_alloc, recent_alloc,
  .           	    ATOMIC_RELAXED);
  .           }
  .           
  .           static inline bool
  .           edata_is_head_get(edata_t *edata) {
194 ( 0.01%)  	return (bool)((edata->e_bits & EDATA_BITS_IS_HEAD_MASK) >>
  .           	    EDATA_BITS_IS_HEAD_SHIFT);
  .           }
  .           
  .           static inline void
  .           edata_is_head_set(edata_t *edata, bool is_head) {
104 ( 0.00%)  	edata->e_bits = (edata->e_bits & ~EDATA_BITS_IS_HEAD_MASK) |
  .           	    ((uint64_t)is_head << EDATA_BITS_IS_HEAD_SHIFT);
  .           }
  .           
  .           static inline bool
  .           edata_state_in_transition(extent_state_t state) {
  .           	return state >= extent_state_transition;
  .           }
  .           
-- line 590 ----------------------------------------
-- line 651 ----------------------------------------
  .           	uintptr_t a_eaddr = (uintptr_t)a;
  .           	uintptr_t b_eaddr = (uintptr_t)b;
  .           
  .           	return (a_eaddr > b_eaddr) - (a_eaddr < b_eaddr);
  .           }
  .           
  .           static inline edata_cmp_summary_t
  .           edata_cmp_summary_get(const edata_t *edata) {
 34 ( 0.00%)  	return (edata_cmp_summary_t){edata_sn_get(edata),
128 ( 0.00%)  		(uintptr_t)edata_addr_get(edata)};
  .           }
  .           
  .           static inline int
  .           edata_cmp_summary_comp(edata_cmp_summary_t a, edata_cmp_summary_t b) {
  .           	int ret;
211 ( 0.01%)  	ret = (a.sn > b.sn) - (a.sn < b.sn);
 70 ( 0.00%)  	if (ret != 0) {
  .           		return ret;
  .           	}
  .           	ret = (a.addr > b.addr) - (a.addr < b.addr);
  .           	return ret;
  .           }
  .           
  .           static inline int
  .           edata_snad_comp(const edata_t *a, const edata_t *b) {
-- line 675 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/jemalloc_internal_inlines_c.h
--------------------------------------------------------------------------------
Ir             

-- line 72 ----------------------------------------
    .           
    .           	assert(usize != 0);
    .           	assert(usize == sz_sa2u(usize, alignment));
    .           	assert(!is_internal || tcache == NULL);
    .           	assert(!is_internal || arena == NULL || arena_is_auto(arena));
    .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
    .           	    WITNESS_RANK_CORE, 0);
    .           
    7 ( 0.00%)  	ret = arena_palloc(tsdn, arena, usize, alignment, zero, tcache);
6,718 ( 0.24%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_palloc (1x)
    .           	assert(ALIGNMENT_ADDR2BASE(ret, alignment) == ret);
    2 ( 0.00%)  	if (config_stats && is_internal && likely(ret != NULL)) {
    .           		arena_internal_add(iaalloc(tsdn, ret), isalloc(tsdn, ret));
    .           	}
    .           	return ret;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           ipalloct(tsdn_t *tsdn, size_t usize, size_t alignment, bool zero,
    .               tcache_t *tcache, arena_t *arena) {
-- line 90 ----------------------------------------
-- line 186 ----------------------------------------
    .           		/*
    .           		 * Existing object alignment is inadequate; allocate new space
    .           		 * and copy.
    .           		 */
    .           		return iralloct_realign(tsdn, ptr, oldsize, size, alignment,
    .           		    zero, tcache, arena, hook_args);
    .           	}
    .           
  435 ( 0.02%)  	return arena_ralloc(tsdn, arena, ptr, oldsize, size, alignment, zero,
43,555 ( 1.54%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/arena.c:_rjem_je_arena_ralloc (29x)
    .           	    tcache, hook_args);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           iralloc(tsd_t *tsd, void *ptr, size_t oldsize, size_t size, size_t alignment,
    .               bool zero, hook_ralloc_args_t *hook_args) {
    .           	return iralloct(tsd_tsdn(tsd), ptr, oldsize, size, alignment, zero,
    .           	    tcache_get(tsd), NULL, hook_args);
-- line 202 ----------------------------------------
-- line 221 ----------------------------------------
    .           	    newsize);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           fastpath_success_finish(tsd_t *tsd, uint64_t allocated_after,
    .               cache_bin_t *bin, void *ret) {
    .           	thread_allocated_set(tsd, allocated_after);
    .           	if (config_stats) {
  588 ( 0.02%)  		bin->tstats.nrequests++;
    .           	}
    .           
    .           	LOG("core.malloc.exit", "result: %p", ret);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           malloc_initialized(void) {
    2 ( 0.00%)  	return (malloc_init_state == malloc_init_initialized);
    .           }
    .           
    .           /*
    .            * malloc() fastpath.  Included here so that we can inline it into operator new;
    .            * function call overhead there is non-negligible as a fraction of total CPU in
    .            * allocation-heavy C++ programs.  We take the fallback alloc to allow malloc
    .            * (which can return NULL) to differ in its behavior from operator new (which
    .            * can't).  It matches the signature of malloc / operator new so that we can
-- line 245 ----------------------------------------
-- line 257 ----------------------------------------
    .           JEMALLOC_ALWAYS_INLINE void *
    .           imalloc_fastpath(size_t size, void *(fallback_alloc)(size_t)) {
    .           	LOG("core.malloc.entry", "size: %zu", size);
    .           	if (tsd_get_allocates() && unlikely(!malloc_initialized())) {
    .           		return fallback_alloc(size);
    .           	}
    .           
    .           	tsd_t *tsd = tsd_get(false);
1,204 ( 0.04%)  	if (unlikely((size > SC_LOOKUP_MAXCLASS) || tsd == NULL)) {
   14 ( 0.00%)  		return fallback_alloc(size);
84,454 ( 2.99%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/jemalloc.c:_rjem_je_malloc_default (14x)
    .           	}
    .           	/*
    .           	 * The code below till the branch checking the next_event threshold may
    .           	 * execute before malloc_init(), in which case the threshold is 0 to
    .           	 * trigger slow path and initialization.
    .           	 *
    .           	 * Note that when uninitialized, only the fast-path variants of the sz /
    .           	 * tsd facilities may be called.
-- line 274 ----------------------------------------
-- line 284 ----------------------------------------
    .           	sz_size2index_usize_fastpath(size, &ind, &usize);
    .           	/* Fast path relies on size being a bin. */
    .           	assert(ind < SC_NBINS);
    .           	assert((SC_LOOKUP_MAXCLASS < SC_SMALL_MAXCLASS) &&
    .           	    (size <= SC_SMALL_MAXCLASS));
    .           
    .           	uint64_t allocated, threshold;
    .           	te_malloc_fastpath_ctx(tsd, &allocated, &threshold);
1,803 ( 0.06%)  	uint64_t allocated_after = allocated + usize;
    .           	/*
    .           	 * The ind and usize might be uninitialized (or partially) before
    .           	 * malloc_init().  The assertions check for: 1) full correctness (usize
    .           	 * & ind) when initialized; and 2) guaranteed slow-path (threshold == 0)
    .           	 * when !initialized.
    .           	 */
    .           	if (!malloc_initialized()) {
    .           		assert(threshold == 0);
-- line 300 ----------------------------------------
-- line 301 ----------------------------------------
    .           	} else {
    .           		assert(ind == sz_size2index(size));
    .           		assert(usize > 0 && usize == sz_index2size(ind));
    .           	}
    .           	/*
    .           	 * Check for events and tsd non-nominal (fast_threshold will be set to
    .           	 * 0) in a single branch.
    .           	 */
1,202 ( 0.04%)  	if (unlikely(allocated_after >= threshold)) {
    .           		return fallback_alloc(size);
    .           	}
    .           	assert(tsd_fast(tsd));
    .           
    .           	tcache_t *tcache = tsd_tcachep_get(tsd);
    .           	assert(tcache == tcache_get(tsd));
    .           	cache_bin_t *bin = &tcache->bins[ind];
    .           	bool tcache_success;
-- line 317 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sc.c
--------------------------------------------------------------------------------
Ir             

-- line 10 ----------------------------------------
    .            * This module computes the size classes used to satisfy allocations.  The logic
    .            * here was ported more or less line-by-line from a shell script, and because of
    .            * that is not the most idiomatic C.  Eventually we should fix this, but for now
    .            * at least the damage is compartmentalized to this file.
    .            */
    .           
    .           size_t
    .           reg_size_compute(int lg_base, int lg_delta, int ndelta) {
2,328 ( 0.08%)  	return (ZU(1) << lg_base) + (ZU(ndelta) << lg_delta);
    .           }
    .           
    .           /* Returns the number of pages in the slab. */
    .           static int
    .           slab_size(int lg_page, int lg_base, int lg_delta, int ndelta) {
    .           	size_t page = (ZU(1) << lg_page);
    .           	size_t reg_size = reg_size_compute(lg_base, lg_delta, ndelta);
    .           
   34 ( 0.00%)  	size_t try_slab_size = page;
  103 ( 0.00%)  	size_t try_nregs = try_slab_size / reg_size;
    .           	size_t perfect_slab_size = 0;
    .           	bool perfect = false;
    .           	/*
    .           	 * This loop continues until we find the least common multiple of the
    .           	 * page size and size class size.  Size classes are all of the form
    .           	 * base + ndelta * delta == (ndelta + base/ndelta) * delta, which is
    .           	 * (ndelta + ngroup) * delta.  The way we choose slabbing strategies
    .           	 * means that delta is at most the page size and ndelta < ngroup.  So
    .           	 * the loop executes for at most 2 * ngroup - 1 iterations, which is
    .           	 * also the bound on the number of pages in a slab chosen by default.
    .           	 * With the current default settings, this is at most 7.
    .           	 */
  297 ( 0.01%)  	while (!perfect) {
    .           		perfect_slab_size = try_slab_size;
    .           		size_t perfect_nregs = try_nregs;
  399 ( 0.01%)  		try_slab_size += page;
  399 ( 0.01%)  		try_nregs = try_slab_size / reg_size;
  133 ( 0.00%)  		if (perfect_slab_size == perfect_nregs * reg_size) {
    .           			perfect = true;
    .           		}
    .           	}
   99 ( 0.00%)  	return (int)(perfect_slab_size / page);
    .           }
    .           
    .           static void
    .           size_class(
    .               /* Output. */
    .               sc_t *sc,
    .               /* Configuration decisions. */
    .               int lg_max_lookup, int lg_page, int lg_ngroup,
    .               /* Inputs specific to the size class. */
    .               int index, int lg_base, int lg_delta, int ndelta) {
  235 ( 0.01%)  	sc->index = index;
    6 ( 0.00%)  	sc->lg_base = lg_base;
    6 ( 0.00%)  	sc->lg_delta = lg_delta;
    6 ( 0.00%)  	sc->ndelta = ndelta;
    .           	size_t size = reg_size_compute(lg_base, lg_delta, ndelta);
1,141 ( 0.04%)  	sc->psz = (size % (ZU(1) << lg_page) == 0);
    .           	if (index == 0) {
    .           		assert(!sc->psz);
    .           	}
  454 ( 0.02%)  	if (size < (ZU(1) << (lg_page + lg_ngroup))) {
  227 ( 0.01%)  		sc->bin = true;
  459 ( 0.02%)  		sc->pgs = slab_size(lg_page, lg_base, lg_delta, ndelta);
    .           	} else {
    .           		sc->bin = false;
    .           		sc->pgs = 0;
    .           	}
   62 ( 0.00%)  	if (size <= (ZU(1) << lg_max_lookup)) {
    3 ( 0.00%)  		sc->lg_delta_lookup = lg_delta;
    .           	} else {
    .           		sc->lg_delta_lookup = 0;
    .           	}
    .           }
    .           
    .           static void
    .           size_classes(
    .               /* Output. */
-- line 86 ----------------------------------------
-- line 99 ----------------------------------------
    .           
    .           	int index = 0;
    .           
    .           	int ndelta = 0;
    .           	int lg_base = lg_tiny_min;
    .           	int lg_delta = lg_base;
    .           
    .           	/* Outputs that we update as we go. */
    5 ( 0.00%)  	size_t lookup_maxclass = 0;
    1 ( 0.00%)  	size_t small_maxclass = 0;
    3 ( 0.00%)  	int lg_large_minclass = 0;
    .           	size_t large_maxclass = 0;
    .           
    .           	/* Tiny size classes. */
    .           	while (lg_base < lg_quantum) {
    .           		sc_t *sc = &sc_data->sc[index];
    .           		size_class(sc, lg_max_lookup, lg_page, lg_ngroup, index,
    .           		    lg_base, lg_delta, ndelta);
    .           		if (sc->lg_delta_lookup != 0) {
-- line 117 ----------------------------------------
-- line 147 ----------------------------------------
    .           		lg_delta++;
    .           		if (sc->psz) {
    .           			npsizes++;
    .           		}
    .           		if (sc->bin) {
    .           			nbins++;
    .           		}
    .           	}
   13 ( 0.00%)  	while (ndelta < ngroup) {
    .           		sc_t *sc = &sc_data->sc[index];
    .           		size_class(sc, lg_max_lookup, lg_page, lg_ngroup, index,
    .           		    lg_base, lg_delta, ndelta);
    .           		index++;
    .           		ndelta++;
    .           		if (sc->psz) {
    .           			npsizes++;
    .           		}
    .           		if (sc->bin) {
    .           			nbins++;
    .           		}
    .           	}
    .           
    .           	/* All remaining groups. */
    1 ( 0.00%)  	lg_base = lg_base + lg_ngroup;
  618 ( 0.02%)  	while (lg_base < ptr_bits - 1) {
    .           		ndelta = 1;
    .           		int ndelta_limit;
    .           		if (lg_base == ptr_bits - 2) {
    .           			ndelta_limit = ngroup - 1;
    .           		} else {
  456 ( 0.02%)  			ndelta_limit = ngroup;
    .           		}
1,079 ( 0.04%)  		while (ndelta <= ndelta_limit) {
    .           			sc_t *sc = &sc_data->sc[index];
    .           			size_class(sc, lg_max_lookup, lg_page, lg_ngroup, index,
    .           			    lg_base, lg_delta, ndelta);
    .           			if (sc->lg_delta_lookup != 0) {
  251 ( 0.01%)  				nlbins = index + 1;
    .           				/* Final written value is correct. */
   24 ( 0.00%)  				lookup_maxclass = (ZU(1) << lg_base)
    .           				    + (ZU(ndelta) << lg_delta);
    .           			}
   48 ( 0.00%)  			if (sc->psz) {
  407 ( 0.01%)  				npsizes++;
    .           			}
  414 ( 0.01%)  			if (sc->bin) {
   31 ( 0.00%)  				nbins++;
    .           				/* Final written value is correct. */
   31 ( 0.00%)  				small_maxclass = (ZU(1) << lg_base)
    .           				    + (ZU(ndelta) << lg_delta);
    .           				if (lg_ngroup > 0) {
  487 ( 0.02%)  					lg_large_minclass = lg_base + 1;
    .           				} else {
    .           					lg_large_minclass = lg_base + 2;
    .           				}
    .           			}
    .           			large_maxclass = (ZU(1) << lg_base)
    .           			    + (ZU(ndelta) << lg_delta);
    .           			index++;
    .           			ndelta++;
    .           		}
    .           		lg_base++;
    .           		lg_delta++;
    .           	}
    .           	/* Additional outputs. */
    .           	int nsizes = index;
    3 ( 0.00%)  	unsigned lg_ceil_nsizes = lg_ceil(nsizes);
    .           
    .           	/* Fill in the output data. */
    1 ( 0.00%)  	sc_data->ntiny = ntiny;
    8 ( 0.00%)  	sc_data->nlbins = nlbins;
    .           	sc_data->nbins = nbins;
    .           	sc_data->nsizes = nsizes;
    .           	sc_data->lg_ceil_nsizes = lg_ceil_nsizes;
    1 ( 0.00%)  	sc_data->npsizes = npsizes;
    1 ( 0.00%)  	sc_data->lg_tiny_maxclass = lg_tiny_maxclass;
    2 ( 0.00%)  	sc_data->lookup_maxclass = lookup_maxclass;
    2 ( 0.00%)  	sc_data->small_maxclass = small_maxclass;
    2 ( 0.00%)  	sc_data->lg_large_minclass = lg_large_minclass;
    3 ( 0.00%)  	sc_data->large_minclass = (ZU(1) << lg_large_minclass);
    1 ( 0.00%)  	sc_data->large_maxclass = large_maxclass;
    .           
    .           	/*
    .           	 * We compute these values in two ways:
    .           	 *   - Incrementally, as above.
    .           	 *   - In macros, in sc.h.
    .           	 * The computation is easier when done incrementally, but putting it in
    .           	 * a constant makes it available to the fast paths without having to
    .           	 * touch the extra global cacheline.  We assert, however, that the two
-- line 235 ----------------------------------------
-- line 248 ----------------------------------------
    .           	 * a ssize_t, and detect passing through 0 correctly.  This
    .           	 * results in optimal generated code.  For this to work, the
    .           	 * maximum allocation size must be less than SSIZE_MAX.
    .           	 */
    .           	assert(SC_LARGE_MAXCLASS < SSIZE_MAX);
    .           }
    .           
    .           void
    8 ( 0.00%)  sc_data_init(sc_data_t *sc_data) {
    .           	size_classes(sc_data, LG_SIZEOF_PTR, LG_QUANTUM, SC_LG_TINY_MIN,
    .           	    SC_LG_MAX_LOOKUP, LG_PAGE, SC_LG_NGROUP);
    .           
    1 ( 0.00%)  	sc_data->initialized = true;
    7 ( 0.00%)  }
    .           
    .           static void
    .           sc_data_update_sc_slab_size(sc_t *sc, size_t reg_size, size_t pgs_guess) {
    .           	size_t min_pgs = reg_size / PAGE;
    .           	if (reg_size % PAGE != 0) {
    .           		min_pgs++;
    .           	}
    .           	/*
-- line 269 ----------------------------------------
-- line 296 ----------------------------------------
    .           		    sc->ndelta);
    .           		if (begin <= reg_size && reg_size <= end) {
    .           			sc_data_update_sc_slab_size(sc, reg_size, pgs);
    .           		}
    .           	}
    .           }
    .           
    .           void
    2 ( 0.00%)  sc_boot(sc_data_t *data) {
10,305 ( 0.36%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/sc.c:_rjem_je_sc_data_init (1x)
    .           	sc_data_init(data);
    .           }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c
--------------------------------------------------------------------------------
Ir             

-- line 27 ----------------------------------------
    .           	"auto",
    .           	"always"
    .           };
    .           
    .           /******************************************************************************/
    .           
    .           static inline bool
    .           metadata_thp_madvise(void) {
   82 ( 0.00%)  	return (metadata_thp_enabled() &&
    .           	    (init_system_thp_mode == thp_mode_default));
    .           }
    .           
    .           static void *
    .           base_map(tsdn_t *tsdn, ehooks_t *ehooks, unsigned ind, size_t size) {
    .           	void *addr;
    2 ( 0.00%)  	bool zero = true;
    2 ( 0.00%)  	bool commit = true;
    .           
    .           	/* Use huge page sizes and alignment regardless of opt_metadata_thp. */
    .           	assert(size == HUGEPAGE_CEILING(size));
    .           	size_t alignment = HUGEPAGE;
    6 ( 0.00%)  	if (ehooks_are_default(ehooks)) {
   14 ( 0.00%)  		addr = extent_alloc_mmap(NULL, size, alignment, &zero, &commit);
  278 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/extent_mmap.c:_rjem_je_extent_alloc_mmap (2x)
    4 ( 0.00%)  		if (have_madvise_huge && addr) {
    6 ( 0.00%)  			pages_set_thp_state(addr, size);
   10 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pages.c:_rjem_je_pages_set_thp_state (2x)
    .           		}
    .           	} else {
    .           		addr = ehooks_alloc(tsdn, ehooks, NULL, size, alignment, &zero,
    .           		    &commit);
    .           	}
    .           
    .           	return addr;
    .           }
-- line 59 ----------------------------------------
-- line 110 ----------------------------------------
    .           	}
    .           }
    .           
    .           static void
    .           base_edata_init(size_t *extent_sn_next, edata_t *edata, void *addr,
    .               size_t size) {
    .           	size_t sn;
    .           
    2 ( 0.00%)  	sn = *extent_sn_next;
    4 ( 0.00%)  	(*extent_sn_next)++;
    .           
    .           	edata_binit(edata, addr, size, sn);
    .           }
    .           
    .           static size_t
    .           base_get_num_blocks(base_t *base, bool with_new_block) {
    .           	base_block_t *b = base->blocks;
    .           	assert(b != NULL);
-- line 127 ----------------------------------------
-- line 174 ----------------------------------------
    .           static void *
    .           base_extent_bump_alloc_helper(edata_t *edata, size_t *gap_size, size_t size,
    .               size_t alignment) {
    .           	void *ret;
    .           
    .           	assert(alignment == ALIGNMENT_CEILING(alignment, QUANTUM));
    .           	assert(size == ALIGNMENT_CEILING(size, alignment));
    .           
   77 ( 0.00%)  	*gap_size = ALIGNMENT_CEILING((uintptr_t)edata_addr_get(edata),
   77 ( 0.00%)  	    alignment) - (uintptr_t)edata_addr_get(edata);
   38 ( 0.00%)  	ret = (void *)((uintptr_t)edata_addr_get(edata) + *gap_size);
    .           	assert(edata_bsize_get(edata) >= *gap_size + size);
  151 ( 0.01%)  	edata_binit(edata, (void *)((uintptr_t)edata_addr_get(edata) +
   76 ( 0.00%)  	    *gap_size + size), edata_bsize_get(edata) - *gap_size - size,
    .           	    edata_sn_get(edata));
    .           	return ret;
    .           }
    .           
    .           static void
    .           base_extent_bump_alloc_post(base_t *base, edata_t *edata, size_t gap_size,
  342 ( 0.01%)      void *addr, size_t size) {
   76 ( 0.00%)  	if (edata_bsize_get(edata) > 0) {
    .           		/*
    .           		 * Compute the index for the largest size class that does not
    .           		 * exceed extent's size.
    .           		 */
    .           		szind_t index_floor =
   38 ( 0.00%)  		    sz_size2index(edata_bsize_get(edata) + 1) - 1;
   76 ( 0.00%)  		edata_heap_insert(&base->avail[index_floor], edata);
1,411 ( 0.05%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_insert (38x)
    .           	}
    .           
    .           	if (config_stats) {
   38 ( 0.00%)  		base->allocated += size;
    .           		/*
    .           		 * Add one PAGE to base_resident for every page boundary that is
    .           		 * crossed by the new allocation. Adjust n_thp similarly when
    .           		 * metadata_thp is enabled.
    .           		 */
  190 ( 0.01%)  		base->resident += PAGE_CEILING((uintptr_t)addr + size) -
  114 ( 0.00%)  		    PAGE_CEILING((uintptr_t)addr - gap_size);
    .           		assert(base->allocated <= base->resident);
    .           		assert(base->resident <= base->mapped);
    .           		if (metadata_thp_madvise() && (opt_metadata_thp ==
    .           		    metadata_thp_always || base->auto_thp_switched)) {
    .           			base->n_thp += (HUGEPAGE_CEILING((uintptr_t)addr + size)
    .           			    - HUGEPAGE_CEILING((uintptr_t)addr - gap_size)) >>
    .           			    LG_HUGEPAGE;
    .           			assert(base->mapped >= base->n_thp << LG_HUGEPAGE);
    .           		}
    .           	}
  228 ( 0.01%)  }
    .           
    .           static void *
    .           base_extent_bump_alloc(base_t *base, edata_t *edata, size_t size,
    .               size_t alignment) {
    .           	void *ret;
    .           	size_t gap_size;
    .           
    .           	ret = base_extent_bump_alloc_helper(edata, &gap_size, size, alignment);
  222 ( 0.01%)  	base_extent_bump_alloc_post(base, edata, gap_size, ret, size);
3,455 ( 0.12%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_extent_bump_alloc_post (37x)
    .           	return ret;
    .           }
    .           
    .           /*
    .            * Allocate a block of virtual memory that is large enough to start with a
    .            * base_block_t header, followed by an object of specified size and alignment.
    .            * On success a pointer to the initialized base_block_t header is returned.
    .            */
    .           static base_block_t *
   24 ( 0.00%)  base_block_alloc(tsdn_t *tsdn, base_t *base, ehooks_t *ehooks, unsigned ind,
    .               pszind_t *pind_last, size_t *extent_sn_next, size_t size,
    .               size_t alignment) {
   10 ( 0.00%)  	alignment = ALIGNMENT_CEILING(alignment, QUANTUM);
    8 ( 0.00%)  	size_t usize = ALIGNMENT_CEILING(size, alignment);
    .           	size_t header_size = sizeof(base_block_t);
    4 ( 0.00%)  	size_t gap_size = ALIGNMENT_CEILING(header_size, alignment) -
    .           	    header_size;
    .           	/*
    .           	 * Create increasingly larger blocks in order to limit the total number
    .           	 * of disjoint virtual memory ranges.  Choose the next size in the page
    .           	 * size class series (skipping size classes that are not a multiple of
    .           	 * HUGEPAGE), or a size large enough to satisfy the requested size and
    .           	 * alignment, whichever is larger.
    .           	 */
    6 ( 0.00%)  	size_t min_block_size = HUGEPAGE_CEILING(sz_psz2u(header_size + gap_size
    .           	    + usize));
    4 ( 0.00%)  	pszind_t pind_next = (*pind_last + 1 < sz_psz2ind(SC_LARGE_MAXCLASS)) ?
    4 ( 0.00%)  	    *pind_last + 1 : *pind_last;
    8 ( 0.00%)  	size_t next_block_size = HUGEPAGE_CEILING(sz_pind2sz(pind_next));
    4 ( 0.00%)  	size_t block_size = (min_block_size > next_block_size) ? min_block_size
    .           	    : next_block_size;
    .           	base_block_t *block = (base_block_t *)base_map(tsdn, ehooks, ind,
    .           	    block_size);
    2 ( 0.00%)  	if (block == NULL) {
    .           		return NULL;
    .           	}
    .           
    .           	if (metadata_thp_madvise()) {
    .           		void *addr = (void *)block;
    .           		assert(((uintptr_t)addr & HUGEPAGE_MASK) == 0 &&
    .           		    (block_size & HUGEPAGE_MASK) == 0);
    .           		if (opt_metadata_thp == metadata_thp_always) {
-- line 275 ----------------------------------------
-- line 281 ----------------------------------------
    .           			base_auto_thp_switch(tsdn, base);
    .           			if (base->auto_thp_switched) {
    .           				pages_huge(addr, block_size);
    .           			}
    .           			malloc_mutex_unlock(tsdn, &base->mtx);
    .           		}
    .           	}
    .           
    2 ( 0.00%)  	*pind_last = sz_psz2ind(block_size);
    2 ( 0.00%)  	block->size = block_size;
    2 ( 0.00%)  	block->next = NULL;
    .           	assert(block_size >= header_size);
    4 ( 0.00%)  	base_edata_init(extent_sn_next, &block->edata,
    4 ( 0.00%)  	    (void *)((uintptr_t)block + header_size), block_size - header_size);
    .           	return block;
   24 ( 0.00%)  }
    .           
    .           /*
    .            * Allocate an extent that is at least as large as specified size, with
    .            * specified alignment.
    .            */
    .           static edata_t *
    .           base_extent_alloc(tsdn_t *tsdn, base_t *base, size_t size, size_t alignment) {
    .           	malloc_mutex_assert_owner(tsdn, &base->mtx);
    .           
    .           	ehooks_t *ehooks = base_ehooks_get_for_metadata(base);
    .           	/*
    .           	 * Drop mutex during base_block_alloc(), because an extent hook will be
    .           	 * called.
    .           	 */
    .           	malloc_mutex_unlock(tsdn, &base->mtx);
    9 ( 0.00%)  	base_block_t *block = base_block_alloc(tsdn, base, ehooks,
  216 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_block_alloc.isra.0 (1x)
    .           	    base_ind_get(base), &base->pind_last, &base->extent_sn_next, size,
    .           	    alignment);
    .           	malloc_mutex_lock(tsdn, &base->mtx);
    2 ( 0.00%)  	if (block == NULL) {
    .           		return NULL;
    .           	}
    2 ( 0.00%)  	block->next = base->blocks;
    1 ( 0.00%)  	base->blocks = block;
    .           	if (config_stats) {
    3 ( 0.00%)  		base->allocated += sizeof(base_block_t);
    .           		base->resident += PAGE_CEILING(sizeof(base_block_t));
    2 ( 0.00%)  		base->mapped += block->size;
    .           		if (metadata_thp_madvise() &&
    .           		    !(opt_metadata_thp == metadata_thp_auto
    .           		      && !base->auto_thp_switched)) {
    .           			assert(base->n_thp > 0);
    .           			base->n_thp += HUGEPAGE_CEILING(sizeof(base_block_t)) >>
    .           			    LG_HUGEPAGE;
    .           		}
    .           		assert(base->allocated <= base->resident);
    .           		assert(base->resident <= base->mapped);
    .           		assert(base->n_thp << LG_HUGEPAGE <= base->mapped);
    .           	}
    1 ( 0.00%)  	return &block->edata;
    .           }
    .           
    .           base_t *
    5 ( 0.00%)  b0get(void) {
    .           	return b0;
   10 ( 0.00%)  }
    .           
    .           base_t *
    .           base_new(tsdn_t *tsdn, unsigned ind, const extent_hooks_t *extent_hooks,
   14 ( 0.00%)      bool metadata_use_hooks) {
    1 ( 0.00%)  	pszind_t pind_last = 0;
    1 ( 0.00%)  	size_t extent_sn_next = 0;
    .           
    .           	/*
    .           	 * The base will contain the ehooks eventually, but it itself is
    .           	 * allocated using them.  So we use some stack ehooks to bootstrap its
    .           	 * memory, and then initialize the ehooks within the base_t.
    .           	 */
    .           	ehooks_t fake_ehooks;
    8 ( 0.00%)  	ehooks_init(&fake_ehooks, metadata_use_hooks ?
    4 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ehooks.c:_rjem_je_ehooks_init (1x)
    .           	    (extent_hooks_t *)extent_hooks :
    .           	    (extent_hooks_t *)&ehooks_default_extent_hooks, ind);
    .           
   10 ( 0.00%)  	base_block_t *block = base_block_alloc(tsdn, NULL, &fake_ehooks, ind,
  312 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_block_alloc.isra.0 (1x)
    .           	    &pind_last, &extent_sn_next, sizeof(base_t), QUANTUM);
    4 ( 0.00%)  	if (block == NULL) {
    .           		return NULL;
    .           	}
    .           
    .           	size_t gap_size;
    .           	size_t base_alignment = CACHELINE;
    .           	size_t base_size = ALIGNMENT_CEILING(sizeof(base_t), base_alignment);
    .           	base_t *base = (base_t *)base_extent_bump_alloc_helper(&block->edata,
    .           	    &gap_size, base_size, base_alignment);
    4 ( 0.00%)  	ehooks_init(&base->ehooks, (extent_hooks_t *)extent_hooks, ind);
    4 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ehooks.c:_rjem_je_ehooks_init (1x)
    4 ( 0.00%)  	ehooks_init(&base->ehooks_base, metadata_use_hooks ?
    4 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/ehooks.c:_rjem_je_ehooks_init (1x)
    .           	    (extent_hooks_t *)extent_hooks :
    .           	    (extent_hooks_t *)&ehooks_default_extent_hooks, ind);
    7 ( 0.00%)  	if (malloc_mutex_init(&base->mtx, "base", WITNESS_RANK_BASE,
  127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
    .           	    malloc_mutex_rank_exclusive)) {
    .           		base_unmap(tsdn, &fake_ehooks, ind, block, block->size);
    .           		return NULL;
    .           	}
    2 ( 0.00%)  	base->pind_last = pind_last;
    2 ( 0.00%)  	base->extent_sn_next = extent_sn_next;
    2 ( 0.00%)  	base->blocks = block;
    2 ( 0.00%)  	base->auto_thp_switched = false;
  291 ( 0.01%)  	for (szind_t i = 0; i < SC_NSIZES; i++) {
  261 ( 0.01%)  		edata_heap_new(&base->avail[i]);
  928 ( 0.03%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_new (232x)
    .           	}
    .           	if (config_stats) {
    2 ( 0.00%)  		base->allocated = sizeof(base_block_t);
    .           		base->resident = PAGE_CEILING(sizeof(base_block_t));
    2 ( 0.00%)  		base->mapped = block->size;
    1 ( 0.00%)  		base->n_thp = (opt_metadata_thp == metadata_thp_always) &&
    .           		    metadata_thp_madvise() ? HUGEPAGE_CEILING(sizeof(base_block_t))
    3 ( 0.00%)  		    >> LG_HUGEPAGE : 0;
    .           		assert(base->allocated <= base->resident);
    .           		assert(base->resident <= base->mapped);
    .           		assert(base->n_thp << LG_HUGEPAGE <= base->mapped);
    .           	}
    6 ( 0.00%)  	base_extent_bump_alloc_post(base, &block->edata, gap_size, base,
   84 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_extent_bump_alloc_post (1x)
    .           	    base_size);
    .           
    .           	return base;
   12 ( 0.00%)  }
    .           
    .           void
    .           base_delete(tsdn_t *tsdn, base_t *base) {
    .           	ehooks_t *ehooks = base_ehooks_get_for_metadata(base);
    .           	base_block_t *next = base->blocks;
    .           	do {
    .           		base_block_t *block = next;
    .           		next = block->next;
    .           		base_unmap(tsdn, ehooks, base_ind_get(base), block,
    .           		    block->size);
    .           	} while (next != NULL);
    .           }
    .           
    .           ehooks_t *
  128 ( 0.00%)  base_ehooks_get(base_t *base) {
    .           	return &base->ehooks;
   64 ( 0.00%)  }
    .           
    .           ehooks_t *
    .           base_ehooks_get_for_metadata(base_t *base) {
    1 ( 0.00%)  	return &base->ehooks_base;
    .           }
    .           
    .           extent_hooks_t *
    .           base_extent_hooks_set(base_t *base, extent_hooks_t *extent_hooks) {
    .           	extent_hooks_t *old_extent_hooks =
    .           	    ehooks_get_extent_hooks_ptr(&base->ehooks);
    .           	ehooks_init(&base->ehooks, extent_hooks, ehooks_ind_get(&base->ehooks));
    .           	return old_extent_hooks;
    .           }
    .           
    .           static void *
    .           base_alloc_impl(tsdn_t *tsdn, base_t *base, size_t size, size_t alignment,
  370 ( 0.01%)      size_t *esn) {
   74 ( 0.00%)  	alignment = QUANTUM_CEILING(alignment);
  185 ( 0.01%)  	size_t usize = ALIGNMENT_CEILING(size, alignment);
  111 ( 0.00%)  	size_t asize = usize + alignment - QUANTUM;
    .           
    .           	edata_t *edata = NULL;
   74 ( 0.00%)  	malloc_mutex_lock(tsdn, &base->mtx);
3,397 ( 0.12%)  	for (szind_t i = sz_size2index(asize); i < SC_NSIZES; i++) {
2,383 ( 0.08%)  		edata = edata_heap_remove_first(&base->avail[i]);
13,088 ( 0.46%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_remove_first (2,000x)
4,000 ( 0.14%)  		if (edata != NULL) {
    .           			/* Use existing space. */
    .           			break;
    .           		}
    .           	}
    .           	if (edata == NULL) {
    .           		/* Try to allocate more space. */
    .           		edata = base_extent_alloc(tsdn, base, usize, alignment);
    .           	}
    .           	void *ret;
    .           	if (edata == NULL) {
    .           		ret = NULL;
    .           		goto label_return;
    .           	}
    .           
    .           	ret = base_extent_bump_alloc(base, edata, usize, alignment);
  111 ( 0.00%)  	if (esn != NULL) {
   33 ( 0.00%)  		*esn = (size_t)edata_sn_get(edata);
    .           	}
    .           label_return:
    .           	malloc_mutex_unlock(tsdn, &base->mtx);
    .           	return ret;
  333 ( 0.01%)  }
    .           
    .           /*
    .            * base_alloc() returns zeroed memory, which is always demand-zeroed for the
    .            * auto arenas, in order to make multi-page sparse data structures such as radix
    .            * tree nodes efficient with respect to physical memory usage.  Upon success a
    .            * pointer to at least size bytes with specified alignment is returned.  Note
    .            * that size is rounded up to the nearest multiple of alignment to avoid false
    .            * sharing.
    .            */
    .           void *
    4 ( 0.00%)  base_alloc(tsdn_t *tsdn, base_t *base, size_t size, size_t alignment) {
    8 ( 0.00%)  	return base_alloc_impl(tsdn, base, size, alignment, NULL);
4,363 ( 0.15%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_alloc_impl (4x)
    .           }
    .           
    .           edata_t *
  165 ( 0.01%)  base_alloc_edata(tsdn_t *tsdn, base_t *base) {
    .           	size_t esn;
  132 ( 0.00%)  	edata_t *edata = base_alloc_impl(tsdn, base, sizeof(edata_t),
27,138 ( 0.96%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:base_alloc_impl (33x)
    .           	    EDATA_ALIGNMENT, &esn);
   66 ( 0.00%)  	if (edata == NULL) {
    .           		return NULL;
    .           	}
    .           	edata_esn_set(edata, esn);
    .           	return edata;
  165 ( 0.01%)  }
    .           
    .           void
    .           base_stats_get(tsdn_t *tsdn, base_t *base, size_t *allocated, size_t *resident,
    .               size_t *mapped, size_t *n_thp) {
    .           	cassert(config_stats);
    .           
    .           	malloc_mutex_lock(tsdn, &base->mtx);
    .           	assert(base->allocated <= base->resident);
-- line 500 ----------------------------------------
-- line 517 ----------------------------------------
    .           }
    .           
    .           void
    .           base_postfork_child(tsdn_t *tsdn, base_t *base) {
    .           	malloc_mutex_postfork_child(tsdn, &base->mtx);
    .           }
    .           
    .           bool
    2 ( 0.00%)  base_boot(tsdn_t *tsdn) {
    5 ( 0.00%)  	b0 = base_new(tsdn, 0, (extent_hooks_t *)&ehooks_default_extent_hooks,
2,119 ( 0.08%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/base.c:_rjem_je_base_new (1x)
    .           	    /* metadata_use_hooks */ true);
    2 ( 0.00%)  	return (b0 == NULL);
    2 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/bit_util.h
--------------------------------------------------------------------------------
Ir             

-- line 20 ----------------------------------------
    .           ffs_llu(unsigned long long x) {
    .           	util_assume(x != 0);
    .           	return JEMALLOC_INTERNAL_FFSLL(x) - 1;
    .           }
    .           
    .           static inline unsigned
    .           ffs_lu(unsigned long x) {
    .           	util_assume(x != 0);
2,436 ( 0.09%)  	return JEMALLOC_INTERNAL_FFSL(x) - 1;
    .           }
    .           
    .           static inline unsigned
    .           ffs_u(unsigned x) {
    .           	util_assume(x != 0);
    .           	return JEMALLOC_INTERNAL_FFS(x) - 1;
    .           }
    .           
-- line 36 ----------------------------------------
-- line 92 ----------------------------------------
    .           	 * as desired.
    .           	 */
    .           	return (8 * sizeof(x) - 1) ^ __builtin_clzll(x);
    .           }
    .           
    .           static inline unsigned
    .           fls_lu(unsigned long x) {
    .           	util_assume(x != 0);
  309 ( 0.01%)  	return (8 * sizeof(x) - 1) ^ __builtin_clzl(x);
    .           }
    .           
    .           static inline unsigned
    .           fls_u(unsigned x) {
    .           	util_assume(x != 0);
    .           	return (8 * sizeof(x) - 1) ^ __builtin_clz(x);
    .           }
    .           #elif defined(_MSC_VER)
-- line 108 ----------------------------------------
-- line 251 ----------------------------------------
    .           #else
    .           	return popcount_u_slow(bitmap);
    .           #endif
    .           }
    .           
    .           static inline unsigned
    .           popcount_lu(unsigned long bitmap) {
    .           #ifdef JEMALLOC_INTERNAL_POPCOUNTL
  195 ( 0.01%)  	return JEMALLOC_INTERNAL_POPCOUNTL(bitmap);
1,053 ( 0.04%)  => /cargo/registry/src/index.crates.io-6f17d22bba15001f/compiler_builtins-0.1.103/./lib/builtins/popcountdi2.c:__popcountdi2 (39x)
    .           #else
    .           	return popcount_lu_slow(bitmap);
    .           #endif
    .           }
    .           
    .           static inline unsigned
    .           popcount_llu(unsigned long long bitmap) {
    .           #ifdef JEMALLOC_INTERNAL_POPCOUNTLL
-- line 267 ----------------------------------------
-- line 274 ----------------------------------------
    .           /*
    .            * Clears first unset bit in bitmap, and returns
    .            * place of bit.  bitmap *must not* be 0.
    .            */
    .           
    .           static inline size_t
    .           cfs_lu(unsigned long* bitmap) {
    .           	util_assume(*bitmap != 0);
1,186 ( 0.04%)  	size_t bit = ffs_lu(*bitmap);
1,186 ( 0.04%)  	*bitmap ^= ZU(1) << bit;
    .           	return bit;
    .           }
    .           
    .           static inline unsigned
    .           ffs_zu(size_t x) {
    .           #if LG_SIZEOF_PTR == LG_SIZEOF_INT
    .           	return ffs_u(x);
    .           #elif LG_SIZEOF_PTR == LG_SIZEOF_LONG
-- line 291 ----------------------------------------
-- line 353 ----------------------------------------
    .           	return fls_u(x);
    .           }
    .           
    .           static inline uint64_t
    .           pow2_ceil_u64(uint64_t x) {
    .           	if (unlikely(x <= 1)) {
    .           		return x;
    .           	}
    1 ( 0.00%)  	size_t msb_on_index = fls_u64(x - 1);
    .           	/*
    .           	 * Range-check; it's on the callers to ensure that the result of this
    .           	 * call won't overflow.
    .           	 */
    .           	assert(msb_on_index < 63);
    4 ( 0.00%)  	return 1ULL << (msb_on_index + 1);
    .           }
    .           
    .           static inline uint32_t
    .           pow2_ceil_u32(uint32_t x) {
    .           	if (unlikely(x <= 1)) {
    .           	    return x;
    .           	}
    .           	size_t msb_on_index = fls_u32(x - 1);
-- line 375 ----------------------------------------
-- line 395 ----------------------------------------
    .           	return fls_u64(x);
    .           #else
    .           	return fls_u32(x);
    .           #endif
    .           }
    .           
    .           static inline unsigned
    .           lg_ceil(size_t x) {
  903 ( 0.03%)  	return lg_floor(x) + ((x & (x - 1)) == 0 ? 0 : 1);
    .           }
    .           
    .           /* A compile-time version of lg_floor and lg_ceil. */
    .           #define LG_FLOOR_1(x) 0
    .           #define LG_FLOOR_2(x) (x < (1ULL << 1) ? LG_FLOOR_1(x) : 1 + LG_FLOOR_1(x >> 1))
    .           #define LG_FLOOR_4(x) (x < (1ULL << 2) ? LG_FLOOR_2(x) : 2 + LG_FLOOR_2(x >> 2))
    .           #define LG_FLOOR_8(x) (x < (1ULL << 4) ? LG_FLOOR_4(x) : 4 + LG_FLOOR_4(x >> 4))
    .           #define LG_FLOOR_16(x) (x < (1ULL << 8) ? LG_FLOOR_8(x) : 8 + LG_FLOOR_8(x >> 8))
-- line 411 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/hook.c
--------------------------------------------------------------------------------
Ir           

-- line 14 ----------------------------------------
  .           
  .           seq_define(hooks_internal_t, hooks)
  .           
  .           static atomic_u_t nhooks = ATOMIC_INIT(0);
  .           static seq_hooks_t hooks[HOOK_MAX];
  .           static malloc_mutex_t hooks_mu;
  .           
  .           bool
  1 ( 0.00%)  hook_boot() {
  5 ( 0.00%)  	return malloc_mutex_init(&hooks_mu, "hooks", WITNESS_RANK_HOOK,
127 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (1x)
  .           	    malloc_mutex_rank_exclusive);
  .           }
  .           
  .           static void *
  .           hook_install_locked(hooks_t *to_install) {
  .           	hooks_internal_t hooks_internal;
  .           	for (int i = 0; i < HOOK_MAX; i++) {
  .           		bool success = seq_try_load_hooks(&hooks_internal, &hooks[i]);
-- line 31 ----------------------------------------
-- line 147 ----------------------------------------
  .           	}								\
  .           	*in_hook = true;
  .           
  .           #define HOOK_EPILOGUE							\
  .           	*in_hook = false;
  .           
  .           void
  .           hook_invoke_alloc(hook_alloc_t type, void *result, uintptr_t result_raw,
232 ( 0.01%)      uintptr_t args_raw[3]) {
 58 ( 0.00%)  	HOOK_PROLOGUE
  .           
  .           	hooks_internal_t hook;
  .           	FOR_EACH_HOOK_BEGIN(&hook)
  .           		hook_alloc h = hook.hooks.alloc_hook;
  .           		if (h != NULL) {
  .           			h(hook.hooks.extra, type, result, result_raw, args_raw);
  .           		}
  .           	FOR_EACH_HOOK_END
  .           
  .           	HOOK_EPILOGUE
319 ( 0.01%)  }
  .           
  .           void
232 ( 0.01%)  hook_invoke_dalloc(hook_dalloc_t type, void *address, uintptr_t args_raw[3]) {
 58 ( 0.00%)  	HOOK_PROLOGUE
  .           	hooks_internal_t hook;
  .           	FOR_EACH_HOOK_BEGIN(&hook)
  .           		hook_dalloc h = hook.hooks.dalloc_hook;
  .           		if (h != NULL) {
  .           			h(hook.hooks.extra, type, address, args_raw);
  .           		}
  .           	FOR_EACH_HOOK_END
  .           	HOOK_EPILOGUE
319 ( 0.01%)  }
  .           
  .           void
  .           hook_invoke_expand(hook_expand_t type, void *address, size_t old_usize,
  .               size_t new_usize, uintptr_t result_raw, uintptr_t args_raw[4]) {
  .           	HOOK_PROLOGUE
  .           	hooks_internal_t hook;
  .           	FOR_EACH_HOOK_BEGIN(&hook)
  .           		hook_expand h = hook.hooks.expand_hook;
-- line 188 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /usr/include/x86_64-linux-gnu/bits/string_fortified.h
--------------------------------------------------------------------------------
Ir             

-- line 21 ----------------------------------------
    .           #ifndef _STRING_H
    .           # error "Never use <bits/string_fortified.h> directly; include <string.h> instead."
    .           #endif
    .           
    .           __fortify_function void *
    .           __NTH (memcpy (void *__restrict __dest, const void *__restrict __src,
    .           	       size_t __len))
    .           {
   87 ( 0.00%)    return __builtin___memcpy_chk (__dest, __src, __len,
  739 ( 0.03%)  => ???:0x000000000011b2d8 (29x)
    .           				 __glibc_objsize0 (__dest));
    .           }
    .           
    .           __fortify_function void *
    .           __NTH (memmove (void *__dest, const void *__src, size_t __len))
    .           {
    5 ( 0.00%)    return __builtin___memmove_chk (__dest, __src, __len,
   23 ( 0.00%)  => ???:0x000000000011b2b8 (1x)
    .           				  __glibc_objsize0 (__dest));
    .           }
    .           
    .           #ifdef __USE_GNU
    .           __fortify_function void *
    .           __NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,
    .           		size_t __len))
    .           {
-- line 44 ----------------------------------------
-- line 51 ----------------------------------------
    .           /* The first two tests here help to catch a somewhat common problem
    .              where the second and third parameter are transposed.  This is
    .              especially problematic if the intended fill value is zero.  In this
    .              case no work is done at all.  We detect these problems by referring
    .              non-existing functions.  */
    .           __fortify_function void *
    .           __NTH (memset (void *__dest, int __ch, size_t __len))
    .           {
1,535 ( 0.05%)    return __builtin___memset_chk (__dest, __ch, __len,
   72 ( 0.00%)  => ???:0x000000000011b258 (1x)
    .           				 __glibc_objsize0 (__dest));
    .           }
    .           
    .           #ifdef __USE_MISC
    .           # include <bits/strings_fortified.h>
    .           
    .           void __explicit_bzero_chk (void *__dest, size_t __len, size_t __destlen)
    .             __THROW __nonnull ((1)) __fortified_attr_access (__write_only__, 1, 2);
-- line 67 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin.c
--------------------------------------------------------------------------------
Ir           

-- line 26 ----------------------------------------
  .           	for (unsigned i = ind1; i <= ind2; i++) {
  .           		bin_shard_sizes[i] = (unsigned)nshards;
  .           	}
  .           
  .           	return false;
  .           }
  .           
  .           void
  1 ( 0.00%)  bin_shard_sizes_boot(unsigned bin_shard_sizes[SC_NBINS]) {
  .           	/* Load the default number of shards. */
  .           	for (unsigned i = 0; i < SC_NBINS; i++) {
 10 ( 0.00%)  		bin_shard_sizes[i] = N_BIN_SHARDS_DEFAULT;
  .           	}
  1 ( 0.00%)  }
  .           
  .           bool
180 ( 0.01%)  bin_init(bin_t *bin) {
252 ( 0.01%)  	if (malloc_mutex_init(&bin->lock, "bin", WITNESS_RANK_BIN,
4,572 ( 0.16%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c:_rjem_je_malloc_mutex_init (36x)
  .           	    malloc_mutex_rank_exclusive)) {
  .           		return true;
  .           	}
 36 ( 0.00%)  	bin->slabcur = NULL;
 72 ( 0.00%)  	edata_heap_new(&bin->slabs_nonfull);
144 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata.c:_rjem_je_edata_heap_new (36x)
  .           	edata_list_active_init(&bin->slabs_full);
  .           	if (config_stats) {
  .           		memset(&bin->stats, 0, sizeof(bin_stats_t));
  .           	}
  .           	return false;
180 ( 0.01%)  }
  .           
  .           void
  .           bin_prefork(tsdn_t *tsdn, bin_t *bin) {
  .           	malloc_mutex_prefork(tsdn, &bin->lock);
  .           }
  .           
  .           void
  .           bin_postfork_parent(tsdn_t *tsdn, bin_t *bin) {
-- line 62 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bitmap.c
--------------------------------------------------------------------------------
Ir           

-- line 91 ----------------------------------------
  .           }
  .           
  .           static size_t
  .           bitmap_info_ngroups(const bitmap_info_t *binfo) {
  .           	return binfo->ngroups;
  .           }
  .           
  .           void
124 ( 0.00%)  bitmap_init(bitmap_t *bitmap, const bitmap_info_t *binfo, bool fill) {
  .           	size_t extra;
  .           
 62 ( 0.00%)  	if (fill) {
  .           		memset(bitmap, 0, bitmap_size(binfo));
  .           		return;
  .           	}
  .           
  .           	memset(bitmap, 0xffU, bitmap_size(binfo));
  .           	extra = (BITMAP_GROUP_NBITS - (binfo->nbits & BITMAP_GROUP_NBITS_MASK))
 62 ( 0.00%)  	    & BITMAP_GROUP_NBITS_MASK;
 62 ( 0.00%)  	if (extra != 0) {
 40 ( 0.00%)  		bitmap[binfo->ngroups - 1] >>= extra;
  .           	}
 62 ( 0.00%)  }
  .           
  .           #endif /* BITMAP_USE_TREE */
  .           
  .           size_t
  .           bitmap_size(const bitmap_info_t *binfo) {
 62 ( 0.00%)  	return (bitmap_info_ngroups(binfo) << LG_SIZEOF_BITMAP);
  .           }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/cache_bin.h
--------------------------------------------------------------------------------
Ir             

-- line 139 ----------------------------------------
    .           	ql_elm(cache_bin_array_descriptor_t) link;
    .           	/* Pointers to the tcache bins. */
    .           	cache_bin_t *bins;
    .           };
    .           
    .           static inline void
    .           cache_bin_array_descriptor_init(cache_bin_array_descriptor_t *descriptor,
    .               cache_bin_t *bins) {
    3 ( 0.00%)  	ql_elm_new(descriptor, link);
    .           	descriptor->bins = bins;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           cache_bin_nonfast_aligned(const void *ptr) {
    .           	if (!config_uaf_detection) {
    .           		return false;
    .           	}
-- line 155 ----------------------------------------
-- line 166 ----------------------------------------
    .           	 * not the desired behavior.
    .           	 */
    .           	return ((uintptr_t)ptr & san_cache_bin_nonfast_mask) == 0;
    .           }
    .           
    .           /* Returns ncached_max: Upper limit on ncached. */
    .           static inline cache_bin_sz_t
    .           cache_bin_info_ncached_max(cache_bin_info_t *info) {
   44 ( 0.00%)  	return info->ncached_max;
    .           }
    .           
    .           /*
    .            * Internal.
    .            *
    .            * Asserts that the pointer associated with earlier is <= the one associated
    .            * with later.
    .            */
-- line 182 ----------------------------------------
-- line 198 ----------------------------------------
    .           	/*
    .           	 * When it's racy, bin->low_bits_full can be modified concurrently. It
    .           	 * can cross the uint16_t max value and become less than
    .           	 * bin->low_bits_empty at the time of the check.
    .           	 */
    .           	if (!racy) {
    .           		cache_bin_assert_earlier(bin, earlier, later);
    .           	}
  176 ( 0.01%)  	return later - earlier;
    .           }
    .           
    .           /*
    .            * Number of items currently cached in the bin, without checking ncached_max.
    .            * We require specifying whether or not the request is racy or not (i.e. whether
    .            * or not concurrent modifications are possible).
    .            */
    .           static inline cache_bin_sz_t
    .           cache_bin_ncached_get_internal(cache_bin_t *bin, bool racy) {
    .           	cache_bin_sz_t diff = cache_bin_diff(bin,
   22 ( 0.00%)  	    (uint16_t)(uintptr_t)bin->stack_head, bin->low_bits_empty, racy);
    .           	cache_bin_sz_t n = diff / sizeof(void *);
    .           	/*
    .           	 * We have undefined behavior here; if this function is called from the
    .           	 * arena stats updating code, then stack_head could change from the
    .           	 * first line to the next one.  Morally, these loads should be atomic,
    .           	 * but compilers won't currently generate comparisons with in-memory
    .           	 * operands against atomics, and these variables get accessed on the
    .           	 * fast paths.  This should still be "safe" in the sense of generating
-- line 225 ----------------------------------------
-- line 248 ----------------------------------------
    .            * A pointer to the position one past the end of the backing array.
    .            *
    .            * Do not call if racy, because both 'bin->stack_head' and 'bin->low_bits_full'
    .            * are subject to concurrent modifications.
    .            */
    .           static inline void **
    .           cache_bin_empty_position_get(cache_bin_t *bin) {
    .           	cache_bin_sz_t diff = cache_bin_diff(bin,
   44 ( 0.00%)  	    (uint16_t)(uintptr_t)bin->stack_head, bin->low_bits_empty,
    .           	    /* racy */ false);
   88 ( 0.00%)  	uintptr_t empty_bits = (uintptr_t)bin->stack_head + diff;
    .           	void **ret = (void **)empty_bits;
    .           
    .           	assert(ret >= bin->stack_head);
    .           
    .           	return ret;
    .           }
    .           
    .           /*
-- line 266 ----------------------------------------
-- line 271 ----------------------------------------
    .            *
    .            * No values are concurrently modified, so should be safe to read in a
    .            * multithreaded environment. Currently concurrent access happens only during
    .            * arena statistics collection.
    .            */
    .           static inline uint16_t
    .           cache_bin_low_bits_low_bound_get(cache_bin_t *bin, cache_bin_info_t *info) {
    .           	return (uint16_t)bin->low_bits_empty -
   22 ( 0.00%)  	    info->ncached_max * sizeof(void *);
    .           }
    .           
    .           /*
    .            * Internal.
    .            *
    .            * A pointer to the position with the lowest address of the backing array.
    .            */
    .           static inline void **
-- line 287 ----------------------------------------
-- line 353 ----------------------------------------
    .           	 * and eagerly checking ret would cause pipeline stall (waiting for the
    .           	 * cacheline).
    .           	 */
    .           
    .           	/*
    .           	 * This may read from the empty position; however the loaded value won't
    .           	 * be used.  It's safe because the stack has one more slot reserved.
    .           	 */
3,923 ( 0.14%)  	void *ret = *bin->stack_head;
    .           	uint16_t low_bits = (uint16_t)(uintptr_t)bin->stack_head;
  666 ( 0.02%)  	void **new_head = bin->stack_head + 1;
    .           
    .           	/*
    .           	 * Note that the low water mark is at most empty; if we pass this check,
    .           	 * we know we're non-empty.
    .           	 */
1,332 ( 0.05%)  	if (likely(low_bits != bin->low_bits_low_water)) {
  631 ( 0.02%)  		bin->stack_head = new_head;
   22 ( 0.00%)  		*success = true;
    .           		return ret;
    .           	}
    .           	if (!adjust_low_water) {
    .           		*success = false;
    .           		return NULL;
    .           	}
    .           	/*
    .           	 * In the fast-path case where we call alloc_easy and then alloc, the
    .           	 * previous checking and computation is optimized away -- we didn't
    .           	 * actually commit any of our operations.
    .           	 */
   70 ( 0.00%)  	if (likely(low_bits != bin->low_bits_empty)) {
    .           		bin->stack_head = new_head;
    .           		bin->low_bits_low_water = (uint16_t)(uintptr_t)new_head;
    .           		*success = true;
    .           		return ret;
    .           	}
    .           	*success = false;
    .           	return NULL;
    .           }
-- line 391 ----------------------------------------
-- line 419 ----------------------------------------
    .           	bin->stack_head += n;
    .           	cache_bin_low_water_adjust(bin);
    .           
    .           	return n;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           cache_bin_full(cache_bin_t *bin) {
3,169 ( 0.11%)  	return ((uint16_t)(uintptr_t)bin->stack_head == bin->low_bits_full);
    .           }
    .           
    .           /*
    .            * Free an object into the given bin.  Fails only if the bin is full.
    .            */
    .           JEMALLOC_ALWAYS_INLINE bool
    .           cache_bin_dalloc_easy(cache_bin_t *bin, void *ptr) {
1,256 ( 0.04%)  	if (unlikely(cache_bin_full(bin))) {
    .           		return false;
    .           	}
    .           
1,256 ( 0.04%)  	bin->stack_head--;
  628 ( 0.02%)  	*bin->stack_head = ptr;
    .           	cache_bin_assert_earlier(bin, bin->low_bits_full,
    .           	    (uint16_t)(uintptr_t)bin->stack_head);
    .           
    .           	return true;
    .           }
    .           
    .           /* Returns false if failed to stash (i.e. bin is full). */
    .           JEMALLOC_ALWAYS_INLINE bool
-- line 448 ----------------------------------------
-- line 565 ----------------------------------------
    .           /*
    .            * Start a fill.  The bin must be empty, and This must be followed by a
    .            * finish_fill call before doing any alloc/dalloc operations on the bin.
    .            */
    .           static inline void
    .           cache_bin_init_ptr_array_for_fill(cache_bin_t *bin, cache_bin_info_t *info,
    .               cache_bin_ptr_array_t *arr, cache_bin_sz_t nfill) {
    .           	cache_bin_assert_empty(bin, info);
   44 ( 0.00%)  	arr->ptr = cache_bin_empty_position_get(bin) - nfill;
    .           }
    .           
    .           /*
    .            * While nfill in cache_bin_init_ptr_array_for_fill is the number we *intend* to
    .            * fill, nfilled here is the number we actually filled (which may be less, in
    .            * case of OOM.
    .            */
    .           static inline void
    .           cache_bin_finish_fill(cache_bin_t *bin, cache_bin_info_t *info,
    .               cache_bin_ptr_array_t *arr, cache_bin_sz_t nfilled) {
    .           	cache_bin_assert_empty(bin, info);
    .           	void **empty_position = cache_bin_empty_position_get(bin);
   66 ( 0.00%)  	if (nfilled < arr->n) {
  110 ( 0.00%)  		memmove(empty_position - nfilled, empty_position - arr->n,
    .           		    nfilled * sizeof(void *));
    .           	}
   22 ( 0.00%)  	bin->stack_head = empty_position - nfilled;
    .           }
    .           
    .           /*
    .            * Same deal, but with flush.  Unlike fill (which can fail), the user must flush
    .            * everything we give them.
    .            */
    .           static inline void
    .           cache_bin_init_ptr_array_for_flush(cache_bin_t *bin, cache_bin_info_t *info,
-- line 598 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pa.c
--------------------------------------------------------------------------------
Ir           

-- line 12 ----------------------------------------
  .           static void
  .           pa_nactive_sub(pa_shard_t *shard, size_t sub_pages) {
  .           	assert(atomic_load_zu(&shard->nactive, ATOMIC_RELAXED) >= sub_pages);
  .           	atomic_fetch_sub_zu(&shard->nactive, sub_pages, ATOMIC_RELAXED);
  .           }
  .           
  .           bool
  .           pa_central_init(pa_central_t *central, base_t *base, bool hpa,
  1 ( 0.00%)      hpa_hooks_t *hpa_hooks) {
  .           	bool err;
  2 ( 0.00%)  	if (hpa) {
  .           		err = hpa_central_init(&central->hpa, base, hpa_hooks);
  .           		if (err) {
  .           			return true;
  .           		}
  .           	}
  .           	return false;
  2 ( 0.00%)  }
  .           
  .           bool
  .           pa_shard_init(tsdn_t *tsdn, pa_shard_t *shard, pa_central_t *central,
  .               emap_t *emap, base_t *base, unsigned ind, pa_shard_stats_t *stats,
  .               malloc_mutex_t *stats_mtx, nstime_t *cur_time,
  .               size_t pac_oversize_threshold, ssize_t dirty_decay_ms,
 14 ( 0.00%)      ssize_t muzzy_decay_ms) {
  .           	/* This will change eventually, but for now it should hold. */
  .           	assert(base_ind_get(base) == ind);
  6 ( 0.00%)  	if (edata_cache_init(&shard->edata_cache, base)) {
151 ( 0.01%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/edata_cache.c:_rjem_je_edata_cache_init (1x)
  .           		return true;
  .           	}
  .           
 18 ( 0.00%)  	if (pac_init(tsdn, &shard->pac, base, emap, &shard->edata_cache,
12,083 ( 0.43%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/pac.c:_rjem_je_pac_init (1x)
  .           	    cur_time, pac_oversize_threshold, dirty_decay_ms, muzzy_decay_ms,
  .           	    &stats->pac_stats, stats_mtx)) {
  .           		return true;
  .           	}
  .           
  1 ( 0.00%)  	shard->ind = ind;
  .           
  1 ( 0.00%)  	shard->ever_used_hpa = false;
  .           	atomic_store_b(&shard->use_hpa, false, ATOMIC_RELAXED);
  .           
  .           	atomic_store_zu(&shard->nactive, 0, ATOMIC_RELAXED);
  .           
  2 ( 0.00%)  	shard->stats_mtx = stats_mtx;
  2 ( 0.00%)  	shard->stats = stats;
  .           	memset(shard->stats, 0, sizeof(*shard->stats));
  .           
  2 ( 0.00%)  	shard->central = central;
  1 ( 0.00%)  	shard->emap = emap;
  1 ( 0.00%)  	shard->base = base;
  .           
  .           	return false;
  8 ( 0.00%)  }
  .           
  .           bool
  .           pa_shard_enable_hpa(tsdn_t *tsdn, pa_shard_t *shard,
  .               const hpa_shard_opts_t *hpa_opts, const sec_opts_t *hpa_sec_opts) {
  .           	if (hpa_shard_init(&shard->hpa_shard, &shard->central->hpa, shard->emap,
  .           	    shard->base, &shard->edata_cache, shard->ind, hpa_opts)) {
  .           		return true;
  .           	}
-- line 73 ----------------------------------------
-- line 116 ----------------------------------------
  .           pa_get_pai(pa_shard_t *shard, edata_t *edata) {
  .           	return (edata_pai_get(edata) == EXTENT_PAI_PAC
  .           	    ? &shard->pac.pai : &shard->hpa_sec.pai);
  .           }
  .           
  .           edata_t *
  .           pa_alloc(tsdn_t *tsdn, pa_shard_t *shard, size_t size, size_t alignment,
  .               bool slab, szind_t szind, bool zero, bool guarded,
448 ( 0.02%)      bool *deferred_work_generated) {
  .           	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
  .           	    WITNESS_RANK_CORE, 0);
  .           	assert(!guarded || alignment <= PAGE);
  .           
  .           	edata_t *edata = NULL;
128 ( 0.00%)  	if (!guarded && pa_shard_uses_hpa(shard)) {
  .           		edata = pai_alloc(tsdn, &shard->hpa_sec.pai, size, alignment,
  .           		    zero, /* guarded */ false, slab, deferred_work_generated);
  .           	}
  .           	/*
  .           	 * Fall back to the PAC if the HPA is off or couldn't serve the given
  .           	 * allocation request.
  .           	 */
  .           	if (edata == NULL) {
 32 ( 0.00%)  		edata = pai_alloc(tsdn, &shard->pac.pai, size, alignment, zero,
  .           		    guarded, slab, deferred_work_generated);
  .           	}
128 ( 0.00%)  	if (edata != NULL) {
  .           		assert(edata_size_get(edata) == size);
 64 ( 0.00%)  		pa_nactive_add(shard, size >> LG_PAGE);
224 ( 0.01%)  		emap_remap(tsdn, shard->emap, edata, szind, slab);
2,990 ( 0.11%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_remap (32x)
  .           		edata_szind_set(edata, szind);
  .           		edata_slab_set(edata, slab);
112 ( 0.00%)  		if (slab && (size > 2 * PAGE)) {
138 ( 0.00%)  			emap_register_interior(tsdn, shard->emap, edata, szind);
1,670 ( 0.06%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:_rjem_je_emap_register_interior (23x)
  .           		}
  .           		assert(edata_arena_ind_get(edata) == shard->ind);
  .           	}
  .           	return edata;
256 ( 0.01%)  }
  .           
  .           bool
  .           pa_expand(tsdn_t *tsdn, pa_shard_t *shard, edata_t *edata, size_t old_size,
  .               size_t new_size, szind_t szind, bool zero, bool *deferred_work_generated) {
  .           	assert(new_size > old_size);
  .           	assert(edata_size_get(edata) == old_size);
  .           	assert((new_size & PAGE_MASK) == 0);
  .           	if (edata_guarded_get(edata)) {
-- line 162 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/mutex.c
--------------------------------------------------------------------------------
Ir           

-- line 104 ----------------------------------------
  .           	if (n_thds > data->max_n_thds) {
  .           		data->max_n_thds = n_thds;
  .           	}
  .           }
  .           
  .           static void
  .           mutex_prof_data_init(mutex_prof_data_t *data) {
  .           	memset(data, 0, sizeof(mutex_prof_data_t));
 58 ( 0.00%)  	nstime_init_zero(&data->max_wait_time);
  .           	nstime_init_zero(&data->tot_wait_time);
 58 ( 0.00%)  	data->prev_owner = NULL;
  .           }
  .           
  .           void
  .           malloc_mutex_prof_data_reset(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	malloc_mutex_assert_owner(tsdn, mutex);
  .           	mutex_prof_data_init(&mutex->prof_data);
  .           }
  .           
-- line 122 ----------------------------------------
-- line 133 ----------------------------------------
  .           		return 0;
  .           	} else {
  .           		return 1;
  .           	}
  .           }
  .           
  .           bool
  .           malloc_mutex_init(malloc_mutex_t *mutex, const char *name,
464 ( 0.02%)      witness_rank_t rank, malloc_mutex_lock_order_t lock_order) {
  .           	mutex_prof_data_init(&mutex->prof_data);
  .           #ifdef _WIN32
  .           #  if _WIN32_WINNT >= 0x0600
  .           	InitializeSRWLock(&mutex->lock);
  .           #  else
  .           	if (!InitializeCriticalSectionAndSpinCount(&mutex->lock,
  .           	    _CRT_SPINCOUNT)) {
  .           		return true;
-- line 149 ----------------------------------------
-- line 159 ----------------------------------------
  .           		if (_pthread_mutex_init_calloc_cb(&mutex->lock,
  .           		    bootstrap_calloc) != 0) {
  .           			return true;
  .           		}
  .           	}
  .           #else
  .           	pthread_mutexattr_t attr;
  .           
348 ( 0.01%)  	if (pthread_mutexattr_init(&attr) != 0) {
232 ( 0.01%)  => ./nptl/./nptl/pthread_mutexattr_init.c:pthread_mutexattr_init@@GLIBC_2.34 (58x)
  .           		return true;
  .           	}
232 ( 0.01%)  	pthread_mutexattr_settype(&attr, MALLOC_MUTEX_TYPE);
754 ( 0.03%)  => ./nptl/./nptl/pthread_mutexattr_settype.c:pthread_mutexattr_settype@@GLIBC_2.34 (58x)
348 ( 0.01%)  	if (pthread_mutex_init(&mutex->lock, &attr) != 0) {
2,900 ( 0.10%)  => ./nptl/./nptl/pthread_mutex_init.c:pthread_mutex_init@@GLIBC_2.2.5 (58x)
 58 ( 0.00%)  		pthread_mutexattr_destroy(&attr);
  .           		return true;
  .           	}
116 ( 0.00%)  	pthread_mutexattr_destroy(&attr);
174 ( 0.01%)  => ./nptl/./nptl/pthread_mutexattr_destroy.c:pthread_mutexattr_destroy@@GLIBC_2.34 (58x)
  .           #endif
  .           	if (config_debug) {
  .           		mutex->lock_order = lock_order;
  .           		if (lock_order == malloc_mutex_address_ordered) {
  .           			witness_init(&mutex->witness, name, rank,
  .           			    mutex_addr_comp, mutex);
  .           		} else {
  .           			witness_init(&mutex->witness, name, rank, NULL, NULL);
  .           		}
  .           	}
116 ( 0.00%)  	return false;
406 ( 0.01%)  }
  .           
  .           void
  .           malloc_mutex_prefork(tsdn_t *tsdn, malloc_mutex_t *mutex) {
  .           	malloc_mutex_lock(tsdn, mutex);
  .           }
  .           
  .           void
  .           malloc_mutex_postfork_parent(tsdn_t *tsdn, malloc_mutex_t *mutex) {
-- line 195 ----------------------------------------
-- line 208 ----------------------------------------
  .           		if (opt_abort) {
  .           			abort();
  .           		}
  .           	}
  .           #endif
  .           }
  .           
  .           bool
  1 ( 0.00%)  malloc_mutex_boot(void) {
  .           #ifdef JEMALLOC_MUTEX_INIT_CB
  .           	postpone_init = false;
  .           	while (postponed_mutexes != NULL) {
  .           		if (_pthread_mutex_init_calloc_cb(&postponed_mutexes->lock,
  .           		    bootstrap_calloc) != 0) {
  .           			return true;
  .           		}
  .           		postponed_mutexes = postponed_mutexes->postponed_next;
  .           	}
  .           #endif
  .           	return false;
  2 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/bin_info.c
--------------------------------------------------------------------------------
Ir           

-- line 3 ----------------------------------------
  .           
  .           #include "jemalloc/internal/bin_info.h"
  .           
  .           bin_info_t bin_infos[SC_NBINS];
  .           
  .           static void
  .           bin_infos_init(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],
  .               bin_info_t infos[SC_NBINS]) {
 63 ( 0.00%)  	for (unsigned i = 0; i < SC_NBINS; i++) {
  .           		bin_info_t *bin_info = &infos[i];
  .           		sc_t *sc = &sc_data->sc[i];
145 ( 0.01%)  		bin_info->reg_size = ((size_t)1U << sc->lg_base)
144 ( 0.01%)  		    + ((size_t)sc->ndelta << sc->lg_delta);
144 ( 0.01%)  		bin_info->slab_size = (sc->pgs << LG_PAGE);
 36 ( 0.00%)  		bin_info->nregs =
 72 ( 0.00%)  		    (uint32_t)(bin_info->slab_size / bin_info->reg_size);
 72 ( 0.00%)  		bin_info->n_shards = bin_shard_sizes[i];
108 ( 0.00%)  		bitmap_info_t bitmap_info = BITMAP_INFO_INITIALIZER(
  .           		    bin_info->nregs);
 72 ( 0.00%)  		bin_info->bitmap_info = bitmap_info;
  .           	}
  .           }
  .           
  .           void
  3 ( 0.00%)  bin_info_boot(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS]) {
  .           	assert(sc_data->initialized);
  .           	bin_infos_init(sc_data, bin_shard_sizes, bin_infos);
  2 ( 0.00%)  }

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/nstime.c
--------------------------------------------------------------------------------
Ir           

-- line 40 ----------------------------------------
  .           	 * initialized (covered by the assert below).  Full-initialize needed
  .           	 * before changing it to non-zero.
  .           	 */
  .           	nstime_assert_initialized(time);
  .           	nstime_set_initialized(time);
  .           }
  .           
  .           void
  6 ( 0.00%)  nstime_init(nstime_t *time, uint64_t ns) {
  .           	nstime_set_initialized(time);
  6 ( 0.00%)  	time->ns = ns;
  6 ( 0.00%)  }
  .           
  .           void
  .           nstime_init2(nstime_t *time, uint64_t sec, uint64_t nsec) {
  .           	nstime_set_initialized(time);
  6 ( 0.00%)  	time->ns = sec * BILLION + nsec;
  .           }
  .           
  .           uint64_t
  1 ( 0.00%)  nstime_ns(const nstime_t *time) {
  .           	nstime_assert_initialized(time);
  1 ( 0.00%)  	return time->ns;
  1 ( 0.00%)  }
  .           
  .           uint64_t
  .           nstime_msec(const nstime_t *time) {
  .           	nstime_assert_initialized(time);
  .           	return time->ns / MILLION;
  .           }
  .           
  .           uint64_t
-- line 71 ----------------------------------------
-- line 76 ----------------------------------------
  .           
  .           uint64_t
  .           nstime_nsec(const nstime_t *time) {
  .           	nstime_assert_initialized(time);
  .           	return time->ns % BILLION;
  .           }
  .           
  .           void
124 ( 0.00%)  nstime_copy(nstime_t *time, const nstime_t *source) {
  .           	/* Source is required to be initialized. */
  .           	nstime_assert_initialized(source);
256 ( 0.01%)  	*time = *source;
  .           	nstime_assert_initialized(time);
124 ( 0.00%)  }
  .           
  .           int
  .           nstime_compare(const nstime_t *a, const nstime_t *b) {
  .           	nstime_pair_assert_initialized(a, b);
  .           	return (a->ns > b->ns) - (a->ns < b->ns);
  .           }
  .           
  .           void
  3 ( 0.00%)  nstime_add(nstime_t *time, const nstime_t *addend) {
  .           	nstime_pair_assert_initialized(time, addend);
  .           	assert(UINT64_MAX - time->ns >= addend->ns);
  .           
  .           	nstime_initialize_operand(time);
  6 ( 0.00%)  	time->ns += addend->ns;
  3 ( 0.00%)  }
  .           
  .           void
  .           nstime_iadd(nstime_t *time, uint64_t addend) {
  .           	nstime_assert_initialized(time);
  .           	assert(UINT64_MAX - time->ns >= addend);
  .           
  .           	nstime_initialize_operand(time);
  .           	time->ns += addend;
-- line 112 ----------------------------------------
-- line 136 ----------------------------------------
  .           	assert((((time->ns | multiplier) & (UINT64_MAX << (sizeof(uint64_t) <<
  .           	    2))) == 0) || ((time->ns * multiplier) / multiplier == time->ns));
  .           
  .           	nstime_initialize_operand(time);
  .           	time->ns *= multiplier;
  .           }
  .           
  .           void
  1 ( 0.00%)  nstime_idivide(nstime_t *time, uint64_t divisor) {
  .           	nstime_assert_initialized(time);
  .           	assert(divisor != 0);
  .           
  .           	nstime_initialize_operand(time);
  4 ( 0.00%)  	time->ns /= divisor;
  1 ( 0.00%)  }
  .           
  .           uint64_t
  .           nstime_divide(const nstime_t *time, const nstime_t *divisor) {
  .           	nstime_pair_assert_initialized(time, divisor);
  .           	assert(divisor->ns != 0);
  .           
  .           	/* No initialize operand -- *time itself remains unchanged. */
  .           	return time->ns / divisor->ns;
-- line 158 ----------------------------------------
-- line 184 ----------------------------------------
  .           	nstime_init(time, ticks_100ns * 100);
  .           }
  .           #elif defined(JEMALLOC_HAVE_CLOCK_MONOTONIC_COARSE)
  .           #  define NSTIME_MONOTONIC true
  .           static void
  .           nstime_get(nstime_t *time) {
  .           	struct timespec ts;
  .           
  6 ( 0.00%)  	clock_gettime(CLOCK_MONOTONIC_COARSE, &ts);
 24 ( 0.00%)  => ???:0x000000000011b278 (2x)
  .           	nstime_init2(time, ts.tv_sec, ts.tv_nsec);
  .           }
  .           #elif defined(JEMALLOC_HAVE_CLOCK_MONOTONIC)
  .           #  define NSTIME_MONOTONIC true
  .           static void
  .           nstime_get(nstime_t *time) {
  .           	struct timespec ts;
  .           
-- line 200 ----------------------------------------
-- line 270 ----------------------------------------
  .           	/* Handle non-monotonic clocks. */
  .           	if (unlikely(nstime_compare(&old_time, time) > 0)) {
  .           		nstime_copy(time, &old_time);
  .           	}
  .           }
  .           nstime_update_t *JET_MUTABLE nstime_update = nstime_update_impl;
  .           
  .           void
  8 ( 0.00%)  nstime_init_update(nstime_t *time) {
  .           	nstime_init_zero(time);
  .           	nstime_update(time);
 12 ( 0.00%)  }
  .           
  .           void
  .           nstime_prof_init_update(nstime_t *time) {
  .           	nstime_init_zero(time);
  .           	nstime_prof_update(time);
  .           }
  .           
  .           
-- line 289 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/ph.h
--------------------------------------------------------------------------------
Ir             

-- line 68 ----------------------------------------
    .           	 * happened since, and we don't track whether or not those removals are
    .           	 * from the aux list.
    .           	 */
    .           	size_t auxcount;
    .           };
    .           
    .           JEMALLOC_ALWAYS_INLINE phn_link_t *
    .           phn_link_get(void *phn, size_t offset) {
  173 ( 0.01%)  	return (phn_link_t *)(((uintptr_t)phn) + offset);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           phn_link_init(void *phn, size_t offset) {
  140 ( 0.00%)  	phn_link_get(phn, offset)->prev = NULL;
    .           	phn_link_get(phn, offset)->next = NULL;
   70 ( 0.00%)  	phn_link_get(phn, offset)->lchild = NULL;
    .           }
    .           
    .           /* Internal utility helpers. */
    .           JEMALLOC_ALWAYS_INLINE void *
    .           phn_lchild_get(void *phn, size_t offset) {
   67 ( 0.00%)  	return phn_link_get(phn, offset)->lchild;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           phn_lchild_set(void *phn, void *lchild, size_t offset) {
   38 ( 0.00%)  	phn_link_get(phn, offset)->lchild = lchild;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           phn_next_get(void *phn, size_t offset) {
  164 ( 0.01%)  	return phn_link_get(phn, offset)->next;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           phn_next_set(void *phn, void *next, size_t offset) {
    2 ( 0.00%)  	phn_link_get(phn, offset)->next = next;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           phn_prev_get(void *phn, size_t offset) {
    .           	return phn_link_get(phn, offset)->prev;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           phn_prev_set(void *phn, void *prev, size_t offset) {
   41 ( 0.00%)  	phn_link_get(phn, offset)->prev = prev;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           phn_merge_ordered(void *phn0, void *phn1, size_t offset,
    .               ph_cmp_t cmp) {
    .           	void *phn0child;
    .           
    .           	assert(phn0 != NULL);
    .           	assert(phn1 != NULL);
    .           	assert(cmp(phn0, phn1) <= 0);
    .           
    .           	phn_prev_set(phn1, phn0, offset);
    .           	phn0child = phn_lchild_get(phn0, offset);
    .           	phn_next_set(phn1, phn0child, offset);
    2 ( 0.00%)  	if (phn0child != NULL) {
    .           		phn_prev_set(phn0child, phn1, offset);
    .           	}
    .           	phn_lchild_set(phn0, phn1, offset);
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           phn_merge(void *phn0, void *phn1, size_t offset, ph_cmp_t cmp) {
    .           	void *result;
    .           	if (phn0 == NULL) {
    .           		result = phn1;
    .           	} else if (phn1 == NULL) {
    1 ( 0.00%)  		result = phn0;
    2 ( 0.00%)  	} else if (cmp(phn0, phn1) < 0) {
    .           		phn_merge_ordered(phn0, phn1, offset, cmp);
    .           		result = phn0;
    .           	} else {
    .           		phn_merge_ordered(phn1, phn0, offset, cmp);
    .           		result = phn1;
    .           	}
    .           	return result;
    .           }
-- line 150 ----------------------------------------
-- line 159 ----------------------------------------
    .           	/*
    .           	 * Multipass merge, wherein the first two elements of a FIFO
    .           	 * are repeatedly merged, and each result is appended to the
    .           	 * singly linked FIFO, until the FIFO contains only a single
    .           	 * element.  We start with a sibling list but no reference to
    .           	 * its tail, so we do a single pass over the sibling list to
    .           	 * populate the FIFO.
    .           	 */
   68 ( 0.00%)  	if (phn1 != NULL) {
    .           		void *phnrest = phn_next_get(phn1, offset);
    .           		if (phnrest != NULL) {
    .           			phn_prev_set(phnrest, NULL, offset);
    .           		}
    .           		phn_prev_set(phn0, NULL, offset);
    .           		phn_next_set(phn0, NULL, offset);
    .           		phn_prev_set(phn1, NULL, offset);
    .           		phn_next_set(phn1, NULL, offset);
-- line 175 ----------------------------------------
-- line 217 ----------------------------------------
    .           			}
    .           		}
    .           	}
    .           	return phn0;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           ph_merge_aux(ph_t *ph, size_t offset, ph_cmp_t cmp) {
   67 ( 0.00%)  	ph->auxcount = 0;
    .           	void *phn = phn_next_get(ph->root, offset);
  134 ( 0.00%)  	if (phn != NULL) {
    .           		phn_prev_set(ph->root, NULL, offset);
    .           		phn_next_set(ph->root, NULL, offset);
    .           		phn_prev_set(phn, NULL, offset);
    .           		phn = phn_merge_siblings(phn, offset, cmp);
    .           		assert(phn_next_get(phn, offset) == NULL);
    1 ( 0.00%)  		ph->root = phn_merge(ph->root, phn, offset, cmp);
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           ph_merge_children(void *phn, size_t offset, ph_cmp_t cmp) {
    .           	void *result;
    .           	void *lchild = phn_lchild_get(phn, offset);
  201 ( 0.01%)  	if (lchild == NULL) {
   35 ( 0.00%)  		result = NULL;
    .           	} else {
    .           		result = phn_merge_siblings(lchild, offset, cmp);
    .           	}
    .           	return result;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           ph_new(ph_t *ph) {
    1 ( 0.00%)  	ph->root = NULL;
    1 ( 0.00%)  	ph->auxcount = 0;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE bool
    .           ph_empty(ph_t *ph) {
    .           	return ph->root == NULL;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           ph_first(ph_t *ph, size_t offset, ph_cmp_t cmp) {
  192 ( 0.01%)  	if (ph->root == NULL) {
    .           		return NULL;
    .           	}
    .           	ph_merge_aux(ph, offset, cmp);
    .           	return ph->root;
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           ph_any(ph_t *ph, size_t offset) {
-- line 270 ----------------------------------------
-- line 311 ----------------------------------------
    .           
    .           	/*
    .           	 * Treat the root as an aux list during insertion, and lazily merge
    .           	 * during a_prefix##remove_first().  For elements that are inserted,
    .           	 * then removed via a_prefix##remove() before the aux list is ever
    .           	 * processed, this makes insert/remove constant-time, whereas eager
    .           	 * merging would make insert O(log n).
    .           	 */
  210 ( 0.01%)  	if (ph->root == NULL) {
   72 ( 0.00%)  		ph->root = phn;
    .           	} else {
    .           		/*
    .           		 * As a special case, check to see if we can replace the root.
    .           		 * This is practically common in some important cases, and lets
    .           		 * us defer some insertions (hopefully, until the point where
    .           		 * some of the items in the aux list have been removed, savings
    .           		 * us from linking them at all).
    .           		 */
   68 ( 0.00%)  		if (cmp(phn, ph->root) < 0) {
    .           			phn_lchild_set(phn, ph->root, offset);
    .           			phn_prev_set(ph->root, phn, offset);
   33 ( 0.00%)  			ph->root = phn;
   33 ( 0.00%)  			ph->auxcount = 0;
    .           			return;
    .           		}
  147 ( 0.01%)  		ph->auxcount++;
    .           		phn_next_set(phn, phn_next_get(ph->root, offset), offset);
    2 ( 0.00%)  		if (phn_next_get(ph->root, offset) != NULL) {
    .           			phn_prev_set(phn_next_get(ph->root, offset), phn,
    .           			    offset);
    .           		}
    .           		phn_prev_set(phn, ph->root, offset);
    .           		phn_next_set(ph->root, phn, offset);
    .           	}
   74 ( 0.00%)  	if (ph->auxcount > 1) {
    .           		unsigned nmerges = ffs_zu(ph->auxcount - 1);
    .           		bool done = false;
    .           		for (unsigned i = 0; i < nmerges && !done; i++) {
    .           			done = ph_try_aux_merge_pair(ph, offset, cmp);
    .           		}
    .           	}
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void *
    .           ph_remove_first(ph_t *ph, size_t offset, ph_cmp_t cmp) {
    .           	void *ret;
    .           
6,186 ( 0.22%)  	if (ph->root == NULL) {
    .           		return NULL;
    .           	}
    .           	ph_merge_aux(ph, offset, cmp);
    .           	ret = ph->root;
   36 ( 0.00%)  	ph->root = ph_merge_children(ph->root, offset, cmp);
    .           
    .           	return ret;
    .           
    .           }
    .           
    .           JEMALLOC_ALWAYS_INLINE void
    .           ph_remove(ph_t *ph, void *phn, size_t offset, ph_cmp_t cmp) {
    .           	void *replace;
    .           	void *parent;
    .           
   93 ( 0.00%)  	if (ph->root == phn) {
    .           		/*
    .           		 * We can delete from aux list without merging it, but we need
    .           		 * to merge if we are dealing with the root node and it has
    .           		 * children.
    .           		 */
   62 ( 0.00%)  		if (phn_lchild_get(phn, offset) == NULL) {
   31 ( 0.00%)  			ph->root = phn_next_get(phn, offset);
   62 ( 0.00%)  			if (ph->root != NULL) {
    .           				phn_prev_set(ph->root, NULL, offset);
    .           			}
    .           			return;
    .           		}
    .           		ph_merge_aux(ph, offset, cmp);
    .           		if (ph->root == phn) {
    .           			ph->root = ph_merge_children(ph->root, offset, cmp);
    .           			return;
-- line 390 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c
--------------------------------------------------------------------------------
Ir           

-- line 6 ----------------------------------------
  .           enum emap_lock_result_e {
  .           	emap_lock_result_success,
  .           	emap_lock_result_failure,
  .           	emap_lock_result_no_extent
  .           };
  .           typedef enum emap_lock_result_e emap_lock_result_t;
  .           
  .           bool
  1 ( 0.00%)  emap_init(emap_t *emap, base_t *base, bool zeroed) {
  2 ( 0.00%)  	return rtree_new(&emap->rtree, base, zeroed);
134 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/rtree.c:_rjem_je_rtree_new (1x)
  .           }
  .           
  .           void
  .           emap_update_edata_state(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
882 ( 0.03%)      extent_state_t state) {
  .           	witness_assert_positive_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
  .           	    WITNESS_RANK_CORE);
  .           
  .           	edata_state_set(edata, state);
  .           
  .           	EMAP_DECLARE_RTREE_CTX;
  .           	rtree_leaf_elm_t *elm1 = rtree_leaf_elm_lookup(tsdn, &emap->rtree,
  .           	    rtree_ctx, (uintptr_t)edata_base_get(edata), /* dependent */ true,
  .           	    /* init_missing */ false);
  .           	assert(elm1 != NULL);
126 ( 0.00%)  	rtree_leaf_elm_t *elm2 = edata_size_get(edata) == PAGE ? NULL :
  .           	    rtree_leaf_elm_lookup(tsdn, &emap->rtree, rtree_ctx,
  .           	    (uintptr_t)edata_last_get(edata), /* dependent */ true,
  .           	    /* init_missing */ false);
  .           
  .           	rtree_leaf_elm_state_update(tsdn, &emap->rtree, elm1, elm2, state);
  .           
  .           	emap_assert_mapped(tsdn, emap, edata);
693 ( 0.02%)  }
  .           
  .           static inline edata_t *
  .           emap_try_acquire_edata_neighbor_impl(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
  .               extent_pai_t pai, extent_state_t expected_state, bool forward,
 26 ( 0.00%)      bool expanding) {
  .           	witness_assert_positive_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
  .           	    WITNESS_RANK_CORE);
  .           	assert(!edata_guarded_get(edata));
  .           	assert(!expanding || forward);
  .           	assert(!edata_state_in_transition(expected_state));
  .           	assert(expected_state == extent_state_dirty ||
  .           	       expected_state == extent_state_muzzy ||
  .           	       expected_state == extent_state_retained);
  .           
  4 ( 0.00%)  	void *neighbor_addr = forward ? edata_past_get(edata) :
  .           	    edata_before_get(edata);
  .           	/*
  .           	 * This is subtle; the rtree code asserts that its input pointer is
  .           	 * non-NULL, and this is a useful thing to check.  But it's possible
  .           	 * that edata corresponds to an address of (void *)PAGE (in practice,
  .           	 * this has only been observed on FreeBSD when address-space
  .           	 * randomization is on, but it could in principle happen anywhere).  In
  .           	 * this case, edata_before_get(edata) is NULL, triggering the assert.
  .           	 */
  4 ( 0.00%)  	if (neighbor_addr == NULL) {
  2 ( 0.00%)  		return NULL;
  .           	}
  .           
  .           	EMAP_DECLARE_RTREE_CTX;
  .           	rtree_leaf_elm_t *elm = rtree_leaf_elm_lookup(tsdn, &emap->rtree,
  .           	    rtree_ctx, (uintptr_t)neighbor_addr, /* dependent*/ false,
  .           	    /* init_missing */ false);
  4 ( 0.00%)  	if (elm == NULL) {
  .           		return NULL;
  .           	}
  .           
  .           	rtree_contents_t neighbor_contents = rtree_leaf_elm_read(tsdn,
  .           	    &emap->rtree, elm, /* dependent */ true);
  .           	if (!extent_can_acquire_neighbor(edata, neighbor_contents, pai,
  .           	    expected_state, forward, expanding)) {
  .           		return NULL;
-- line 80 ----------------------------------------
-- line 86 ----------------------------------------
  .           	emap_update_edata_state(tsdn, emap, neighbor, extent_state_merging);
  .           	if (expanding) {
  .           		extent_assert_can_expand(edata, neighbor);
  .           	} else {
  .           		extent_assert_can_coalesce(edata, neighbor);
  .           	}
  .           
  .           	return neighbor;
 22 ( 0.00%)  }
  .           
  .           edata_t *
  .           emap_try_acquire_edata_neighbor(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
  4 ( 0.00%)      extent_pai_t pai, extent_state_t expected_state, bool forward) {
  6 ( 0.00%)  	return emap_try_acquire_edata_neighbor_impl(tsdn, emap, edata, pai,
131 ( 0.00%)  => /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/src/emap.c:emap_try_acquire_edata_neighbor_impl (2x)
  .           	    expected_state, forward, /* expand */ false);
  4 ( 0.00%)  }
  .           
  .           edata_t *
  .           emap_try_acquire_edata_neighbor_expand(tsdn_t *tsdn, emap_t *emap,
  .               edata_t *edata, extent_pai_t pai, extent_state_t expected_state) {
  .           	/* Try expanding forward. */
  .           	return emap_try_acquire_edata_neighbor_impl(tsdn, emap, edata, pai,
  .           	    expected_state, /* forward */ true, /* expand */ true);
  .           }
-- line 109 ----------------------------------------
-- line 116 ----------------------------------------
  .           
  .           	emap_update_edata_state(tsdn, emap, edata, new_state);
  .           }
  .           
  .           static bool
  .           emap_rtree_leaf_elms_lookup(tsdn_t *tsdn, emap_t *emap, rtree_ctx_t *rtree_ctx,
  .               const edata_t *edata, bool dependent, bool init_missing,
  .               rtree_leaf_elm_t **r_elm_a, rtree_leaf_elm_t **r_elm_b) {
 64 ( 0.00%)  	*r_elm_a = rtree_leaf_elm_lookup(tsdn, &emap->rtree, rtree_ctx,
  .           	    (uintptr_t)edata_base_get(edata), dependent, init_missing);
130 ( 0.00%)  	if (!dependent && *r_elm_a == NULL) {
  .           		return true;
  .           	}
  .           	assert(*r_elm_a != NULL);
  .           
 64 ( 0.00%)  	*r_elm_b = rtree_leaf_elm_lookup(tsdn, &emap->rtree, rtree_ctx,
  .           	    (uintptr_t)edata_last_get(edata), dependent, init_missing);
  2 ( 0.00%)  	if (!dependent && *r_elm_b == NULL) {
  .           		return true;
  .           	}
  .           	assert(*r_elm_b != NULL);
  .           
  .           	return false;
  .           }
  .           
  .           static void
  .           emap_rtree_write_acquired(tsdn_t *tsdn, emap_t *emap, rtree_leaf_elm_t *elm_a,
  .               rtree_leaf_elm_t *elm_b, edata_t *edata, szind_t szind, bool slab) {
  .           	rtree_contents_t contents;
  .           	contents.edata = edata;
  .           	contents.metadata.szind = szind;
  .           	contents.metadata.slab = slab;
321 ( 0.01%)  	contents.metadata.is_head = (edata == NULL) ? false :
  .           	    edata_is_head_get(edata);
  .           	contents.metadata.state = (edata == NULL) ? 0 : edata_state_get(edata);
  .           	rtree_leaf_elm_write(tsdn, &emap->rtree, elm_a, contents);
128 ( 0.00%)  	if (elm_b != NULL) {
  .           		rtree_leaf_elm_write(tsdn, &emap->rtree, elm_b, contents);
  .           	}
  .           }
  .           
  .           bool
  .           emap_register_boundary(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
 16 ( 0.00%)      szind_t szind, bool slab) {
  .           	assert(edata_state_get(edata) == extent_state_active);
  .           	EMAP_DECLARE_RTREE_CTX;
  .           
  .           	rtree_leaf_elm_t *elm_a, *elm_b;
  .           	bool err = emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, edata,
  .           	    false, true, &elm_a, &elm_b);
  .           	if (err) {
  .           		return true;
  .           	}
  .           	assert(rtree_leaf_elm_read(tsdn, &emap->rtree, elm_a,
  .           	    /* dependent */ false).edata == NULL);
  .           	assert(rtree_leaf_elm_read(tsdn, &emap->rtree, elm_b,
  .           	    /* dependent */ false).edata == NULL);
  .           	emap_rtree_write_acquired(tsdn, emap, elm_a, elm_b, edata, szind, slab);
  1 ( 0.00%)  	return false;
 11 ( 0.00%)  }
  .           
  .           /* Invoked *after* emap_register_boundary. */
  .           void
  .           emap_register_interior(tsdn_t *tsdn, emap_t *emap, edata_t *edata,
345 ( 0.01%)      szind_t szind) {
  .           	EMAP_DECLARE_RTREE_CTX;
  .           
  .           	assert(edata_slab_get(edata));
  .           	assert(edata_state_get(edata) == extent_state_active);
  .           
  .           	if (config_debug) {
  .           		/* Making sure the boundary is registered already. */
  .           		rtree_leaf_elm_t *elm_a, *elm_b;
-- line 188 ----------------------------------------
-- line 202 ----------------------------------------
  .           	rtree_contents_t contents;
  .           	contents.edata = edata;
  .           	contents.metadata.szind = szind;
  .           	contents.metadata.slab = true;
  .           	contents.metadata.state = extent_state_active;
  .           	contents.metadata.is_head = false; /* Not allowed to access. */
  .           
  .           	assert(edata_size_get(edata) > (2 << LG_PAGE));
 46 ( 0.00%)  	rtree_write_range(tsdn, &emap->rtree, rtree_ctx,
  .           	    (uintptr_t)edata_base_get(edata) + PAGE,
  .           	    (uintptr_t)edata_last_get(edata) - PAGE, contents);
253 ( 0.01%)  }
  .           
  .           void
  .           emap_deregister_boundary(tsdn_t *tsdn, emap_t *emap, edata_t *edata) {
  .           	/*
  .           	 * The edata must be either in an acquired state, or protected by state
  .           	 * based locks.
  .           	 */
  .           	if (!emap_edata_is_acquired(tsdn, emap, edata)) {
-- line 221 ----------------------------------------
-- line 241 ----------------------------------------
  .           		rtree_clear_range(tsdn, &emap->rtree, rtree_ctx,
  .           		    (uintptr_t)edata_base_get(edata) + PAGE,
  .           		    (uintptr_t)edata_last_get(edata) - PAGE);
  .           	}
  .           }
  .           
  .           void
  .           emap_remap(tsdn_t *tsdn, emap_t *emap, edata_t *edata, szind_t szind,
544 ( 0.02%)      bool slab) {
  .           	EMAP_DECLARE_RTREE_CTX;
  .           
 64 ( 0.00%)  	if (szind != SC_NSIZES) {
  .           		rtree_contents_t contents;
  .           		contents.edata = edata;
  .           		contents.metadata.szind = szind;
  .           		contents.metadata.slab = slab;
  .           		contents.metadata.is_head = edata_is_head_get(edata);
  .           		contents.metadata.state = edata_state_get(edata);
  .           
 32 ( 0.00%)  		rtree_write(tsdn, &emap->rtree, rtree_ctx,
  .           		    (uintptr_t)edata_addr_get(edata), contents);
  .           		/*
  .           		 * Recall that this is called only for active->inactive and
  .           		 * inactive->active transitions (since only active extents have
  .           		 * meaningful values for szind and slab).  Active, non-slab
  .           		 * extents only need to handle lookups at their head (on
  .           		 * deallocation), so we don't bother filling in the end
  .           		 * boundary.
  .           		 *
  .           		 * For slab extents, we do the end-mapping change.  This still
  .           		 * leaves the interior unmodified; an emap_register_interior
  .           		 * call is coming in those cases, though.
  .           		 */
126 ( 0.00%)  		if (slab && edata_size_get(edata) > PAGE) {
 23 ( 0.00%)  			uintptr_t key = (uintptr_t)edata_past_get(edata)
  .           			    - (uintptr_t)PAGE;
  .           			rtree_write(tsdn, &emap->rtree, rtree_ctx, key,
  .           			    contents);
  .           		}
  .           	}
375 ( 0.01%)  }
  .           
  .           bool
  .           emap_split_prepare(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
544 ( 0.02%)      edata_t *edata, size_t size_a, edata_t *trail, size_t size_b) {
  .           	EMAP_DECLARE_RTREE_CTX;
  .           
  .           	/*
  .           	 * We use incorrect constants for things like arena ind, zero, ranged,
  .           	 * and commit state, and head status.  This is a fake edata_t, used to
  .           	 * facilitate a lookup.
  .           	 */
  .           	edata_t lead = {0};
-- line 293 ----------------------------------------
-- line 294 ----------------------------------------
  .           	edata_init(&lead, 0U, edata_addr_get(edata), size_a, false, 0, 0,
  .           	    extent_state_active, false, false, EXTENT_PAI_PAC, EXTENT_NOT_HEAD);
  .           
  .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, &lead, false, true,
  .           	    &prepare->lead_elm_a, &prepare->lead_elm_b);
  .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, trail, false, true,
  .           	    &prepare->trail_elm_a, &prepare->trail_elm_b);
  .           
128 ( 0.00%)  	if (prepare->lead_elm_a == NULL || prepare->lead_elm_b == NULL
128 ( 0.00%)  	    || prepare->trail_elm_a == NULL || prepare->trail_elm_b == NULL) {
  .           		return true;
  .           	}
  .           	return false;
352 ( 0.01%)  }
  .           
  .           void
  .           emap_split_commit(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
 64 ( 0.00%)      edata_t *lead, size_t size_a, edata_t *trail, size_t size_b) {
  .           	/*
  .           	 * We should think about not writing to the lead leaf element.  We can
  .           	 * get into situations where a racing realloc-like call can disagree
  .           	 * with a size lookup request.  I think it's fine to declare that these
  .           	 * situations are race bugs, but there's an argument to be made that for
  .           	 * things like xallocx, a size lookup call should return either the old
  .           	 * size or the new size, but not anything else.
  .           	 */
 64 ( 0.00%)  	emap_rtree_write_acquired(tsdn, emap, prepare->lead_elm_a,
  .           	    prepare->lead_elm_b, lead, SC_NSIZES, /* slab */ false);
 64 ( 0.00%)  	emap_rtree_write_acquired(tsdn, emap, prepare->trail_elm_a,
  .           	    prepare->trail_elm_b, trail, SC_NSIZES, /* slab */ false);
 32 ( 0.00%)  }
  .           
  .           void
  .           emap_merge_prepare(tsdn_t *tsdn, emap_t *emap, emap_prepare_t *prepare,
  .               edata_t *lead, edata_t *trail) {
  .           	EMAP_DECLARE_RTREE_CTX;
  .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, lead, true, false,
  .           	    &prepare->lead_elm_a, &prepare->lead_elm_b);
  .           	emap_rtree_leaf_elms_lookup(tsdn, emap, rtree_ctx, trail, true, false,
-- line 332 ----------------------------------------

--------------------------------------------------------------------------------
-- Auto-annotated source: /vol/projects/mu/target/release/build/tikv-jemalloc-sys-bac9cde9f8d8e048/out/build/include/jemalloc/internal/atomic.h
--------------------------------------------------------------------------------
Ir             

-- line 75 ----------------------------------------
    .           /*
    .            * Not all platforms have 64-bit atomics.  If we do, this #define exposes that
    .            * fact.
    .            */
    .           #if (LG_SIZEOF_PTR == 3 || LG_SIZEOF_INT == 3)
    .           #  define JEMALLOC_ATOMIC_U64
    .           #endif
    .           
  740 ( 0.03%)  JEMALLOC_GENERATE_ATOMICS(void *, p, LG_SIZEOF_PTR)
    .           
    .           /*
    .            * There's no actual guarantee that sizeof(bool) == 1, but it's true on the only
    .            * platform that actually needs to know the size, MSVC.
    .            */
  277 ( 0.01%)  JEMALLOC_GENERATE_ATOMICS(bool, b, 0)
    .           
  196 ( 0.01%)  JEMALLOC_GENERATE_EXPANDED_INT_ATOMICS(unsigned, u, LG_SIZEOF_INT)
    .           
3,071 ( 0.11%)  JEMALLOC_GENERATE_EXPANDED_INT_ATOMICS(size_t, zu, LG_SIZEOF_PTR)
    .           
   40 ( 0.00%)  JEMALLOC_GENERATE_EXPANDED_INT_ATOMICS(ssize_t, zd, LG_SIZEOF_PTR)
    .           
    6 ( 0.00%)  JEMALLOC_GENERATE_EXPANDED_INT_ATOMICS(uint8_t, u8, 0)
    .           
    2 ( 0.00%)  JEMALLOC_GENERATE_EXPANDED_INT_ATOMICS(uint32_t, u32, 2)
    .           
    .           #ifdef JEMALLOC_ATOMIC_U64
    2 ( 0.00%)  JEMALLOC_GENERATE_EXPANDED_INT_ATOMICS(uint64_t, u64, 3)
    .           #endif
    .           
    .           #undef ATOMIC_INLINE
    .           
    .           #endif /* JEMALLOC_INTERNAL_ATOMIC_H */

--------------------------------------------------------------------------------
The following files chosen for auto-annotation could not be found:
--------------------------------------------------------------------------------
  ./elf/../bits/stdlib-bsearch.h
  ./elf/../elf/dl-tls.c
  ./elf/../sysdeps/generic/dl-new-hash.h
  ./elf/../sysdeps/generic/dl-protected.h
  ./elf/../sysdeps/generic/ldsodefs.h
  ./elf/../sysdeps/unix/sysv/linux/dl-sysdep.c
  ./elf/../sysdeps/x86/dl-cacheinfo.h
  ./elf/../sysdeps/x86/dl-prop.h
  ./elf/../sysdeps/x86_64/dl-machine.h
  ./elf/../sysdeps/x86_64/dl-trampoline.h
  ./elf/./dl-find_object.h
  ./elf/./dl-map-segments.h
  ./elf/./elf/dl-cache.c
  ./elf/./elf/dl-catch.c
  ./elf/./elf/dl-deps.c
  ./elf/./elf/dl-environ.c
  ./elf/./elf/dl-find_object.c
  ./elf/./elf/dl-hwcaps.c
  ./elf/./elf/dl-hwcaps_split.c
  ./elf/./elf/dl-init.c
  ./elf/./elf/dl-load.c
  ./elf/./elf/dl-lookup-direct.c
  ./elf/./elf/dl-lookup.c
  ./elf/./elf/dl-minimal-malloc.c
  ./elf/./elf/dl-minimal.c
  ./elf/./elf/dl-misc.c
  ./elf/./elf/dl-object.c
  ./elf/./elf/dl-reloc.c
  ./elf/./elf/dl-runtime.c
  ./elf/./elf/dl-tunables.c
  ./elf/./elf/dl-tunables.h
  ./elf/./elf/dl-version.c
  ./elf/./elf/do-rel.h
  ./elf/./elf/get-dynamic-info.h
  ./elf/./elf/rtld.c
  ./elf/./get-dynamic-info.h
  ./libio/./libio/fileops.c
  ./libio/./libio/genops.c
  ./libio/./libio/iogetdelim.c
  ./libio/./libio/strops.c
  ./malloc/./malloc/malloc.c
  ./misc/../sysdeps/unix/sysv/linux/mmap64.c
  ./nptl/../string/bits/string_fortified.h
  ./nptl/./nptl/libc-cleanup.c
  ./nptl/./nptl/pthread_getattr_np.c
  ./nptl/./nptl/pthread_mutex_init.c
  ./nptl/./nptl/pthread_mutex_trylock.c
  ./nptl/./nptl/pthread_mutex_unlock.c
  ./nptl/./nptl/pthread_mutexattr_init.c
  ./nptl/./nptl/pthread_mutexattr_settype.c
  ./posix/../sysdeps/unix/sysv/linux/x86/sysconf.c
  ./stdio-common/../include/scratch_buffer.h
  ./stdio-common/../libio/strfile.h
  ./stdio-common/./stdio-common/isoc23_sscanf.c
  ./stdio-common/./stdio-common/printf-parse.h
  ./stdio-common/./stdio-common/vfscanf-internal.c
  ./stdlib/../stdlib/strtol.c
  ./stdlib/../stdlib/strtol_l.c
  ./stdlib/./stdlib/getenv.c
  ./string/../string/strcspn.c
  ./string/../sysdeps/x86_64/multiarch/../multiarch/memset-vec-unaligned-erms.S
  ./string/../sysdeps/x86_64/multiarch/../multiarch/strchr-sse2.S
  ./string/../sysdeps/x86_64/multiarch/../multiarch/strcmp-sse2.S
  ./string/../sysdeps/x86_64/multiarch/../multiarch/strlen-sse2.S
  ./string/../sysdeps/x86_64/multiarch/memchr-avx2.S
  ./string/../sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S
  ./string/../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S
  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S
  ./string/../sysdeps/x86_64/multiarch/strlen-avx2.S
  /cargo/registry/src/index.crates.io-6f17d22bba15001f/compiler_builtins-0.1.103/./lib/builtins/popcountdi2.c
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/fmt/mod.rs
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/dec2flt/common.rs
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/dec2flt/mod.rs
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/dec2flt/parse.rs
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/num/mod.rs
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/str/converts.rs
  /rustc/07dca489ac2d933c78d3c5158e3f43beefeb02ce/library/core/src/str/validations.rs

--------------------------------------------------------------------------------
Ir               
--------------------------------------------------------------------------------
201,312 ( 7.13%)  events annotated

